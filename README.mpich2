

			  MPICH2 Release 0.94

MPICH2 is an all-new implementation of MPI from the group at Argonne
National Laboratory.  It shares many goals with the original MPICH but
no actual code.  It is intended to become a portable, high-performance
implementation of the entire MPI-2 standard.  This release has many
MPI-2 features but is not quite complete.  It also currently supports
only a few communication methods.

This is an early release of MPICH2.  It has been tested by us on a
variety of machines in our own environment, but not extensively tested
by outside users.  If you have problems, please report them to
mpich2-maint@mcs.anl.gov; we are interested in incorporating user
experiences as quickly as possible.

Getting Started
---------------

The following instructions take you through a sequence of steps to get
the default configuration (TCP communication, MPD process management) of
MPICH2 up and running.  Alternate configuration options are described
later, in the section "Alternative configurations".  

1.  You will need the following prerequisites.

      This tar file mpich2.tar.gz
      A C compiler (gcc is sufficient)
      A Fortran compiler if Fortran applications are to be used (g77 is
	sufficient. 
      A C++ compiler for the C++ MPI bindings (g++ is sufficient)
      Python 2.2 or later (for the default MPD process manager)
      PyXML and an XML parser like expat (in order to use mpiexec)

    configure will check for these prerequisites and try to work around
    deficiencies if possible.

2.  Unpack the tar file and go to the top level directory:

      tar xfz mpich2.tar.gz
      cd mpich2-0.94

    If your tar doesn't accept the z option, use

      gunzip mpich2.tar.gz
      tar xf mpich2.tar
      cd mpich2-0.94

3.  Choose an installation directory (the default is /usr/local/bin):

      mkdir /home/you/mpich2-install

4.  Configure MPICH2, specifying the installation directory:

      ./configure -prefix=/home/you/mpich2-install >& configure.log

    Other configure options are described below.  You might also prefer
    to do a VPATH build.  Check the configure.log file to make sure
    everything went will.  Problems should be self-explanatory, but if
    not, sent configure.log to mpich2-maint@mcs.anl.gov.

5.  Build MPICH2:

      make >& make.log

    This step should succeed if there were no problems with the
    preceding step.  Check make.log.  If there were problems, send
    make.log to mpich2-maint@mcs.anl.gov.

6.  Install the MPICH2 commands:

      make install >& install.log

    This step collects all required executables and scripts in the bin
    subdirectory of the directory specified by the prefix argument to
    configure. 

7.  Add the bin subdirectory of the installation directory to your path:

      setenv PATH /home/you/mpich2-install:$PATH

    for csh and tcsh, or 

      export PATH=/home/you/mpich2-install:$PATH

    for bash and sh.  Check that everything is in order at this point by
    doing 

      which mpd
      which mpiexec
      which mpirun

    All should refer to the commands in the bin subdirectory of your
    install directory.

8.  MPICH2, unlike MPICH, uses an external process manager for
    scalable startup of large MPI jobs.  The default process manager is
    called MPD, which is a ring of daemons on the machines where you
    will run your MPI programs.  In the next few steps, you will get his
    ring up and tested.  

    Begin by placing in your home directory a file named mpd.config,
    containing the line 

      password=<passwd>

    where <passwd> is a string known only to yourself.  It should not be
    your normal Unix password.  Make this file readable only by you:

      chmod og-r mpd.conf

9.  The first sanity check consists of bringing up a ring of one mpd on
    the local machine, testing one mpd command, and bringing the "ring"
    down. 

      mpd &
      mpdtrace
      mpdallexit

    The output of mpdtrace should be the hostname of the machine you are
    running on.  The mpdallexit causes the mpd daemon to exit.

10. Now we will bring up a ring of mpd's on a set of machines.  Create a
    file consisting of a list of machine names, one per line.  Name this
    file mpd.hosts.  These hostnames will be used as targets for ssh or
    rsh, so include full domain names if necessary.  Check that you can
    reach these machines with ssh or rsh without entering a password.
    If not, you will need to configure ssh or rsh so that this can be
    done.  You can test by doing

      ssh othermachine date

    or

      rsh othermachine date

11. Start the daemons on (some of ) the hosts in the file:

      mpdboot -n <number to start>  




