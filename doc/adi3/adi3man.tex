% 
%   This is a latex file that generates a reference manual for 
%   ADI-3 
%
\documentclass{article}
\usepackage{/home/gropp/data/share/refman}
\usepackage{/home/gropp/sowing-proj/sowing/docs/doctext/tpage}
\usepackage{epsf}
\textheight=9in
\textwidth=6.1in
\oddsidemargin=.2in
\topmargin=-.50in
\newread\testfile

%
% For now, let findex be the same as index.  This will allow us to
% more easily separate function and nonfunction index entries later.
\let\findex=\index
%
% Modify the way titles are handled for no breaks between pages
\def\mantitle#1#2#3{\pagerule\nobreak
\ifmancontents\addcontentsline{toc}{subsection}{#1}\fi
\findex{#1}}

\makeindex

\begin{document}

\markright{ADI-3 Reference Manual}

\def\nopound{\catcode`\#=13}
{\nopound\gdef#{{\tt \char`\#}}}
\catcode`\_=13
\def_{{\tt \char`\_}}
\catcode`\_=11
\def\code#1{{\tt #1}}
\let\url=\code

%\tpageoneskip
\ANLTMTitle{MPICH Abstract Device Interface\\
Version 3\\
Reference Manual\\\ \\Draft of \today}{\em 
William Gropp\\
Ewing Lusk\\
Mathematics and Computer Science Division\\
Argonne National Laboratory}{00}{\today}

\clearpage

\pagenumbering{roman}
\tableofcontents
\clearpage

\pagenumbering{arabic}
\pagestyle{headings}

\section{Introduction}
This document contains detailed documentation on the routines that are part of
the Abstract Device Interface, version 3, used to implement the MPICH2000
model MPI implementation. 

% As an alternate to this manual, the reader should consider using the
% script \code{mpiman}; this is a script that uses \code{xman} to provide
% a X11 Window System interface to the data in this manual.

\section{Discussion}
The ADI contains a large number of routines, but only a few of these
are related to the lowest-level communication operations.  Most of the
rest are used to provide opportunities for performance optimization
(e.g., \code{MPID_Isend}) or to support the objects that MPI
provides and that a device implementation may (or may not) need to
understand (e.g., MPI attributes and groups).  

The ADI is organized as a number of separate modules.
For each module except for the core communication module (called MPID
CORE), there is a sample (but complete) implementation that may be used to
build other implementations of the ADI.  These sample
implementations are refered to as the \emph{generic} module
implementations. For example,
the ADI contains routines to create and manipulate MPI communicators;
most implementations of the ADI will use the generic module for
handling communicators.  Other modules, such as the point-to-point
communications module, are more likely to be replaced with a
device-specific implementation.  Even in the cases where an
implementation of the ADI replaces a module, the generic
implementation of that module will often serve as a guide.
As a guide to implementing the MPID CORE, we provide several implementations
for popular interprocess communication mechanisms, including TCP.

There are roughly three classes of modules:
\begin{enumerate}
\item The core: basic process creation and communication. This
contains one module, MPID CORE. 
\item Support for MPI objects.  These modules support the various MPI
objects such as Datatypes, Groups, Communicators, and keyvals.
\item Communication optimization.  These modules provide an
intermediate level of functionality between that provided by MPID CORE
and that provided by MPI.  The rationale for these are described in
Section~\ref{sec-intermediate}. 
\end{enumerate}

The modules are (THIS LIST IS INCOMPLETE):
\begin{description}
\item[MPID CORE.] Basic functions to start, stop, and communicate
between processes.  This also includes the \code{mpiexec} program.
This is the only module for which there is no generic implementation; as
described in Section~\ref{sec-minimal}, implementing just this module,
combined with the generic implementation of the other modules, provides a
complete ADI-3 implementation.
\item[Attribute.] MPI Keyvals and attributes
\item[Datatype.] MPI Datatypes and pack/unpack operations and a new
object, \code{MPID_Segment}, used only within the ADI.
\item[Dynamic.] Support for MPI-2 dynamic process operations.  This
module must be implemented to support the dynamic process chapter of MPI-2.
\item[Group.] MPI Groups
\item[Communicator.] MPI Communicators
\item[Request.] MPI Requests.  Requests are fundemental to many
operations; most devices that implement more than MPID CORE will need
to replace this module.
\item[Window.] MPI Windows.
\item[Point to point]Functions directly implementing the MPI
point-to-point communication functions or some key subsets.
\item[Topology.]Information on the physical interconnect.  Currently
limited to hierarchical information necessary to support clusters of
SMPs and similar systems.
\item[Timer.] Support for fast timers, including synchronized timers.
\item[Stream.] Communication routines designed to support
important algorithms for collective communication.
\item[Collective.] Alternate support for collective
routines. Note that this may end up being an MPI module, not
an MPID module, exploiting the routines in the \textbf{Stream} module.
\item[Utility.] Routines to simplify correctness and performance
debugging, and to provide a common interface to OS services that have
different interfaces on different platforms.
\item[Environment.] This module contains both compile-time and run-time
constants that describe the environment. 
\item[Communication.] The routines that carry out communication among MPI
  processes. 
\item[Error Reporting.] This provides support for detailed error
reporting.  Most implementations will use the generic implementation
of this module.
\item[Extensions.] This module contains no required routines; instead,
it documents some of the various extensions that the MPICH group is
considering, such as checkpointing.
\end{description}

By dividing the ADI into these modules, it becomes easy to implement
MPI quickly by implementing just MPID CORE and using the generic
modules for everything else.  Other modules can be replaced as
warranted; for example, a new machine will likely need a new Timers
module in order to access a system-specific timer.  Implementors
trying to get the maximum performance for point-to-point operations
may want to replace that module with one carefully tuned to their
specific platform.

Not included in this list is the ADIO module used to support MPI-IO.
See \cite{ThakurGroLus96} and \url{http://www.mcs.anl.gov/~thakur/adio} 
%\cite{romio-adio-manual} 
%(ROMIO ADIO Manual)
for a description of that module.

\subsection{The Core}
\label{sec-minimal}

While ADI-3 defines a large number of routines, only a few must be implemented
by a developer.  For all modules except the \code{MPID_CORE} module, MPICH
provides a generic implementation.  Thus, only the routines in
\code{MPID_CORE} must be implemented when porting MPICH to a new interprocess
communication mechanism.  To further simplify the task of implementing MPI
with ADI-3, the core does not contains the routines needed to implement the
I/O or dynamic process chapters of the MPI Standard (it does contain the
routines needed to implement the remote memory access chapter).  The
\code{MPID_CORE} consists of the following routines:  

\begin{description}
\item[\code{MPID_Init}.] This is the device's counterpart to
\code{MPI_Init_thread}. 
\item[\code{MPID_Finalize}.] This is the devices's counterpart to
\code{MPI_Finalize}
\item[\code{MPID_Abort}.] This is the device's counterpart to \code{MPI_Abort}.
\item[\code{MPID_Put_contig}.] This is used for data movement; it is roughly an
\code{MPI_Put} using only contiguous data.  However, it uses a
different mechanism for indicating completion of data transfers (see \code{MPID_Flags_waitsome} and \code{MPID_Flags_testsome})
\item[\code{MPID_Rhcv}.] This is used for messages; it is roughly an
active-message call.  However, only predefined actions are allowed,
and these actions are enumerated.
\item[\code{MPID_Flags_waitsome}, \code{MPID_Flags_testsome}.] These
are used to complete \code{MPID_Rhcv} 
and \code{MPID_Put_contig} operations (and non-core routines such as
\code{MPID_Get_contig} and \code{MPID_Put_sametype}). 
\item[\code{MPID_Poll}.] This routine provides a way for a polling routine to
be called.  A device is permitted to define this as a no-op if polling
is not needed.  In other words, this routine is provided to
\emph{allow} polling implementations, not to \emph{require} them.
Question: Is this routine still needed?
\item[\code{mpiexec}.] An implementation of \code{mpiexec}, as defined in the
  MPI-2 standard, for starting MPI jobs.  While not a routine, it must be 
  provided by any device.  
\end{description}
From these routines, together with the implementation of MPI in terms of MPID
that we are planning to carry out, all of MPI can be implemented\footnote{See
Section~\ref{sec-passive-target} for some caveats about pure polling
implementations.}.  However, this approach may sacrifice performance
for simplicity.  For increased performance, one might want to do direct
implementations of some of the non-core MPID routines, such as
\code{MPID_Isend}, directly, without relying on core routines.  For example, a
shared-memory implementation might want to rely on \code{memcpy} directly
instead of \code{MPID_Rhcv}.

This is not the only small set of routines from which MPI can be
implemented.  However, the one-sided operations defined in MPI-2 make
it difficult to use a two-sided (message-passing-like) core.
Alternately, \code{MPID_Put} isn't actually necessary; the same
functionality is provided by \code{MPID_Rhcv}.  However, experience
with active messages, as well as current trends in interconnects,
suggests that a remote memory put operation that does not require any
other processing at the target (destination) should be separated from
the more general active-message invocation (\code{MPID_Rhcv}).

\subsection{Dynamic}
For a full MPI-2 implementation, the dynamic module must also be
implemented.  That is, for a full MPI-2 implementation, the dynamic
module is really part of \code{MPID_CORE}.  This module is separated
out both because it is unnecessary for MPI-1 and because the
implementation of these routines is (usually) very different from the
communication support in \code{MPID_CORE}.  The contents of the
dynamic module include:

\begin{description}
\item[\code{MPID_Comm_Spawn_Multiple}.] Spawns new processes.
\item[\code{MPID_Port_open}.] Open a port for connecting to processes. 
\item[\code{MPID_Port_close}.] Close a port opened with \code{MPID_Port_open}.
\item[\code{MPID_Comm_connect}.] Connect to an MPI program.
\item[\code{MPID_Comm_attach}.] Attach to an MPI program.
\item[\code{MPID_Comm_disconnect}.] Detach from an MPI program.
\end{description}
An implementation that does not plan to support \code{MPI_Comm_spawn} or
any of the other MPI-2 functions in the Dynamic chapter of the MPI-2 Standard
can implement these as routines that return failure.  Such an implementation
will not implement all of MPI-2. 
%  The
%   routines \code{MPID_Open_port}, \code{MPID_Close_port},
%   \code{MPID_Comm_connect}, \code{MPID_Comm_attach}, and
%   \code{MPID_Comm_disconnect} can be considered a 
%   ``submodule'' supporting the corresponding part of the MPI-2 standard.
%   Just 
%   as for \code{MPID_Comm_spawn_multiple}, an implementation that chooses to
%   support only the subset of MPI-2 that does not include the MPI routines for
%   connecting two groups of MPI processes may implement these routines by
%   simply having them return failure.

\subsection{Design Rationale}
\label{sec-intermediate}
\label{sec-rationale}
The design of MPI puts a number of constraints on a high-quality
implementation.  These constraints help explain the rationale behind
the design of ADI-3.

We start with a few general principles.  The first is that all valid
MPI programs should work.  This implies that implementations should
maintain proper flow control and should strive not to allocate new
memory after \code{MPI_Init_thread}.  It certainly should not require
the allocatation of memory proportional to the size of a message.
This simple rule has significant consequences, starting with the
handling of datatypes.

\subsubsection{Noncontiguous Datatypes}
Any message in MPI may be defined by a datatype representing
non-contiguous locations in memory; there is no limit (other than the
usual limits of available memory) on the size of this message.  Since
no interconnects provide a way to send arbitrarily large
non-contiguous messages (and even in shared memory, the complexity of
handling the layouts of both the sending and receiving datatypes), an
ADI must provide a way to pack and unpack datatypes to and from
contiguous buffers.  
Further, because the message may be arbitrarily large, an internal
buffer,
used only to hold a contiguous form of the message, should
not be allocated for the entire message.
The consequence of this rule is the \code{MPID_Segment} object and the
routines to perform partial pack and unpack operations.

A less obvious consequence of the need to handle noncontiguous
datatypes in pieces (or segments) is that an ADI interface that
provides MPI-like point-to-point operations, but only for contiguous
data, cannot be used without significant modification.\footnote{Noncontiguous
  messages must be sent in fragments (to bound the amount of memory
  used by the implementation); reassembling these, particularly in the
  presence of multiple communicating threads, is challenging to say the least,
  since an MPI-like interface has no way to include additional data, such as a
  message sequence number or other hook that could be used to put the
  fragments back together.}

% \textbf{To see what the modifications would look like, see the old RMQ
% design that I did; it used 
% handlers within the ack-loop.} 

\subsubsection{Passive Target RMA}
\label{sec-passive-target}
\textbf{text describing passive targets as requiring a non-polling
interface and basically an active-message call.  Since passive target
requires mpid-rhcv, we put it in as a building block.}

\subsubsection{Efficient Collective Algorithms}
\begin{enumerate}
\item Store and forward --- Since the message may need to be placed in
special memory, separating out the buffer used for the final receive
(possibly after unpacking into a buffer described with non-contiguous
data type) and the memory from which the message could be forwarded is
an important goal.  Further, store and
forward operations often should be \emph{pipelined}; that is, sent in
chunks, where each chunk is forwarded it as it is received.
Just to make things more complex, in collective algorithms, the data
received is often processed (e.g., for reduce) and/or forwarded to
multiple destinations.  These needs, taken together, encourage the
definition of store and forward ``building block'' routines; the \code{Stream}
module contains these routines.

\item Scatter/Gather --- A number of algorithms rely on dividing the
message up into separate pieces, based on viewing the message as a
contiguous array of bytes, and (in some cases) gathering the pieces
up.  For example, a simple broadcast can be written as a scatter step
followed by an Allgather step.  To write these for arbitrary (i.e.,
noncontiguous) datatypes requires the ability to divide the message up
independent of the datatype.  

\item Blocking --- An advantage of the (non File) collective
operations is that they are blocking.  This allows the code
implementing the collective operation to remain in control; in
particular, it can wait for an operation to complete and then perform
the next step.  This feature influenced the design of the
\code{Stream} module.
\end{enumerate}

\subsubsection{Multi-method Communication}
An important implementation of the this ADI is the model multi-method
implementation, based on separate message queues for each method.
This device illustrates a more complex use of the ADI interface.  It
implements not only the MPID CORE but most of the point-to-point
routines as well, along with the collective support routines.  The
generic modules are used for the rest of the ADI implementation.

To make it possible to implement this kind of multi-method device, the
ADI is careful with how MPI requests are handled.  It also defined a
very high-level ADI interface (e.g., \code{MPID_Isend}) that gives the
device the flexibility to determine the appropriate method and allows
the method to determine the appropriate protocol for delivering a message.

\subsubsection{Polling}
ADI-3 has been designed (like ADI-1 and ADI-2) to permit both polling and
non-polling (e.g., interrupt-driven or thread-scheduled) implementations.
Thus, some routines are provided that provide convenient points for a polling
implementation to poll or wait.  \code{MPID_Flags_waitsome} and
\code{MPID_Test_waitsome} are examples.

It is the responsibility of the implementation of the ADI to ensure that
progress is made on all communication, particularly passive-target RMA
operations.  This makes it nearly impossible to support an implementation
based entirely on polling.  However, many operations can use polling to reduce
the costs associated with context switches, in combination with a nonpolling
method.

\subsection{Contrast with ADI-1 and ADI-2}
\label{sec-historical}
\textbf{This section will describe differences in concept and goals}

In ADI-1 and ADI-2, non-contiguous datatypes were handled by first
allocating a buffer large enough to hold the \emph{entire} message and
then either packing the message into that buffer (for a send) or
unpacking it after it was delivered (for a receive).  This was a
reasonable expedient when the original MPICH implementation was done,
but is not acceptable in a mature package.

\texttt{MPID_CORE} in ADI-3 corresponds to the `channel' device in ADI-2 in
the sense that it is a small set of routines, which, when combined with the
generic implementations of the other modules, gives a complete ADI
implementation.  

The ADI-2 channel interface was designed to fit the high-performance system
software of the day, which was proprietary message-passing libraries on
distributed memory parallel computers.  With the success of MPI, these
proprietary libraries have vanished, and advances in interconnect technology
have changed the API used to access high-performance interprocess
communication.  ADI-3 adapts to these changes.


\subsection{Our Plans for Using ADI-3 to Implement MPI-2}
\label{sec:plans}

%
% Replace ``Channel Device'' with ``Rhcv Device'' since the new core
% is not based on communication channels but is more of an active message
% core
%
This subsection describes our implementation plans at Argonne.  The next
subsection describes how collaborators (vendors, researchers, and others) can
use the structure of ADI-3 to leverage our implementation work and their own
high-speed communication methodologies to produce a particular MPI
implememtation.

As discussed above, ADI-3 contains a large number of functions in order to
accomodate collaborators who want to themselves optimize a large part of the
data structures and algorithms needed in a complete MPI implementation.  At the
same time there is a much smaller number of routines, called the ``ADI Core''
in this document, that capture that part of the ADI that is closest to the
communication method. It is possible (but not required) that the full ADI be
implemented in terms of the core.

We intend to implement all of MPI in terms of the full ADI-3 interface (not
just the core).  Since the full ADI-3 is relatively rich, this will be the
simplest part of the implementation.  The most difficult part will be the
implementation of the collective operations, which will be in terms of some of
the low-level ADI-3 functions, not in terms of the MPI point-to-point
operations or their ADI-3 ``equivalents''.

We then intend to provide two implementations of the full ADI-3 interface.
The first, already under way, is what we call the RMQ Device.  It will be 
multimethod, multi-threaded, and will be optimized for peak performance within
those constraints.  Inside this device implementation there will be a furhter
interface level, not yet defined, allowing new methods to be incorporated into
the RMQ device in addition to the methods we will initially support:  TCP/IP,
shared memory, and VIA.  We also intend to implement another device, called 
the Rhcv Device, which will consist of an implementation of all of ADI-3 in
terms of the small number of core functions.  We will also supply a number of
sample implementations of the core.  It is expected that the RMQ Device will
be in some cases faster than the Rhcv Device, for example in the shared
memory case, where the core routines may not provide the optimal communication
model. 


\subsection{How Collaborators May Participate}
\label{sec:collaborators}

Thus there are several options for collaborators, depending on their
objectives.
\begin{enumerate}
\item A collaborator might implement all of ADI-3 in his own way, using his
  own data structures, handling datatypes, etc., but taking advantage of our
  collective operations, for example, and argument-checking checking code in
  the MPI-over-ADI-3 implementation.
\item A collaborator might implement only the routines in MPI Core, taking
  advantage of our data structures for communicators, datatypes, and other
  objects making up our Rhcv Device implementation.
\item A collaborator might do something in between the above two options,
  using only parts of the Rhcv Device implementation.
\item A collaborator might add a method to the RMQ Device, using its internal
  interface and library routines, and not implement any of the ADI-3 functions.
\end{enumerate}

\section{Things Left to do}
What relationship does the ADI interface to dynamic processes 
have to the BNR interface?  Should there be a connection between BNR groups
and MPID groups?


\code{MPID_Join} involves sockets, so we might make it part of the
utility module.  But what routines from \code{MPID_Core} must it call
to connect up the processes?

%Error reporting.  The ADI needs to know how to create MPI error codes.

%\section{ADI-3 Routines}
%\input adi3func.tex

%\section{ADI-3 Datastructures}
%\input adi3data.tex


\section{Integrating a Device into the MPICH build tree}
Still to do.  This section needs to cover:
\begin{enumerate}
\item Directory structure.  Where will MPICH look for a new device?
\item Local configure/setup script.  How should the top-level MPICH configure
  invoke the device-specific setup scripts?
\item mpiexec.  How does MPICH create the correct mpiexec?
\item Installation.  How does MPICH get the proper device-specific files
  installed; e.g., the mpd for the mpd device?
\item Device-specific documentation, such as environment variables and
  command-line arguments used only by a particular device.
\item Testing codes for device-specific functions.
\end{enumerate}

\section{Motivation}
Why is the ADI so complex?  Consider the following scenarios:
\begin{enumerate}
\item Consider
\begin{verbatim}
    MPI_Irsend( MPI_BOTTOM, 1, my_indexed_datatype, ... )
\end{verbatim}
where the datatype describes 50MB of data, each entry of which is a
structure.
\item Consider
\begin{verbatim}
    MPI_Accumulate( MPI_BOTTOM, 1, my_indexed_datatype, ..., 
       my_vector_type, ..., MPI_MAXLOC )
\end{verbatim}
where \code{my_vector_type} is
a vector datatype of structures, and the amount of data is 50MB, and
the access is passive target (the target process makes no MPI calls,
but the operation must complete).
\item 
\end{enumerate}

Any of these may be implemented using the MPID CORE interface.
However, that interface may not provide the best efficiency.  Consider
the case of a simple send and receive of contiguous data.  In a system
providing cache-coherent shared-memory hardware, it may be possible to
implement this by having the sender directly manipulate the queues of
the receiver, including delivering the data.  The MPID CORE
implementation cannot express that approach, so a more powerful (but
more specific and thus less general) interface is also provided.  Of
course, for maximum flexibility, an implementor can bypass the ADI and
implement the MPI routines directly, perhaps taking advantage of the
parameter checking code in the MPICH2 implementation.

% Add manual entries to the table of contents
\mancontentstrue

\section{Overview}
\input Overview.tex
\input OpaqOverview.tex

\subsection{Data Structures and Constants}
\input DSOverview.tex
%\input Constants.tex
\input DyOverview.tex

\section{Summary of MPID Routines by Module}

See the table of contents for an up-to-date-list.
% %
% % I find this section very dangerous until and unless it is updated
% % automatically.  For example, MPID_Irecv was added some time ago, but
% % since it wasn't in the list, we talked about why it might be needed.
% %
% \textbf{THE FOLLOWING LIST IS CURRENTLY UPDATED BY HAND AND IS THUS OUT OF
%   DATE.  CHECK THE CONTENTS PAGE FOR A LIST OF ROUTINES}

% \subsection{MPID Core}
% \begin{verbatim}
%     MPID_Abort
%     MPID_Finalize
%     MPID_Flags_testsome
%     MPID_Flags_waitsome
%     MPID_Put_contig
%     MPID_Rhcv
%     MPID_Init
% \end{verbatim}
% %    MPID_Hid_Cancel (handler)
% %    MPID_Hid_Request_to_send (handler)

% \subsection{Dynamic}
% \begin{verbatim}
%     MPID_Comm_accept
%     MPID_Comm_connect
%     MPID_Comm_disconnect
%     MPID_Comm_spawn_multiple
%     MPID_Port_close
%     MPID_Port_open
% \end{verbatim}

% \subsection{Handlers}
% %\begin{verbatim}
%     Need enumeration of non-core handlers for \code{MPID_Rhcv}.  Perhaps these 
%     belong in various modules.
% %\end{verbatim}

% \subsection{Attributes}
% \begin{verbatim}
%     MPID_Attr_delete
%     MPID_Attr_find
%     MPID_Attr_list_walk
%     MPID_Attr_predefined
%     MPID_Comm_attr_notify
% \end{verbatim}
% %   MPID_Lang_t


% \subsection{Datatypes}
% \begin{verbatim}
%     MPID_Datatype_free
%     MPID_Datatype_incr
%     MPID_Datatype_new
%     MPID_Pack_size
%     MPID_Pack
%     MPID_Unpack
%     MPID_Segment_free
%     MPID_Segment_init_pack
%     MPID_Segment_init_unpack
%     MPID_Segment_pack
%     MPID_Segment_unpack
% \end{verbatim}
% %    MPID_Datatype
% %    MPID_Segment

% \subsection{Groups}
% \begin{verbatim}
%     MPID_Group_free
%     MPID_Group_incr
%     MPID_Group_new
% \end{verbatim}
% %    MPID_Group
% %    MPID_Lpidmask

% \subsection{Communicators}
% \begin{verbatim}
%     MPID_Comm_create
%     MPID_Comm_free
%     MPID_Comm_incr
%     MPID_Comm_thread_lock
%     MPID_Comm_thread_unlock
% \end{verbatim}
% %    MPID_Comm

% \subsection{Requests}
% \begin{verbatim}
%     MPID_Request_cancel
%     MPID_Request_send_FOA
%     MPID_Request_recv_FOA
%     MPID_Request_free
%     MPID_Request_iprobe
%     MPID_Request_new
%     MPID_Request_ready
% \end{verbatim}
% % MPID_Request

% \subsection{Communication}
% \begin{verbatim}
%     MPID_Flags_testall
%     MPID_Flags_waitall
%     MPID_Get_contig
%     MPID_Isend
%     MPID_Irecv
%     MPID_Irsend
%     MPID_Issend
%     MPID_Testsome
%     MPID_Waitsome
%     MPID_Memory_register
%     MPID_Memory_unregister
%     MPID_tBsend
% \end{verbatim}

% \subsection{Streams}
% \begin{verbatim}
%     MPID_Stream_iforward
%     MPID_Stream_irecv
%     MPID_Stream_isend
%     MPID_Stream_wait
% \end{verbatim}
% %MPID_Stream

% \subsection{Window Objects}
% \begin{verbatim}
%     MPID_Mem_alloc
%     MPID_Mem_free
% \end{verbatim}
% %    MPID_Win

% \subsection{Timers}
% \begin{verbatim}
%     MPID_Gwtick
%     MPID_Gwtime_init
%     MPID_Gwtime_diff
%     MPID_Wtime_init
%     MPID_Wtick
%     MPID_Wtime_diff
%     MPID_Wtime
% \end{verbatim}

% \subsection{Topology}
% \begin{verbatim}
%     MPID_Topo_cluster_info
% \end{verbatim}

% \subsection{Utility}
% \begin{verbatim}
%     MPID_Calloc
%     MPID_Free
%     MPID_Malloc
%     MPID_Memcpy
%     MPID_Strdup
% \end{verbatim}

% \subsection{Environment}
% \begin{verbatim}
%     MPID_MAX_THREAD_LEVEL
%     MPID_THREAD_LEVEL
% \end{verbatim}

% \subsection{Error Reporting}
% \begin{verbatim}
%     MPID_Err_create_code
%     MPID_Err_get_string
%     MPID_Err_set_msg
%     MPID_Err_add_class
%     MPID_Err_add_code
%     MPID_Err_delete_code
%     MPID_Err_delete_class
% \end{verbatim}

\section{MPID Core}
\input MPID_CORE-list.tex
%\input Handlers.tex

\section{mpiexec}
%\textbf{Need some discussion of mpiexec}
This section has not been written yet.  It will cover
\begin{enumerate}
\item How each device's \code{mpiexec} is built (since it may be a shell
  script or program)
\item How \code{mpiexec} is installed, and how any data files (e.g., lists of
  hosts) are created
\item Any commands or programs that must be run before \code{mpiexec} may be
  used, along with programs to undo that (e.g., \code{lamboot} and
  \code{lamkill}). 
\item How special information is passed between \code{mpiexec} and the MPI
  program, including how to set the attributes like \code{MPI_APPNUM} and
  whether processes have command-line arguments and environment variables
  provided by \code{mpiexec} (if not, \code{MPI_Init} may want to provide
  them). 
\end{enumerate}

In addition, there are other parts to the behavior of \code{mpiexec}
that are not covered in the MPI standard.  For example, how are the
return codes from individual MPI processes handled?  Some
possibilities are:
\begin{description}
\item The maximum value of all return codes.
\item The sum of all return codes.  Note that there may be
a limit of 255 imposed by the operating system.
\item The number of non-zero return codes.
\item A bit vector that indicats which processes had non-zero return
codes.
\end{description}

\section{Attributes}
\input AttrOverview.tex
\subsection{Keyval functions}
\input KyOverview.tex
\input InfoOverview.tex
\input Attribute-list.tex

\section{Datatypes}
Datatypes are a key part of MPI; they provide a great deal of expressive
power.  They also are difficult to implement efficiently; see 
\cite{gropp-swider-lusk99,Traeff:1999:FFE} for some techniques for speeding
the handling of datatypes.
In addition to the requirements described in Section~\ref{sec-rationale}, 
MPI-2 requires that it be possible to return the arguments that the user
used in creating the datatype.  This requires that, even if the datatype can
be simplified for the purposes of data motion (e.g., pack and unpack), 

Datatypes are constructed by the implementation of the MPI routines such as 
\code{MPI_Type_create_struct}.  That routine will call
\code{MPID_Datatype_new} to acquire a structure and will then set the fields
in that structure appropriately. 

\input Datatype-list.tex
\input SGOverview.tex
\input Segment-list.tex

\section{Groups}
\input LocalPID.tex
\input Group-list.tex

\section{Communicators}
\input Communicator-list.tex

\section{Dynamic}
\input Dynamic-list.tex

\section{Requests}
\input Request-list.tex

\section{Communication}
\input CMOverview.tex
\input Communication-list.tex

\section{Streams}
\input StmOverview.tex
\input Stream-list.tex

\section{Collective}
%\input CollOverview.tex
\input Collective-list.tex

\section{Window Objects}
\input Win-list.tex

\section{Timers}
\input Timer-list.tex

\section{Topology}
\input TopoOverview.tex
\input Topology-list.tex

\section{Utility}
\input Memory.tex
\input Utility-list.tex 

\section{Environment}
\input Environment-list.tex

\section{Error Reporting}
MPI allows separate error codes and classes.  An error \emph{class} is
a predefined value that roughly corresponds to a Unix error code
(e.g., \code{E_NOMEM}).  From a user's perspective, the major problem
with a simple error code (class in MPI terms) is that it is a
\emph{generic} message that gives no information about the particular
cause of the error.  For example, the Unix error code \code{E_NOMEM}
only tells the user that no memory is available.  The user might
prefer a message that included the amount of memory requested, such as 
\begin{verbatim}
Requested memory unavailable (requested 104732811 bytes 
in file getmem.c, line 137).
\end{verbatim}
The MPI error code provides a way to provide this kind of detailed
information.  An MPI error code may be though of as an
augmented error class that allows the MPI library to provide more
detailed information about an error.  MPICH uses error codes for this
purpose.  

In MPICH, an error code has three components:

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
instance id&base code&base class\\
\hline
\end{tabular}
\end{center}
where the fields are
\begin{description}
\item[\texttt{base class}]The MPI error class, from the defined classes
\item[\texttt{base code}]Specifies more specific error message than
the \code{base class} but which contains no instance-specific data
\item[\texttt{instance id}]Used to indicate an instance-specific message
\end{description}

Values of zero for the \code{base code} and \code{instance id} are
always valid; in that case the error code is simply the error class.

The values encoded in \code{instance id} are defined by
\code{MPID_Err_create_code}; the routine \code{MPID_Err_get_string}
understands the \code{instance id} field.  One possible implementation
of this is described in (technical note not yet written, but described
in ALICE brown bag talk and Coding Standards document), and is
included with the MPICH implementation.

Messages are \code{char *} instead of \code{wchar_t *} because (a)
strings in C are \code{char *} and error strings in MPI are \code{char
*}.  However, we'd like to support Unicode messages.  We may just
return \code{char *} always, and allow the user to interpret the
message as either \code{char} or \code{wchar_t}.

\input Error-list.tex

\subsection{Error Handling}
\label{sec:error-handling}

When an error occurs, it may be necessary to take error recovery
actions.  In many MPI-1 implementations, error actions are often
limited to (a) return an error code or (b) abort MPI program.
Case (a) is taken for simple errors such as invalid parameters to a
routine while case (b) is taken for cases such as communication
failure.  

While these are acceptable responses to errors in an MPI
implementation, we would like to support a more flexible error
handling approach, without adding complexity to all of the code.
\begin{enumerate}
\item Local, non-fatal errors.  Return an error code using the error
code routines.
\item Local, fatal errors (such as SEGV).  Call \code{MPID_Abort}.
\item Nonlocal, non-fatal errors.  There are two cases:
   \begin{enumerate}
   \item Communication failure to a process.  Call
       \code{MPID_Err_link}.
   \item Notification received that communication to a process in a 
       communicator shared with this process has failed.  Call 
       \code{MPID_Err_partner}.
   \end{enumerate}
\end{enumerate}
By default, \code{MPID_Err_link} and \code{MPID_Err_partner} simply
call \code{MPID_Abort}.  However, a ADI implementation that is
tolerant of communication failure can use these to 

%\input Error-handling-list.tex

\section{Extensions}
\input Extension-list.tex

\subsection{Other Extensions}
In some applications, it isn't necessary to communicate floating point values
to their full precision.  Some groups (see, for example,
\url{http://nemo.physics.ncsu.edu/~briggs/software}) have experimented with 40
and 48 bit versions of 64 bit floating point data.  For slow networks (and
slow memory systems), this can lead to significant communication performance
improvements.  This may seem like a niche extension, but MPI almost specifies
this for I/O in the form of
\code{MPI_Register_datarep}\findex{MPI_Register_datarep}.   

\openin\testfile{None-list.tex}
\ifeof\testfile\else
\section{Miscellaneous}
\input None-list.tex
\fi
\closein\testfile

\let\SaveBibliography=\thebibliography
\def\thebibliography#1{\SaveBibliography{#1}\addcontentsline{toc}{section}{References}}
\bibliography{/home/MPI/allbib,/home/gropp/Update/new/gropp}
\bibliographystyle{plain}

% Index
%\openin\testfile{adi3man.ind}
%\ifeof\testfile\else
\let\SaveIndex=\theindex
\def\theindex{\SaveIndex\addcontentsline{toc}{section}{Index}}
\input adi3man.ind
%\fi
%\closein\testfile

\end{document}
