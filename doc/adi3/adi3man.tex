% 
%   This is a latex file that generates a reference manual for 
%   ADI-3 
%
\documentclass{article}
\usepackage{/home/gropp/data/share/refman}
\usepackage{/home/gropp/sowing-proj/sowing/docs/doctext/tpage}
\usepackage{url} 
\usepackage{epsf}
\textheight=9in
\textwidth=6.1in
\oddsidemargin=.2in
\topmargin=-.50in
\newread\testfile

%
% For now, let findex be the same as index.  This will allow us to
% more easily separate function and nonfunction index entries later.
\let\findex=\index
%
% Modify the way titles are handled for no breaks between pages
\def\mantitle#1#2#3{\pagerule\nobreak
\ifmancontents\addcontentsline{toc}{subsection}{#1}\fi
\index{#1}}

\makeindex

\begin{document}

\markright{ADI-3 Reference Manual}

\def\nopound{\catcode`\#=13}
{\nopound\gdef#{{\tt \char`\#}}}
\catcode`\_=13
\def_{{\tt \char`\_}}
\catcode`\_=11
\def\code#1{{\tt #1}}
%\let\url=\code
\def\makeussubscript{\catcode`\_=8}
\def\makeustext{\catcode`\_=11}
%\tpageoneskip
\ANLTMTitle{MPICH Abstract Device Interface\\
Version 3\\
Reference Manual\\\ \\Draft of \today}{\em 
William Gropp\\
Ewing Lusk\\
Mathematics and Computer Science Division\\
Argonne National Laboratory}{00}{\today}

\clearpage

\pagenumbering{roman}
\tableofcontents
\clearpage

\pagenumbering{arabic}
\pagestyle{headings}

\section{Introduction}
This document contains detailed documentation on the routines that are part of
the Abstract Device Interface, version 3, used to implement the MPICH2000
model MPI implementation. 

This is a draft document.  No part of it should be considered as final.  All
parts are currently under discusssion, and comments are welcome.  In
particular, no decision has been made even on the choice of functions, much
less their particular argument lists or semantics.  This document should be
viewed as a starting point for discussions that presents one possible ADI-3
design. 

% As an alternate to this manual, the reader should consider using the
% script \code{mpiman}; this is a script that uses \code{xman} to provide
% a X11 Window System interface to the data in this manual.

\section{Discussion}
The ADI contains a large number of routines, but only a few of these
are related to the lowest-level communication operations.  Most of the
rest are used to provide opportunities for performance optimization
(e.g., \code{MPID_Isend}) or to support the objects that MPI
provides and that a device implementation may (or may not) need to
understand (e.g., MPI attributes and groups).  

The ADI is organized as a number of separate modules.
For each module except for the core communication module (called MPID
CORE), there is a sample (but complete) implementation that may be used to
build other implementations of the ADI.  These sample
implementations are refered to as the \emph{generic} module
implementations. For example,
the ADI contains routines to create and manipulate MPI communicators;
most implementations of the ADI will use the generic module for
handling communicators.  Other modules, such as the point-to-point
communications module, are more likely to be replaced with a
device-specific implementation.  Even in the cases where an
implementation of the ADI replaces a module, the generic
implementation of that module will often serve as a guide.
As a guide to implementing the MPID CORE, we provide several implementations
for popular interprocess communication mechanisms, including TCP.

There are roughly three classes of modules:
\begin{enumerate}
\item The core: basic process creation and communication. This
contains one module, MPID CORE. 
\item Support for MPI objects.  These modules support the various MPI
objects such as Datatypes, Groups, Communicators, and keyvals.
\item Communication optimization.  These modules provide an
intermediate level of functionality between that provided by MPID CORE
and that provided by MPI.  The rationale for these are described in
Section~\ref{sec-intermediate}. 
\end{enumerate}

The modules are (THIS LIST IS INCOMPLETE):
\begin{description}
\item[MPID CORE.] Basic functions to start, stop, and communicate
between processes.  This also includes the \code{mpiexec} program.
This is the only module for which there is no generic implementation; as
described in Section~\ref{sec-minimal}, implementing just this module,
combined with the generic implementation of the other modules, provides a
complete ADI-3 implementation.
\item[Attribute.] MPI Keyvals and attributes
\item[Datatype.] MPI Datatypes and pack/unpack operations and a new
object, \code{MPID_Segment}, used only within the ADI.
\item[Dynamic.] Support for MPI-2 dynamic process operations.  This
module must be implemented to support the dynamic process chapter of MPI-2.
\item[Group.] MPI Groups
\item[Communicator.] MPI Communicators
\item[Request.] MPI Requests.  Requests are fundemental to many
operations; most devices that implement more than MPID CORE will need
to replace this module.
\item[Window.] MPI Windows.
\item[Point to point]Functions directly implementing the MPI
point-to-point communication functions or some key subsets.
\item[Topology.]Information on the physical interconnect.  Currently
limited to hierarchical information necessary to support clusters of
SMPs and similar systems.
\item[Timer.] Support for fast timers, including synchronized timers.
\item[Stream.] Communication routines designed to support
important algorithms for collective communication.
\item[Collective.] Alternate support for collective
routines. Note that this may end up being an MPI module, not
an MPID module, exploiting the routines in the \textbf{Stream} module.
\item[Utility.] Routines to simplify correctness and performance
debugging, and to provide a common interface to OS services that have
different interfaces on different platforms.
\item[Environment.] This module contains both compile-time and run-time
constants that describe the environment. 
\item[Communication.] The routines that carry out communication among MPI
  processes. 
\item[Error Reporting.] This provides support for detailed error
reporting.  Most implementations will use the generic implementation
of this module.
\item[Extensions.] This module contains no required routines; instead,
it documents some of the various extensions that the MPICH group is
considering, such as checkpointing.
\end{description}

By dividing the ADI into these modules, it becomes easy to implement
MPI quickly by implementing just MPID CORE and using the generic
modules for everything else.  Other modules can be replaced as
warranted; for example, a new machine will likely need a new Timers
module in order to access a system-specific timer.  Implementors
trying to get the maximum performance for point-to-point operations
may want to replace that module with one carefully tuned to their
specific platform.

Not included in this list is the ADIO module used to support MPI-IO.
See \cite{ThakurGroLus96} and \gb\url{http://www.mcs.anl.gov/~thakur/adio} 
%\cite{romio-adio-manual} 
%(ROMIO ADIO Manual)
for a description of that module.

\subsection{The Core}
\label{sec-minimal}

While ADI-3 defines a large number of routines, only a few must be implemented
by a developer.  For all modules except the \code{MPID_CORE} module, MPICH
provides a generic implementation.  Thus, only the routines in
\code{MPID_CORE} must be implemented when porting MPICH to a new interprocess
communication mechanism.  To further simplify the task of implementing MPI
with ADI-3, the core does not contains the routines needed to implement the
I/O or dynamic process chapters of the MPI Standard (it does contain the
routines needed to implement the remote memory access chapter).  The
\code{MPID_CORE} consists of the following routines:  

\begin{description}
\item[\code{MPID_Init}.] This is the device's counterpart to
\code{MPI_Init_thread}. 
\item[\code{MPID_Finalize}.] This is the devices's counterpart to
\code{MPI_Finalize}
\item[\code{MPID_Abort}.] This is the device's counterpart to \code{MPI_Abort}.
\item[\code{MPID_Put_contig}.] This is used for data movement; it is roughly an
\code{MPI_Put} using only contiguous data.  However, it uses a
different mechanism for indicating completion of data transfers (see \code{MPID_Flags_waitsome} and \code{MPID_Flags_testsome})
\item[\code{MPID_Rhcv}.] This is used for messages; it is roughly an
active-message call.  However, only predefined actions are allowed,
and these actions are enumerated.
\item[\code{MPID_Flags_waitsome}, \code{MPID_Flags_testsome}.] These
are used to complete \code{MPID_Rhcv} 
and \code{MPID_Put_contig} operations (and non-core routines such as
\code{MPID_Get_contig} and \code{MPID_Put_sametype}). 
\item[\code{MPID_Poll}.] This routine provides a way for a polling routine to
be called.  A device is permitted to define this as a no-op if polling
is not needed.  In other words, this routine is provided to
\emph{allow} polling implementations, not to \emph{require} them.
This routine also provides a way for an MPI implementation that used
ADI3 to express ``polling point,'' locations in the code where it
would be a helpful to poll in a polling implementation.  Even a
non-polling implementation may use these points to check for
communication, allowing a clever implementation to avoid the cost of a
context switch in some cases. 
\item[\code{mpiexec}.] An implementation of \code{mpiexec}, as defined in the
  MPI-2 standard, for starting MPI jobs.  While not a routine, it must be 
  provided by any device.  
\end{description}
From these routines, together with the implementation of MPI in terms of MPID
that we are planning to carry out, all of MPI can be implemented\footnote{See
Section~\ref{sec-passive-target} for some caveats about pure polling
implementations.}.  However, this approach may sacrifice performance
for simplicity.  For increased performance, one might want to do direct
implementations of some of the non-core MPID routines, such as
\code{MPID_Isend}, directly, without relying on core routines.  For example, a
shared-memory implementation might want to rely on \code{memcpy} directly
instead of \code{MPID_Rhcv}.

This is not the only small set of routines from which MPI can be
implemented.  However, the one-sided operations defined in MPI-2 make
it difficult to use a two-sided (message-passing-like) core.
Alternately, \code{MPID_Put_contig} isn't actually necessary; the same
functionality is provided by \code{MPID_Rhcv}.  However, experience
with active messages, as well as current trends in interconnects,
suggests that a remote memory put operation that does not require any
other processing at the target (destination) should be separated from
the more general active-message invocation (\code{MPID_Rhcv}).

\subsubsection{The Role of the CORE}
The \code{MPID_CORE} is intended to provide a small set of routines
with which all of MPI can be implemented.  These routines are
\emph{not} intended as the only approach for implementing the full
ADI.  In particular, the remote handler routine, \code{MPID_Rhcv}, is
not directly by any implementation of the MPI routines.  That is, the
implementation of the MPI routines will call routines from various ADI
modules; it is the implementation of the ADI routines \emph{for the
core device} that use \code{MPID_Rhcv}.  It is possible to implement
the ADI without implementing \code{MPID_Rhcv} by implementing all of
the other ADI modules.

\textbf{Important Change.}  The current design uses different though
related approach for the basic send and receive operations.  This
involves the construction of a transfer object, which is really a
collection of data blocks to move, along with some control over how
they are moved.  This is similar to the \code{MPID_Rhcv} interface,
but opens up the construction of the list of data to move so the more
complex communications can be described.  This is important for
optimizing collective communication.  It is a slighly different
solution to the approach used in the second version of Illinois Fast
Messages \cite{pakinxx}; FM provides a send-fragment operation.

\subsection{Dynamic}
For a full MPI-2 implementation, the dynamic module must also be
implemented.  That is, for a full MPI-2 implementation, the dynamic
module is really part of \code{MPID_CORE}.  This module is separated
out both because it is unnecessary for MPI-1 and because the
implementation of these routines is (usually) very different from the
communication support in \code{MPID_CORE}.  The contents of the
dynamic module include:

\begin{description}
\item[\code{MPID_Comm_Spawn_Multiple}.] Spawns new processes.
\item[\code{MPID_Port_open}.] Open a port for connecting to processes. 
\item[\code{MPID_Port_close}.] Close a port opened with \code{MPID_Port_open}.
\item[\code{MPID_Comm_connect}.] Connect to an MPI program.
\item[\code{MPID_Comm_attach}.] Attach to an MPI program.
\item[\code{MPID_Comm_disconnect}.] Detach from an MPI program.
\end{description}
An implementation that does not plan to support \code{MPI_Comm_spawn} or
any of the other MPI-2 functions in the Dynamic chapter of the MPI-2 Standard
can implement these as routines that return failure.  Such an implementation
will not implement all of MPI-2. 
%  The
%   routines \code{MPID_Open_port}, \code{MPID_Close_port},
%   \code{MPID_Comm_connect}, \code{MPID_Comm_attach}, and
%   \code{MPID_Comm_disconnect} can be considered a 
%   ``submodule'' supporting the corresponding part of the MPI-2 standard.
%   Just 
%   as for \code{MPID_Comm_spawn_multiple}, an implementation that chooses to
%   support only the subset of MPI-2 that does not include the MPI routines for
%   connecting two groups of MPI processes may implement these routines by
%   simply having them return failure.

\subsection{Design Rationale}
\label{sec-intermediate}
\label{sec-rationale}
The design of MPI puts a number of constraints on a high-quality
implementation.  These constraints help explain the rationale behind
the design of ADI-3.

We start with a few general principles.  The first is that all valid
MPI programs should work.  This implies that implementations should
maintain proper flow control and should strive not to allocate new
memory after \code{MPI_Init_thread}.  It certainly should not require
the allocatation of memory proportional to the size of a message.
This simple rule has significant consequences, starting with the
handling of datatypes.

\subsubsection{Noncontiguous Datatypes}
Any message in MPI may be defined by a datatype representing
non-contiguous locations in memory; there is no limit (other than the
usual limits of available memory) on the size of this message.  Since
no interconnects provide a way to send arbitrarily large
non-contiguous messages (and even in shared memory, the complexity of
handling the layouts of both the sending and receiving datatypes), an
ADI must provide a way to pack and unpack datatypes to and from
contiguous buffers.  
Further, because the message may be arbitrarily large, an internal
buffer,
used only to hold a contiguous form of the message, should
not be allocated for the entire message.
The consequence of this rule is the \code{MPID_Segment} object and the
routines to perform partial pack and unpack operations.

A less obvious consequence of the need to handle noncontiguous
datatypes in pieces (or segments) is that an ADI interface that
provides MPI-like point-to-point operations, but only for contiguous
data, cannot be used without significant modification.\footnote{Noncontiguous
  messages must be sent in fragments (to bound the amount of memory
  used by the implementation); reassembling these, particularly in the
  presence of multiple communicating threads, is challenging to say the least,
  since an MPI-like interface has no way to include additional data, such as a
  message sequence number or other hook that could be used to put the
  fragments back together.}

% \textbf{To see what the modifications would look like, see the old RMQ
% design that I did; it used 
% handlers within the ack-loop.} 

\subsubsection{Passive Target RMA}
\label{sec-passive-target}
\textbf{text describing passive targets as requiring a non-polling
interface and basically an active-message call.  Since passive target
requires mpid-rhcv, we put it in as a building block.}

\subsubsection{Efficient Collective Algorithms}
\begin{enumerate}
\item Store and forward --- Since the message may need to be placed in
special memory, separating out the buffer used for the final receive
(possibly after unpacking into a buffer described with non-contiguous
data type) and the memory from which the message could be forwarded is
an important goal.  Further, store and
forward operations often should be \emph{pipelined}; that is, sent in
chunks, where each chunk is forwarded it as it is received.
Just to make things more complex, in collective algorithms, the data
received is often processed (e.g., for reduce) and/or forwarded to
multiple destinations.  These needs, taken together, encourage the
definition of store and forward ``building block'' routines; the \code{Stream}
module contains these routines.

\item Scatter/Gather --- A number of algorithms rely on dividing the
message up into separate pieces, based on viewing the message as a
contiguous array of bytes, and (in some cases) gathering the pieces
up.  For example, a simple broadcast can be written as a scatter step
followed by an Allgather step.  To write these for arbitrary (i.e.,
noncontiguous) datatypes requires the ability to divide the message up
independent of the datatype.  

\item Blocking --- An advantage of the (non File) collective
operations is that they are blocking.  This allows the code
implementing the collective operation to remain in control; in
particular, it can wait for an operation to complete and then perform
the next step.  This feature influenced the design of the
\code{Stream} module.
\end{enumerate}

\subsubsection{Multi-method Communication}
An important implementation of the this ADI is the model multi-method
implementation, based on separate message queues for each method.
This device illustrates a more complex use of the ADI interface.  It
implements not only the MPID CORE but most of the point-to-point
routines as well, along with the collective support routines.  The
generic modules are used for the rest of the ADI implementation.

To make it possible to implement this kind of multi-method device, the
ADI is careful with how MPI requests are handled.  It also defined a
very high-level ADI interface (e.g., \code{MPID_Isend}) that gives the
device the flexibility to determine the appropriate method and allows
the method to determine the appropriate protocol for delivering a message.

\subsubsection{Polling}
ADI-3 has been designed (like ADI-1 and ADI-2) to permit both polling and
non-polling (e.g., interrupt-driven or thread-scheduled) implementations.
Thus, some routines are provided that provide convenient points for a polling
implementation to poll or wait.  \code{MPID_Flags_waitsome} and
\code{MPID_Test_waitsome} are examples.

It is the responsibility of the implementation of the ADI to ensure that
progress is made on all communication, particularly passive-target RMA
operations.  This makes it nearly impossible to support an implementation
based entirely on polling.  However, many operations can use polling to reduce
the costs associated with context switches, in combination with a nonpolling
method.

\subsection{Contrast with ADI-1 and ADI-2}
\label{sec-historical}
\textbf{This section will describe differences in concept and goals}

In ADI-1 and ADI-2, non-contiguous datatypes were handled by first
allocating a buffer large enough to hold the \emph{entire} message and
then either packing the message into that buffer (for a send) or
unpacking it after it was delivered (for a receive).  This was a
reasonable expedient when the original MPICH implementation was done,
but is not acceptable in a mature package.

\texttt{MPID_CORE} in ADI-3 corresponds to the `channel' device in ADI-2 in
the sense that it is a small set of routines, which, when combined with the
generic implementations of the other modules, gives a complete ADI
implementation.  

The ADI-2 channel interface was designed to fit the high-performance system
software of the day, which was proprietary message-passing libraries on
distributed memory parallel computers.  With the success of MPI, these
proprietary libraries have vanished, and advances in interconnect technology
have changed the API used to access high-performance interprocess
communication.  ADI-3 adapts to these changes.


\subsection{Our Plans for Using ADI-3 to Implement MPI-2}
\label{sec:plans}

%
% Replace ``Channel Device'' with ``Rhcv Device'' since the new core
% is not based on communication channels but is more of an active message
% core
%
This subsection describes our implementation plans at Argonne.  The next
subsection describes how collaborators (vendors, researchers, and others) can
use the structure of ADI-3 to leverage our implementation work and their own
high-speed communication methodologies to produce a particular MPI
implememtation.

As discussed above, ADI-3 contains a large number of functions in order to
accomodate collaborators who want to themselves optimize a large part of the
data structures and algorithms needed in a complete MPI implementation.  At the
same time there is a much smaller number of routines, called the ``ADI Core''
in this document, that capture that part of the ADI that is closest to the
communication method. It is possible (but not required) that the full ADI be
implemented in terms of the core.

We intend to implement all of MPI in terms of the full ADI-3 interface (not
just the core).  Since the full ADI-3 is relatively rich, this will be the
simplest part of the implementation.  The most difficult part will be the
implementation of the collective operations, which will be in terms of some of
the low-level ADI-3 functions, not in terms of the MPI point-to-point
operations or their ADI-3 ``equivalents''.

We then intend to provide two implementations of the full ADI-3 interface.
The first, already under way, is what we call the RMQ Device.  It will be 
multimethod, multi-threaded, and will be optimized for peak performance within
those constraints.  Inside this device implementation there will be a furhter
interface level, not yet defined, allowing new methods to be incorporated into
the RMQ device in addition to the methods we will initially support:  TCP/IP,
shared memory, and VIA.  We also intend to implement another device, called 
the Rhcv Device, which will consist of an implementation of all of ADI-3 in
terms of the small number of core functions.  We will also supply a number of
sample implementations of the core.  It is expected that the RMQ Device will
be in some cases faster than the Rhcv Device, for example in the shared
memory case, where the core routines may not provide the optimal communication
model. 


\subsection{How Collaborators May Participate}
\label{sec:collaborators}

Thus there are several options for collaborators, depending on their
objectives.
\begin{enumerate}
\item A collaborator might implement all of ADI-3 in his own way, using his
  own data structures, handling datatypes, etc., but taking advantage of our
  collective operations, for example, and argument-checking checking code in
  the MPI-over-ADI-3 implementation.
\item A collaborator might implement only the routines in MPI Core, taking
  advantage of our data structures for communicators, datatypes, and other
  objects making up our Rhcv Device implementation.
\item A collaborator might do something in between the above two options,
  using only parts of the Rhcv Device implementation.
\item A collaborator might add a method to the RMQ Device, using its internal
  interface and library routines, and not implement any of the ADI-3 functions.
\end{enumerate}

\section{Things Left to do}
What relationship does the ADI interface to dynamic processes 
have to the BNR interface?  Should there be a connection between BNR groups
and MPID groups?

\code{MPID_Join} involves sockets, so we might make it part of the
utility module.  But what routines from \code{MPID_Core} must it call
to connect up the processes?

One way to look at any design for MPI is to look at the following
cases (see Section~\ref{sec:motivation} for more details):
\begin{enumerate}
\item \code{MPI_Testsome} and \code{MPI_Waitsome}.  These should be
efficient and fair, not just loops over \code{MPI_Test}.
\item Contiguous and simple (e.g., strided or Unix-style iov) should
be handled efficiently and more complex datatypes can be handled
without making a copy of the entire buffer.  
\item Passive target RMA operations (e.g., \code{MPI_Accumulate} with
\code{MPI_Win_lock} and \code{MPI_Win_unlock}).
\item Implement fast collective algorithms for non-contiguous
datatypes for \code{MPI_Bcast} and \code{MPI_Allreduce}.
\end{enumerate}
In addition, heterogeneous systems need also look at the handling of
complex datatypes while maintaining the ability to send without first
making a copy of the entire buffer.

Another set of features that has not yet been handled is providing
hooks so that a device can be informed about the creation and
destruction of MPI objects, particularly communicators, datatypes, and
files.  This is used in the version of the Globus device that allows
the use of the vendor native MPI within a system.  It can also be used
to help a device optimize for particular operatoins.

Yet another task is to clearly reorganize this document around
specific, important interfaces, such as BNR, Xfer, and the
communication agent interface (Brian and Rob are working on this).
The agent interface needs (1) progress, (2) intra method, (3) method
progress, and (4) inter method operations.

%Error reporting.  The ADI needs to know how to create MPI error codes.

%\section{ADI-3 Routines}
%\input adi3func.tex

%\section{ADI-3 Datastructures}
%\input adi3data.tex


\section{Integrating a Device into the MPICH build tree}
Still to do.  This section needs to cover:
\begin{enumerate}
\item mpiexec.  How does MPICH create the correct mpiexec?
\item Installation.  How does MPICH get the proper device-specific files
  installed; e.g., the mpd for the mpd device?
\item Device-specific documentation, such as environment variables and
  command-line arguments used only by a particular device.
\item Testing codes for device-specific functions.
\end{enumerate}

\subsection{Directory Structure}
\label{sec:adi3-dirs}
A device should be placed in a subdirectory of \file{mpich/src/mpid/};
for example, \file{mpich/src/mpid/mm} is the multi-method ADI
delivered with \mpich.  The directory name is the same as the device
name specified to the \mpich\ \code{configure} with the
\code{--with-device} option.

\subsection{Device Configuration and Setup}
\label{sec:adi3-setup}
Each device must have a \code{configure} script.  This will be run by
the \mpich\ \code{configure} as part of the top-level configuration.
Any other commands that a device needs for setup should be run using
the \code{AC_OUTPUT_COMMANDS} \code{autoconf} macro.  Autoconf version
2.13 or later should be used.  Do not modify the \mpich\
\code{configure} to support a device.

\section{Motivation}
\label{sec:motivation}

Why is the ADI so complex?  Consider the following scenarios:
\begin{enumerate}
\item Consider
\begin{verbatim}
    MPI_Irsend( MPI_BOTTOM, 1, my_indexed_datatype, ... )
\end{verbatim}
where the datatype describes 50MB of data, each entry of which is a
structure.
\item Consider
\begin{verbatim}
    MPI_Accumulate( MPI_BOTTOM, 1, my_indexed_datatype, ..., 
       my_vector_type, ..., MPI_MAXLOC )
\end{verbatim}
where \code{my_vector_type} is
a vector datatype of structures, and the amount of data is 50MB, and
the access is passive target (the target process makes no MPI calls,
but the operation must complete).
\item 
\end{enumerate}

Any of these may be implemented using the MPID CORE interface.
However, that interface may not provide the best efficiency.  Consider
the case of a simple send and receive of contiguous data.  In a system
providing cache-coherent shared-memory hardware, it may be possible to
implement this by having the sender directly manipulate the queues of
the receiver, including delivering the data.  The MPID CORE
implementation cannot express that approach, so a more powerful (but
more specific and thus less general) interface is also provided.  Of
course, for maximum flexibility, an implementor can bypass the ADI and
implement the MPI routines directly, perhaps taking advantage of the
parameter checking code in the MPICH2 implementation.

% Add manual entries to the table of contents
\mancontentstrue

\section{Overview}
\input Overview.tex
\input OpaqOverview.tex

\subsection{Data Structures and Constants}
\input DSOverview.tex
%\input Constants.tex
\input DyOverview.tex

\section{Summary of MPID Routines by Module}

See the table of contents for an up-to-date-list.
% %
% % I find this section very dangerous until and unless it is updated
% % automatically.  For example, MPID_Irecv was added some time ago, but
% % since it wasn't in the list, we talked about why it might be needed.
% %
% \textbf{THE FOLLOWING LIST IS CURRENTLY UPDATED BY HAND AND IS THUS OUT OF
%   DATE.  CHECK THE CONTENTS PAGE FOR A LIST OF ROUTINES}

% \subsection{MPID Core}
% \begin{verbatim}
%     MPID_Abort
%     MPID_Finalize
%     MPID_Flags_testsome
%     MPID_Flags_waitsome
%     MPID_Put_contig
%     MPID_Rhcv
%     MPID_Init
% \end{verbatim}
% %    MPID_Hid_Cancel (handler)
% %    MPID_Hid_Request_to_send (handler)

% \subsection{Dynamic}
% \begin{verbatim}
%     MPID_Comm_accept
%     MPID_Comm_connect
%     MPID_Comm_disconnect
%     MPID_Comm_spawn_multiple
%     MPID_Port_close
%     MPID_Port_open
% \end{verbatim}

% \subsection{Handlers}
% %\begin{verbatim}
%     Need enumeration of non-core handlers for \code{MPID_Rhcv}.  Perhaps these 
%     belong in various modules.
% %\end{verbatim}

% \subsection{Attributes}
% \begin{verbatim}
%     MPID_Attr_delete
%     MPID_Attr_find
%     MPID_Attr_list_walk
%     MPID_Attr_predefined
%     MPID_Comm_attr_notify
% \end{verbatim}
% %   MPID_Lang_t


% \subsection{Datatypes}
% \begin{verbatim}
%     MPID_Datatype_free
%     MPID_Datatype_incr
%     MPID_Datatype_new
%     MPID_Pack_size
%     MPID_Pack
%     MPID_Unpack
%     MPID_Segment_free
%     MPID_Segment_init_pack
%     MPID_Segment_init_unpack
%     MPID_Segment_pack
%     MPID_Segment_unpack
% \end{verbatim}
% %    MPID_Datatype
% %    MPID_Segment

% \subsection{Groups}
% \begin{verbatim}
%     MPID_Group_free
%     MPID_Group_incr
%     MPID_Group_new
% \end{verbatim}
% %    MPID_Group
% %    MPID_Lpidmask

% \subsection{Communicators}
% \begin{verbatim}
%     MPID_Comm_create
%     MPID_Comm_free
%     MPID_Comm_incr
%     MPID_Comm_thread_lock
%     MPID_Comm_thread_unlock
% \end{verbatim}
% %    MPID_Comm

% \subsection{Requests}
% \begin{verbatim}
%     MPID_Request_cancel
%     MPID_Request_send_FOA
%     MPID_Request_recv_FOA
%     MPID_Request_free
%     MPID_Request_iprobe
%     MPID_Request_new
%     MPID_Request_ready
% \end{verbatim}
% % MPID_Request

% \subsection{Communication}
% \begin{verbatim}
%     MPID_Flags_testall
%     MPID_Flags_waitall
%     MPID_Get_contig
%     MPID_Isend
%     MPID_Irecv
%     MPID_Irsend
%     MPID_Issend
%     MPID_Testsome
%     MPID_Waitsome
%     MPID_Memory_register
%     MPID_Memory_unregister
%     MPID_tBsend
% \end{verbatim}

% \subsection{Streams}
% \begin{verbatim}
%     MPID_Stream_iforward
%     MPID_Stream_irecv
%     MPID_Stream_isend
%     MPID_Stream_wait
% \end{verbatim}
% %MPID_Stream

% \subsection{Window Objects}
% \begin{verbatim}
%     MPID_Mem_alloc
%     MPID_Mem_free
% \end{verbatim}
% %    MPID_Win

% \subsection{Timers}
% \begin{verbatim}
%     MPID_Gwtick
%     MPID_Gwtime_init
%     MPID_Gwtime_diff
%     MPID_Wtime_init
%     MPID_Wtick
%     MPID_Wtime_diff
%     MPID_Wtime
% \end{verbatim}

% \subsection{Topology}
% \begin{verbatim}
%     MPID_Topo_cluster_info
% \end{verbatim}

% \subsection{Utility}
% \begin{verbatim}
%     MPID_Calloc
%     MPID_Free
%     MPID_Malloc
%     MPID_Memcpy
%     MPID_Strdup
% \end{verbatim}

% \subsection{Environment}
% \begin{verbatim}
%     MPID_MAX_THREAD_LEVEL
%     MPID_THREAD_LEVEL
% \end{verbatim}

% \subsection{Error Reporting}
% \begin{verbatim}
%     MPID_Err_create_code
%     MPID_Err_get_string
%     MPID_Err_set_msg
%     MPID_Err_add_class
%     MPID_Err_add_code
%     MPID_Err_delete_code
%     MPID_Err_delete_class
% \end{verbatim}

\section{MPID Core}
\input MPID_CORE-list.tex
%\input Handlers.tex

\section{mpiexec}
%\textbf{Need some discussion of mpiexec}
This section has not been written yet.  It will cover
\begin{enumerate}
\item How each device's \code{mpiexec} is built (since it may be a shell
  script or program)
\item How \code{mpiexec} is installed, and how any data files (e.g., lists of
  hosts) are created
\item Any commands or programs that must be run before \code{mpiexec} may be
  used, along with programs to undo that (e.g., \code{lamboot} and
  \code{lamkill}). 
\item How special information is passed between \code{mpiexec} and the MPI
  program, including how to set the attributes like \code{MPI_APPNUM} and
  whether processes have command-line arguments and environment variables
  provided by \code{mpiexec} (if not, \code{MPI_Init} may want to provide
  them). 
\end{enumerate}

In addition, there are other parts to the behavior of \code{mpiexec}
that are not covered in the MPI standard.  For example, how are the
return codes from individual MPI processes handled?  Some
possibilities are:
\begin{description}
\item The maximum value of all return codes.
\item The sum of all return codes.  Note that there may be
a limit of 255 imposed by the operating system.
\item The number of non-zero return codes.
\item A bit vector that indicates which processes had non-zero return
codes.
\end{description}

\code{mpiexec} also needs to provide an API that allows other tools such as
debuggers to locate all processes in an MPI job.

\section{Attributes}
\input AttrOverview.tex
\subsection{Keyval functions}
\input KyOverview.tex
\input InfoOverview.tex
\input Attribute-list.tex

\section{Datatypes}
Datatypes are a key part of MPI; they provide a great deal of expressive
power.  They also are difficult to implement efficiently; see 
\cite{gropp-swider-lusk99,Traeff:1999:FFE} for some techniques for speeding
the handling of datatypes.
In addition to the requirements described in Section~\ref{sec-rationale}, 
MPI-2 requires that it be possible to return the arguments that the user
used in creating the datatype.  This requires that, even if the datatype can
be simplified for the purposes of data motion (e.g., pack and unpack), 

Datatypes are constructed by the implementation of the MPI routines such as 
\code{MPI_Type_create_struct}.  That routine will call
\code{MPID_Datatype_new} to acquire a structure and will then set the fields
in that structure appropriately. 

\input Datatype-list.tex
\input SGOverview.tex
\input Segment-list.tex

\section{Groups}
\input LocalPID.tex
\input Group-list.tex

\section{Communicators}
\input Communicator-list.tex

\section{Dynamic}
\input Dynamic-list.tex

\section{Requests}
\input Request-list.tex

\section{Communication}
\input CMOverview.tex
\input Communication-list.tex

\section{Streams}
\input StmOverview.tex
\input Stream-list.tex

\section{Collective}
%\input CollOverview.tex
\input Collective-list.tex

\section{Window Objects}
\label{sec:window-objects}
\input Win-list.tex

\section{Timers}
\label{sec:timers}
The ADI must define a timer because one is needed for \code{MPI_Wtime}.  In
addition, timers are a valuable feature for use \emph{by} the ADI and/or MPI
implementation for use in performance instrumentation.  To meet these needs,
the ADI defines two kinds of timers, each of which generates a
\emph{timestamp} rather than a time value in seconds.  
One timer is local to a process. The other is global to all MPI processes, and
is intended to provide a time value that can be compared between different
processes.  

Question: Should the basic timer be part of MPID CORE?  In some sense, it
should.  Yet we could use the OS-supplied timers by default on most platforms.

\input Timer-list.tex

\section{Topology}
\input TopoOverview.tex
\input Topology-list.tex

\section{Utility}
\input Memory.tex
\input Utility-list.tex 

\section{Environment}
\input Environment-list.tex

\section{Error Reporting}
MPI allows separate error codes and classes.  An error \emph{class} is
a predefined value that roughly corresponds to a Unix error code
(e.g., \code{E_NOMEM}).  From a user's perspective, the major problem
with a simple error code (class in MPI terms) is that it is a
\emph{generic} message that gives no information about the particular
cause of the error.  For example, the Unix error code \code{E_NOMEM}
only tells the user that no memory is available.  The user might
prefer a message that included the amount of memory requested, such as 
\begin{verbatim}
Requested memory unavailable (requested 104732811 bytes 
in file getmem.c, line 137).
\end{verbatim}
The MPI error code provides a way to provide this kind of detailed
information.  An MPI error code may be though of as an
augmented error class that allows the MPI library to provide more
detailed information about an error.  MPICH uses error codes for this
purpose.  

In MPICH, an error code has three components:

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
instance id&base code&base class\\
\hline
\end{tabular}
\end{center}
where the fields are
\begin{description}
\item[\texttt{base class}]The MPI error class, from the defined classes
\item[\texttt{base code}]Specifies a more specific error message than
the \code{base class} but which contains no instance-specific data
\item[\texttt{instance id}]Used to indicate an instance-specific message
\end{description}

Values of zero for the \code{base code} and \code{instance id} are
always valid; in that case the error code is simply the error class.

The values encoded in \code{instance id} are defined by
\code{MPID_Err_create_code}; the routine \code{MPID_Err_get_string}
understands the \code{instance id} field.  One possible implementation
of this is described in (technical note not yet written, but described
in ALICE brown bag talk and Coding Standards document), and is
included with the MPICH implementation.

Messages are \code{char *} instead of \code{wchar_t *} because (a)
strings in C are \code{char *} and error strings in MPI are \code{char
*}.  However, we'd like to support Unicode messages.  We may just
return \code{char *} always, and allow the user to interpret the
message as either \code{char} or \code{wchar_t}.

\input Error-list.tex

\subsection{Implementation of Error Reporting}
\label{sec:error-report-impl}
The MPICH-1 error reporting routines use the X/Open \code{catgets} message
catalog routines.  These have the advantage of being fairly common and
portable.  Unfortunately, they are a nightmare to use because messages must be
managed as numbers (actually, a \emph{pair} of numbers) that must be kept
consistant with a central database.  An 
alternative interface, used by GNU and Sun, is the \code{gettext} interface.
This identifies messages by a string rather than a number.
Because there are portable implementations of \code{gettext}
(\url{http://www.gnu.org/software/gettext/gettext.html}) and because it is
easy to implement a version of \code{gettext} that does not support multiple
languages, we plan to implement the error handling on top of the GNU
\code{gettext} tools.  The GNU \code{gettext} distribution includes utilities
and \code{emacs} modes for creating and maintaing files of messages.

Here is a short primer on using the \code{gettext} routines.  Note that for
the most part these will not be used directly in the MPICH code; rather, they
will be used as part of the implementation of the error reporting ADI3
routines.

The initialization is performed with this code:
\begin{verbatim}
   /* Initialization */
   /* GNU recommends LC_ALL but other systems may prefer LC_MESSAGES */
   setlocale( LC_ALL, "" );
   /* LOCALEDIR is a const char * to the directory containing the
      messages.  It should come from the runtime parameter system */
   bindtextdomain( "mpich", LOCALEDIR );
   textdomain( "mpich" );
\end{verbatim}

Any message is translated by using \code{gettext}:
\begin{verbatim}
    printf( gettext( "Aborting Program with code %d\n" ), code );
\end{verbatim}
The message translations are maintained in a \code{PO} file (for ``portable
object'').  These files are converted into \code{MO} files (for ``machine
object'') by a message compiler (\code{msgfmt}).  These files are placed in
the \code{LOCALEDIR} with \file{/usr/share/locale/<LANG>/<PACKAGE>.mo}
For example, in Linux, they might be installed in
\file{/usr/share/locale/en_US/LC_MESSAGES/mpich.mo}.   For Linux, the default
\code{LOCALEDIR} is \file{/usr/share/locale}.  
Question: these directory names aren't consistent (the documentation is
poor).  What should they be?

\subsection{Error Handling}
\label{sec:error-handling}

When an error occurs, it may be necessary to take error recovery
actions.  In many MPI-1 implementations, error actions are often
limited to (a) return an error code or (b) abort MPI program.
Case (a) is taken for simple errors such as invalid parameters to a
routine while case (b) is taken for cases such as communication
failure.  

While these are acceptable responses to errors in an MPI
implementation, we would like to support a more flexible error
handling approach, without adding complexity to all of the code.
\begin{enumerate}
\item Local, non-fatal errors.  Return an error code using the error
code routines.
\item Local, fatal errors (such as SEGV).  Call \code{MPID_Abort}.
\item Nonlocal, non-fatal errors.  There are two cases:
   \begin{enumerate}
   \item Communication failure to a process.  Call
       \code{MPID_Err_link}.
   \item Notification received that communication to a process in a 
       communicator shared with this process has failed.  Call 
       \code{MPID_Err_partner}.
   \end{enumerate}
\end{enumerate}
By default, \code{MPID_Err_link} and \code{MPID_Err_partner} simply
call \code{MPID_Abort}.  However, a ADI implementation that is
tolerant of communication failure can use these to 

%\input Error-handling-list.tex

\section{Miscellaneous}
\label{sec:misc}

\subsection{Flow Control}
\label{sec:flow-control}
These routines provide support for flow control on communications.
They should be used if no other mechanism provides flow control.

Questions:

\makeussubscript
There are several different approaches for flow control.  All of these
mechanisms make use of a system that counts the amount of resources.
These are usually managed one per communication link.  In the typical
case, two values are maintained for each communication link.  The
sender keeps a count of the number of messages (or bytes or packets
etc.) sent ($n_{sent}$), and the receiver maintains the number of
message buffers 
in use ($n_{inuse}$).  The value of $n_{inuse}$ increases each time a
message is received and decreases each time a message is consumed
(e.g., because it matches a posted receive). 
The flow control approaches differ in how these values are updated. 

\begin{enumerate}
\item Occasional acks update the resource count.  One approach is a
high-watermark system.  Once $n_{inuse}$ falls below $n_{high}$, an
update is sent onces $n_{inuse}$ rises above $n_{high2}$. 

\item Updates on every packet.  Every packet from the receiver
contains an increment to be applied to $n_{sent}$.  For symmetric
communication patterns, no separate ack packet is needed.  Note,
however, that an ack packet is sometimes needed, following the
high-watermark approach above.  This can happen if, for example,
messages are sent in only one direction.  

This method can reduce the number of flow-control packets sent in many
cases.  It is particularly appropriate for communication systems that
send fixed (or minimum) sized packets, so that the extra space
required (only a byte or two in any event) has no impact on the time
to move the packet from one process to another.  

\item Negative acks.  Messages are sent to indicate that a message was
\emph{not} accepted. 

\end{enumerate}
\makeustext

Question: which should we implement?  Both of the positive acks
strategies? 

The likely routines are
\code{MPID_Flow_init} and \code{MPID_Flow_finalize} to create and
release the data structures and routines/macros \code{MPID_Flow_check}
to check that a message can be send, \code{MPID_Flow_ack} to process
an ack packet, \code{MPID_Flow_recv} to update 
\makeussubscript$n_{inuse}$\makeustext and
\code{MPID_Flow_send} to update \makeussubscript$n_{sent}$\makeustext.

\section{Extensions}
\input Extension-list.tex

\subsection{Other Extensions}
In some applications, it isn't necessary to communicate floating point values
to their full precision.  Some groups (see, for example,
\url{http://nemo.physics.ncsu.edu/~briggs/software}) have experimented with 40
and 48 bit versions of 64 bit floating point data.  For slow networks (and
slow memory systems), this can lead to significant communication performance
improvements.  This may seem like a niche extension, but MPI almost specifies
this for I/O in the form of
\code{MPI_Register_datarep}\findex{MPI_Register_datarep}.   

\openin\testfile{None-list.tex}
\ifeof\testfile\else
\section{Miscellaneous}
\input None-list.tex
\fi
\closein\testfile

\appendix
\section{Rationales}
\label{sec:rationales}
This appendix contains rationales and discussion on some of the
decisions made in the design of the ADI.

\subsection{Opaque Handles and Objects}

The ADI3 design provides only one level of isolation of objects: to
the user, the handles to the objects are opaque (in fact, they are
almost always an integer).  To all parts of the \mpich\ implementation,
the objects are well-defined structures.  An alternative design would
use more modular data encapsulation, so that only those modules that
needed the internals of the object would have the definition of the
structure.  For example, for the List type, instead of \code{MPID_List
*} we could use \code{MPID_List_t}, defined as
\begin{verbatim}
 typedef struct MPID_List *MPID_List_t;
\end{verbatim}
so that the form of \code{MPID_List} is isolated to the list
management routines.
This uses the fact that if the typedef is to a pointer to a structure,
the internals of the 
structure can be left unspecified except to the routines that actually 
manipulate them.  In the \code{MPID_List} case, this would allow the
implementation 
to use simple lists, hash tables, or HB trees, or skip lists without 
affecting or needing to recompile any code that simply refered to
\code{MPID_List}. 
However, we decided, at least for now, to leave \code{MPID_List} and
most other objects defined for all parts of the implementation.  If
there is a need to allow for multiple implementations of some
structure, we may revisit this.  

\subsection{Generalized Waitsome}
We can generalize \code{MPID_Flags_waitsome} by adding a min and max
count to return.  This allows us to combine all 
wait/test functions and add some useful generalizations.  For
example,

\begin{center}
\begin{tabular}[lll]
      min\_count  & max\_count&    equivalent routine\\\hline
      0          & 1        &    test or testany\\
      0          & count    &    testsome or testall (what is the difference?)
      1          & 1        &    wait or waitany\\
      1          & count    &    waitsome\\
      count      & count    &    waitall
\end{tabular}
\end{center}
  This allows the generalization:
\begin{center}
\begin{tabular}[lll]
      min_count  & max\_count&    equivalent routine\\\hline
      0          & 4        &    testforatmost(4)
\end{tabular}
\end{center}
This may be useful for some uses where the flags to complete on must be 
copied first into a temporary (allocated on the stack) variable.

%  Votes: Rusty votes no.

\subsection{Predefined Attribute Values}
An earlier version of this interface defined a single routine to return the
value of each attribute (e.g., \code{MPI_WTIME_IS_GLOBAL} or
\code{MPI_IO}).  This interface is not the correct one because it is
not modular.  In particular, separate modules such as the timer module
have difficulty providing the correct value with this interface.
Rather than defining a separate routine for each attribute value, we
choose to make these values (all integers) available as part of the
'MPIR_Process' per-process data structure.  

% /*@
%   MPID_Attr_predefined - Return the value of a predefined attribute

%   Notes: 
%   This is the wrong interface.  Each subsystem should provide the values
%   separately.  Question: is it better just to include these values as
%   part of the PerProcess block?  Then there is no need for this routine,
%   and each system would just access the PerProcess block directly.

%   Input Parameter:
% . keyval - A predefined keyval (the C version).  Note that these are part of 
%   'mpi.h', and thus are known to the device.

%   Notes:


%   Return value:
%   Value of the attribute (the value itself, not a pointer to it).  For example,
%   the value of 'MPI_TAG_UB' is the integer value of the largest tag.  

%   Module:
%   Attribute

%   Question:
%   The value of 'MPI_WTIME_IS_GLOBAL' is better known by the timer.  Should
%   we let the timer package provide this value?  Similarly, the value
%   of 'MPI_LASTUSECODE' is known by the error reporting module.  One 
%   possibility is to require each of the modules to have an initialization 
%   step, and for each modules'' initialization module to set the value for the
%   appropriate attributes.

%   @*/
% int MPID_Attr_predefined( int keyval )
% {
% }

\let\SaveBibliography=\thebibliography
\def\thebibliography#1{\SaveBibliography{#1}\addcontentsline{toc}{section}{References}}
\bibliography{/home/MPI/allbib,/home/gropp/Update/new/gropp}
\bibliographystyle{plain}

% Index
%\openin\testfile{adi3man.ind}
%\ifeof\testfile\else
\let\SaveIndex=\theindex
\long\def\theindex#1{\SaveIndex{#1}\addcontentsline{toc}{section}{Index}}
\input adi3man.ind
%\fi
%\closein\testfile

\end{document}
