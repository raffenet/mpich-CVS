% 
%   This is a latex file that generates a reference manual for 
%   ADI-3 
%
\documentclass[dvipdfm]{article}
\usepackage{refman}
\usepackage{tpage}
\usepackage[dvipdfm]{hyperref} % Upgraded url package
%\usepackage{url} 
\usepackage{epsf}
\textheight=9in
\textwidth=6.1in
\oddsidemargin=.2in
\topmargin=-.50in
\newread\testfile

\newcommand{\mpich}{\texttt{mpich}}
\newcommand{\Mpich}{\texttt{Mpich}}

%
% For now, let findex be the same as index.  This will allow us to
% more easily separate function and nonfunction index entries later.
\let\findex=\index
%
% Modify the way titles are handled for no breaks between pages
\def\mantitle#1#2#3{\pagerule\nobreak
\ifmancontents\addcontentsline{toc}{subsection}{#1}\fi
\index{#1}}

\makeindex

\begin{document}

\markright{ADI-3 Reference Manual}

\def\nopound{\catcode`\#=13}
{\nopound\gdef#{{\tt \char`\#}}}
\catcode`\_=13
\def_{{\tt \char`\_}}
\catcode`\_=11
\def\code#1{{\tt #1}}
%\let\url=\code
\def\makeussubscript{\catcode`\_=8}
\def\makeustext{\catcode`\_=11}
%\tpageoneskip
\ANLTMTitle{MPICH Abstract Device Interface\\
Version 3.2\\
Reference Manual\\\ \\Draft of \today}{\em 
William Gropp\\
Ewing Lusk\\
Your Name Here\\
Mathematics and Computer Science Division\\
Argonne National Laboratory}{00}{\today}

\clearpage

\pagenumbering{roman}
\tableofcontents
\clearpage

\pagenumbering{arabic}
\pagestyle{headings}

\section{Introduction}
This document contains detailed documentation on the routines that are part of
the Abstract Device Interface, version 3, used to implement the MPICH2000
model MPI implementation. 

This is a draft document.  No part of it should be considered as final.  All
parts are currently under discusssion, and comments are welcome.  In
particular, no decision has been made even on the choice of functions, much
less their particular argument lists or semantics.  This document should be
viewed as a starting point for discussions that presents one possible ADI-3
design. 

% As an alternate to this manual, the reader should consider using the
% script \code{mpiman}; this is a script that uses \code{xman} to provide
% a X11 Window System interface to the data in this manual.

\section{Discussion}
The ADI contains a large number of routines, but only a few of these
are related to the lowest-level communication operations.  Most of the
rest are used to provide opportunities for performance optimization
(e.g., \code{MPID_Isend}) or to support the objects that MPI
provides and that a device implementation may (or may not) need to
understand (e.g., MPI attributes and groups).  

The ADI is organized as a number of separate modules.
For each module except for the core communication module (called MPID
CORE), there is a sample (but complete) implementation that may be used to
build other implementations of the ADI.  These sample
implementations are refered to as the \emph{generic} module
implementations. For example,
the ADI contains routines to create and manipulate MPI communicators;
most implementations of the ADI will use the generic module for
handling communicators.  Other modules, such as the point-to-point
communications module, are more likely to be replaced with a
device-specific implementation.  Even in the cases where an
implementation of the ADI replaces a module, the generic
implementation of that module will often serve as a guide.
As a guide to implementing the MPID CORE, we provide several implementations
for popular interprocess communication mechanisms, including TCP.

There are roughly three classes of modules:
\begin{enumerate}
\item The core: basic process creation and communication. This
contains one module, MPID CORE. 
\item Support for MPI objects.  These modules support the various MPI
objects such as Datatypes, Groups, Communicators, and keyvals.
\item Communication optimization.  These modules provide an
intermediate level of functionality between that provided by MPID CORE
and that provided by MPI.  The rationale for these are described in
Section~\ref{sec-intermediate}. 
\end{enumerate}

The modules are:
\begin{description}
\item[MPID CORE.] Basic functions to start, stop, and communicate
between processes.  This also includes the \code{mpiexec} program.
This is the only module for which there is no generic implementation; as
described in Section~\ref{sec-minimal}, implementing just this module,
combined with the generic implementation of the other modules, provides a
complete ADI-3 implementation.  This set of routines is in turn
layered, so that basic implementations need implement only a few
routines.  Slightly more sophisticated implementations, by
implementing a few more of the routines in this module (rather than
using the optional emulation routines provided) can provide better and
more complete services.

\item[Attribute.] MPI Keyvals and attributes

\item[Datatype.] MPI Datatypes and pack/unpack operations and a new
object, \code{MPID_Segment}, used only within the MPI implementation.

\item[Dynamic.] Support for MPI-2 dynamic process operations.  This
module must be implemented to support the dynamic process chapter of MPI-2.

\item[Group.] MPI Groups

\item[Communicator.] MPI Communicators

\item[Request.] MPI Requests.  Requests are fundemental to many
operations; most devices that implement more than MPID CORE will need
to replace this module.

\item[Window.] MPI Windows.

\item[Point to point]Functions directly implementing the MPI
point-to-point communication functions or some key subsets.

\item[Topology.]Information on the physical interconnect.  Currently
limited to hierarchical information necessary to support clusters of
SMPs and similar systems.

\item[Timer.] Support for fast timers, including synchronized timers.

\item[Stream.] Communication routines designed to support
important algorithms for collective communication.

\item[Collective.] Alternate support for collective
routines. Note that this may end up being an MPI module, not
an MPID module, exploiting the routines in the \textbf{Stream} module.

\item[Utility.] Routines to simplify correctness and performance
debugging, and to provide a common interface to OS services that have
different interfaces on different platforms.

\item[Environment.] This module contains both compile-time and run-time
constants that describe the environment. 

% \item[Communication.] The routines that carry out communication among MPI
%   processes. 

\item[Error Reporting.] This provides support for detailed error
reporting.  Most implementations will use the generic implementation
of this module.

\item[Extensions.] This module contains no required routines; instead,
it documents some of the various extensions that the MPICH group is
considering, such as checkpointing.
\end{description}

By dividing the ADI into these modules, it becomes easy to implement
MPI quickly by implementing just MPID CORE and using the generic
modules for everything else.  Other modules can be replaced as
warranted; for example, a new machine will likely need a new Timers
module in order to access a system-specific timer.  Implementors
trying to get the maximum performance for point-to-point operations
may want to replace that module with one carefully tuned to their
specific platform.

Not included in this list is the ADIO module used to support MPI-IO.
See \cite{ThakurGroLus96} and \url{http://www.mcs.anl.gov/~thakur/adio} 
%\cite{romio-adio-manual} 
%(ROMIO ADIO Manual)
for a description of that module.

\subsection{The Core}
\label{sec-minimal}

While ADI-3 defines a large number of routines, only a few must be implemented
by a developer who is porting MPICH to a new communications platform.
For all modules except the \code{MPID_CORE} module, MPICH 
provides a generic implementation.  Thus, only the routines in
\code{MPID_CORE} must be implemented when porting MPICH to a new interprocess
communication mechanism.  To further simplify the task of implementing MPI
with ADI-3, the core does not contains the routines needed to implement the
I/O or dynamic process chapters of the MPI Standard (it does contain the
routines needed to implement the remote memory access chapter).  The
\code{MPID_CORE} consists of the following routines:  

\begin{description}
\item[\code{MPID_Init}.] This is the device's counterpart to
\code{MPI_Init_thread}.  There is no \code{MPID_Init_thread} because
\code{MPID_Init} is sufficient (\code{MPI_Init_thread} is an MPI-2
replacement for \code{MPI_Init}; if the MPI Forum was starting from
scratch, there would probably not be a separate \code{MPI_Init_thread}).

\item[\code{MPID_Finalize}.] This is the devices's counterpart to
\code{MPI_Finalize}

\item[\code{MPID_Abort}.] This is the device's counterpart to \code{MPI_Abort}.

\item[\code{MPID_Put_contig}.] This is used for data movement; it is
roughly equivalent to an

\code{MPI_Put} that handles only contiguous data.  However, it uses a
different mechanism for indicating completion of data transfers.
% (see \code{MPID_Flags_waitsome} and \code{MPID_Flags_testsome})

\item[several routines for remote service]These are used to implement
the MPI-1 cancel of send and the MPI-2 passive-target RMA routines.
There may be five of these: cancel-send, abort, passive-put,
passive-get, and passive-accumulate.  The three passive RMA calls will
need to handle arbitrary datatypes.  All five of these functions in
turn can be implemented using a single remote service call.  In
previous versions of this document, this remote service call was
provided by the routine \code{MPID_Rhcv}.

% \item[\code{MPID_Rhcv}.] This is used for messages; it is roughly an
% active-message call.  However, only predefined actions are allowed,
% and these actions are enumerated.

%\item[\code{MPID_Flags_waitsome}, \code{MPID_Flags_testsome}.] These
%are used to complete \code{MPID_Rhcv} 
%and \code{MPID_Put_contig} operations (and non-core routines such as
%\code{MPID_Get_contig} and \code{MPID_Put_sametype}). 

\item[polling routines]Several polling routines are defined to allow
for thread-safe implementations of polling.  Note that an ADI
implementation that does not need polling is free to define these as
macros that expand to no code, so the overhead of these is such a
device is zero.  These routines are defined to provide polling points
in the MPI code.

% \item[\code{MPID_Poll}.] This routine provides a way for a polling routine to
% be called.  A device is permitted to define this as a no-op if polling
% is not needed.  In other words, this routine is provided to
% \emph{allow} polling implementations, not to \emph{require} them.
% This routine also provides a way for an MPI implementation that used
% ADI3 to express ``polling point,'' locations in the code where it
% would be a helpful to poll in a polling implementation.  Even a
% non-polling implementation may use these points to check for
% communication, allowing a clever implementation to avoid the cost of a
% context switch in some cases. 

\item[\code{mpiexec}.] An implementation of \code{mpiexec}, as defined in the
  MPI-2 standard, for starting MPI jobs.  While not a routine, it must be 
  provided by any device.  
\end{description}
From these routines, together with the implementation of MPI in terms of MPID
that we are planning to carry out, all of MPI can be implemented\footnote{See
Section~\ref{sec-passive-target} for some caveats about pure polling
implementations.}.  However, this approach may sacrifice performance
for simplicity.  For increased performance, one might want to do direct
implementations of some of the non-core MPID routines, such as
\code{MPID_Isend}, directly, without relying on core routines.  For example, a
shared-memory implementation might want to rely on \code{memcpy} directly
instead of \code{MPID_Rhcv}.

This is not the only small set of routines from which MPI can be
implemented.  However, the one-sided operations defined in MPI-2 make
it difficult to use a two-sided (message-passing-like) core.
Alternately, \code{MPID_Put_contig} isn't actually necessary; the same
functionality is provided by \code{MPID_Rhcv}.  However, experience
with active messages, as well as current trends in interconnects,
suggests that a remote memory put operation that does not require any
other processing at the target (destination) should be separated from
the more general active-message invocation (\code{MPID_Rhcv}).

\subsubsection{The Role of the CORE}
The \code{MPID_CORE} is intended to provide a small set of routines
with which all of MPI can be implemented.  These routines are
\emph{not} intended as the only approach for implementing the full
ADI.  In particular, the remote handler routine, \code{MPID_Rhcv}, is
not directly by any implementation of the MPI routines.  That is, the
implementation of the MPI routines will call routines from various ADI
modules; it is the implementation of the ADI routines \emph{for the
core device} that use \code{MPID_Rhcv}.  It is possible to implement
the ADI without implementing \code{MPID_Rhcv} by implementing all of
the other ADI modules.

\textbf{Important Change.}  The current design uses different though
related approach for the basic send and receive operations.  This
involves the construction of a transfer object, which is really a
collection of data blocks to move, along with some control over how
they are moved.  This is similar to the \code{MPID_Rhcv} interface,
but opens up the construction of the list of data to move so the more
complex communications can be described.  This is important for
optimizing collective communication.  It is a slighly different
solution to the approach used in the second version of Illinois Fast
Messages \cite{Pakin97}; FM provides a send-fragment operation.

\subsection{Dynamic}
% \textbf{Should the routines in this section be replaced by BNR calls?
% How do these relate to BNR?}
% What relationship does the ADI interface to dynamic processes 
% have to the BNR interface?  
% How do BNR groups and MPID groups relate (note that there can be many
% MPI groups in an application, more than we probably want BNR to know
% about. 

For a full MPI-2 implementation, the dynamic module must also be
implemented.  That is, for a full MPI-2 implementation, the dynamic
module is really part of \code{MPID_CORE}.  This module is separated
out both because it is unnecessary for MPI-1 and because the
implementation of these routines may be very different from the
communication support in \code{MPID_CORE}.  
The contents of the
dynamic module include:

\begin{description}
\item[\code{MPID_Comm_spawn_multiple}.] Spawns new processes.
\item[\code{MPID_Port_open}.] Open a port for connecting to processes. 
\item[\code{MPID_Port_close}.] Close a port opened with \code{MPID_Port_open}.
\item[\code{MPID_Comm_connect}.] Connect to an MPI program.
\item[\code{MPID_Comm_attach}.] Attach to an MPI program.
\item[\code{MPID_Comm_disconnect}.] Detach from an MPI program.
\end{description}
An implementation that does not plan to support \code{MPI_Comm_spawn} or
any of the other MPI-2 functions in the Dynamic chapter of the MPI-2 Standard
can implement these as routines that return failure.  Such an implementation
will not implement all of MPI-2. 

The implementation of these routines in the MPICH ADI3 device make use
of the BNR dynamic process management routines \cite{toas01:bnr-design}.
These routines 
provide portable access through a scalable API to third-party process
managers.  
%  The
%   routines \code{MPID_Open_port}, \code{MPID_Close_port},
%   \code{MPID_Comm_connect}, \code{MPID_Comm_attach}, and
%   \code{MPID_Comm_disconnect} can be considered a 
%   ``submodule'' supporting the corresponding part of the MPI-2 standard.
%   Just 
%   as for \code{MPID_Comm_spawn_multiple}, an implementation that chooses to
%   support only the subset of MPI-2 that does not include the MPI routines for
%   connecting two groups of MPI processes may implement these routines by
%   simply having them return failure.

\subsection{Design Rationale}
\label{sec-intermediate}
\label{sec-rationale}
The design of MPI puts a number of constraints on a high-quality
implementation.  These constraints help explain the rationale behind
the design of ADI-3.

We start with a few general principles.  The first is that all valid
MPI programs should work.  This implies that implementations should
maintain proper flow control and should strive not to allocate new
memory after \code{MPI_Init_thread} (but is allowed to allocate
additional space on the fly for MPI programs that use large numbers of
MPI objects).  It certainly should not require
the allocation of memory proportional to the size of a message.
This simple rule has significant consequences, starting with the
handling of datatypes.

As part of an effort to keep MPI executables small (particularly those
that are statically linked) and to reduce the
startup time for an MPI application, as well as enforcing modularity
in the components of MPI, we try to limit the number of MPI modules
that a trival MPI program must include and perform lazy initialization
(that is, initialization on first use rather than during
\code{MPI_Init_thread}) where possible.

\subsubsection{Noncontiguous Datatypes}
Any message in MPI may be defined by a datatype representing
non-contiguous locations in memory; there is no limit (other than the
usual limits of available memory) on the size of this message.  Since
no interconnects provide a way to send arbitrarily large
non-contiguous messages (and even in shared memory, the complexity of
handling the layouts of both the sending and receiving datatypes), an
ADI must provide a way to pack and unpack datatypes to and from
contiguous buffers.  
Further, because the message may be arbitrarily large, an internal
buffer,
used only to hold a contiguous form of the message, should
not be allocated for the entire message.
The consequence of this rule is the \code{MPID_Segment} object and the
routines to perform partial pack and unpack operations.

A less obvious consequence of the need to handle noncontiguous
datatypes in pieces (or segments) is that an ADI interface that
provides MPI-like point-to-point operations, but only for contiguous
data, cannot be used without significant modification.\footnote{Noncontiguous
  messages must be sent in fragments (to bound the amount of memory
  used by the implementation); reassembling these, particularly in the
  presence of multiple communicating threads, is challenging to say the least,
  since an MPI-like interface has no way to include additional data, such as a
  message sequence number or other hook that could be used to put the
  fragments back together.}

% \textbf{To see what the modifications would look like, see the old RMQ
% design that I did; it used 
% handlers within the ack-loop.} 

\subsubsection{Passive Target RMA and Polling}
\label{sec-passive-target}
Passive target remote memory access (RMA) operations are among the
most demanding from an MPI implementation standpoint.  The term
``passive target'' reflects the situation in RMA operations that use
\code{MPI_Win_lock} and \code{MPI_Win_unlock} operations without the
target process making any MPI calls.  In MPI-1 (with the possible
exception of \code{MPI_Cancel} on a send operation), because all
communication is two-party (point-to-point) or multiparty
(collective), it is possible to implement MPI solely in a polling mode
because in any correct MPI program, you can guarantee that some MPI
call will eventually be made by the target process.  In passive target
RMA, this is no longer true.  This means that, unless there is
significant hardware support for RMA operations, a correct MPI-2
implementation will contain at least some operations that must be
handled by a communication ``agent''.  This agent may be a separate
thread, signal handler, or a separate process, but a pure polling
implementation is no longer possible.  Because of this, the ADI-3
design includes a simplified active-message or remote handler request
operation, provided by the \code{MPID_Rhcv} call.  Note that this is
\emph{required} only for the passive target operations but can be used
to implement all of MPI.  In addition, high-performance
implementations of the ADI are likely to use a hybrid polling and non-polling
strategy because of the lower latency that polling approaches usually provide.

\subsubsection{Efficient Collective Algorithms}
One of the weaknesses of many MPI implementations has been in the
performance of their collective operations.  This is a consequence of
using MPI point-to-point operations, combined with the generality of
MPI datatypes.  While it is always possible to build custom
implementations of the collective routines for specific systems, a
portable but much higher performance version of the collective
routines can be built by using the following concepts:
\begin{enumerate}
\item Store and forward --- Since the message may need to be placed in
special memory, separating out the buffer used for the final receive
(possibly after unpacking into a buffer described with non-contiguous
data type) and the memory from which the message could be forwarded is
an important goal.  Further, store and
forward operations often should be \emph{pipelined}; that is, sent in
chunks, where each chunk is forwarded it as it is received.
Just to make things more complex, in collective algorithms, the data
received is often processed (e.g., for reduce) and/or forwarded to
multiple destinations.  These needs, taken together, encourage the
definition of store and forward ``building block'' routines; the \code{Stream}
module contains these routines.

\textbf{Has stream been replaced by xfer?  If not, how do xfer and
stream relate to each other?}
\textbf{Answer (maybe): yes, I believe that xfer replaces stream.
However, much of the stream rationale remains, and we may want some of
the ``short cut'' routines that stream provides.}

\item Scatter/Gather --- A number of algorithms rely on dividing the
message up into separate pieces, based on viewing the message as a
contiguous array of bytes, and (in some cases) gathering the pieces
up.  For example, a simple broadcast can be written as a scatter step
followed by an Allgather step.  To write these for arbitrary (i.e.,
noncontiguous) datatypes requires the ability to divide the message up
independent of the datatype.  

\item Blocking --- An advantage of the (non File) collective
operations is that they are blocking.  This allows the code
implementing the collective operation to remain in control; in
particular, it can wait for an operation to complete and then perform
the next step.  This feature influenced the design of the
\code{Stream} module.
\end{enumerate}

\subsubsection{Multi-method Communication}
An important implementation of this ADI is the model multi-method
implementation, based on separate message queues for each method.
This device illustrates a more complex use of the ADI interface.  It
implements not only the MPID CORE but most of the point-to-point
routines as well, along with the collective support routines.  The
generic modules are used for the rest of the ADI implementation.

To make it possible to implement this kind of multi-method device, the
ADI is careful with how MPI requests are handled.  It also defines a
very high-level ADI interface (e.g., \code{MPID_Isend}) that gives the
device the flexibility to determine the appropriate method and allows
the method to determine the appropriate protocol for delivering a message.

\subsubsection{Polling}
ADI-3 has been designed (like ADI-1 and ADI-2) to permit both polling and
non-polling (e.g., interrupt-driven or thread-scheduled) implementations.
Thus, some routines are provided that provide convenient points for a polling
implementation to poll or wait.  In addition, there are two types of
polls defined: a weak poll, where a polling-based implementation may
choose whether or not to perform a poll, and a strong poll, where a
polling implementation is expected to poll.  For example, a weak poll
may implement some form of skip polling, checking only those
communication methods that have fast polls or that haven't been
checked recently (e.g., check a shared-memory queue but not a
socket-based method).  A strong poll will check both.

In addition, some poll routines accept a time-quantum to provide a
hint at the time that should be waited.  For example, in certain
circumstances, actively polling (e.g., in a spin loop) for one or two
times the round-trip delay and then switching to a more passive form
(e.g., yield the thread) provides better performance than either
spinning only or yeilding immediately.

%\code{MPID_Flags_waitsome} and
%\code{MPID_Test_waitsome} are examples.

It is the responsibility of the implementation of the ADI to ensure that
progress is made on all communication, particularly passive-target RMA
operations.  This makes it nearly impossible to support an implementation
based entirely on polling.  However, many operations can use polling to reduce
the costs associated with context switches, in combination with a nonpolling
method.

The implementation of the MPI routines will contain calls to the ADI's
polling interface at appropriate places, such as before posting a
receive or after posting a send.  An implementation that exclusively
relies on a separate communication agent thread is free to define
these routines as macros that expand to no code (thus eliminating all
overhead). 
%These are the \code{MPID_PE_poll_xxx} routines (see
%Section~???).

\subsubsection{Contrast with ADI-1 and ADI-2}
\label{sec-historical}

ADI-1, the first abstract device interface, was designed to enable MPI
to be quickly and efficiently ported to massively parallel computers
that had proprietary message-passing libraries.  ADI-1 was thus
closely tied to pre-existing systems, since demonstrating that MPI
could be implemented with low overhead was the overriding concern.

ADI-2 was an attempt at a multidevice method with more hooks for
optimizations but was still designed to sit on top of existing
message-passing systems.  

ADI-3 is the first ADI design to be free of the constraints of pre-MPI
message-passing systems, primarily because the success of MPI has
driven out all other message-passing systems (except for PVM, which
has a different target audience).  In addition, the developement of
fast commodity networks and the widescale deployment of SMP clusters
has changed the typical MPI platform.  An advantage of this is that we
can now design the ADI with a view towards lower-level, more flexible
communication primatives. 

An example is in the handling of datatypes in the three ADIs.
In ADI-1 and ADI-2, non-contiguous datatypes were handled by first
allocating a buffer large enough to hold the \emph{entire} message and
then either packing the message into that buffer (for a send) or
unpacking it after it was delivered (for a receive).  This was a
reasonable expedient when the original MPICH implementation was done,
but is not acceptable in a mature package.

\texttt{MPID_CORE} in ADI-3 corresponds to the `channel' device in ADI-2 in
the sense that it is a small set of routines, which, when combined with the
generic implementations of the other modules, gives a complete ADI
implementation.  

The ADI-2 channel interface was designed to fit the high-performance system
software of the day, which was proprietary message-passing libraries on
distributed memory parallel computers.  With the success of MPI, these
proprietary libraries have vanished, and advances in interconnect technology
have changed the API used to access high-performance interprocess
communication.  ADI-3 adapts to these changes.

\subsection{Our Plans for Using ADI-3 to Implement MPI-2}
\label{sec:plans}

%
% Replace ``Channel Device'' with ``Rhcv Device'' since the new core
% is not based on communication channels but is more of an active message
% core
%
This subsection describes our implementation plans at Argonne.  
Section~\ref{sec:collaborators} describes how others (vendors,
researchers, and others) can 
use the structure of ADI-3 to leverage our implementation work and their own
high-speed communication methodologies to produce a particular MPI
implememtation.

As discussed above, ADI-3 contains a large number of functions in order to
accomodate collaborators who want to themselves optimize a large part of the
data structures and algorithms needed in a complete MPI implementation.  At the
same time there is a much smaller number of routines, called the ``ADI Core''
in this document, that capture that part of the ADI that is closest to the
communication method. It is possible (but not required) that the full ADI be
implemented in terms of the core.

We intend to implement all of MPI in terms of the full ADI-3 interface (not
just the core).  Since the full ADI-3 is relatively rich, this will be the
simplest part of the implementation.  The most difficult part will be the
implementation of the collective operations, which will be in terms of some of
the low-level ADI-3 functions, not in terms of the MPI point-to-point
operations or their ADI-3 ``equivalents''.

We then intend to provide two implementations of the full ADI-3 interface.
The first, already under way, is what we call the multimethod Device.
It will be  
multimethod, multi-threaded, and will be optimized for peak performance within
those constraints.  Inside this device implementation there will be a furhter
interface level, not yet defined, allowing new methods to be incorporated into
the multimethod device in addition to the methods we will initially
support:  TCP/IP, 
shared memory, and VIA.  We also intend to implement another device, called 
the Rhcv Device, which will consist of an implementation of all of ADI-3 in
terms of the small number of core functions.  We will also supply a number of
sample implementations of the core.  It is expected that the RMQ Device will
be in some cases faster than the Rhcv Device, for example in the shared
memory case, where the core routines may not provide the optimal communication
model. 

\textbf{The above paragraph is out-of-date.  What is the current plan?
Will there only be a simple method interface?}

One of the reasons for the success of MPICH was the ease with which
other groups could take the code and replace the ADI implementation
with their own device.  We intent not only to preserve this feature of
MPICH, but, through the multi-method device, provide an alternative
route that allows outside groups to take advantage of the
communication methods (such as shared memory or TCP) implemented
within the multi-method device while providing their own method for a
specialized communication layer (such as VIA or LAPI).

\section{Things Left to do}

\code{MPID_Join} involves sockets, so we might make it part of the
utility module.  But what routines from \code{MPID_Core} must it call
to connect up the processes?

One way to look at any design for MPI is to look at the following
cases (see Section~\ref{sec:motivation} for more details):
\begin{enumerate}
\item \code{MPI_Testsome} and \code{MPI_Waitsome}.  These should be
efficient and fair, not just loops over \code{MPI_Test}.
\item Contiguous and simple (e.g., strided or Unix-style \code{struct
iovec}) should 
be handled efficiently; in addition, more complex datatypes must be handled
without making a copy of the entire buffer.  
\item Passive target RMA operations (e.g., \code{MPI_Accumulate} with
\code{MPI_Win_lock} and \code{MPI_Win_unlock}).
\item Implement fast collective algorithms for non-contiguous
datatypes for \code{MPI_Bcast} and \code{MPI_Allreduce}.
\end{enumerate}
In addition, heterogeneous systems need also look at the handling of
complex datatypes while maintaining the ability to send without first
making a copy of the entire buffer.

Another set of features that has not yet been handled is providing
hooks so that a device can be informed about the creation and
destruction of MPI objects, particularly communicators, datatypes, and
files.  This is used in the version of the Globus device that allows
the use of the vendor native MPI within a system.  It can also be used
to help a device optimize for particular operations.  
The routines called on creation have the form
\code{MPID_Dev_xxx_create_hook}, e.g.,
\code{MPID_Dev_comm_create_hook}; on destruction, the routine name has
the form \code{MPID_Dev_xxx_destroy_hook}. This makes  
it clear that these are hooks and not integral parts of the generic
object creation or destruction code.  A device may define these as a
no-op macro to eliminate any unnecessary function calls.

Yet another task is to clearly reorganize this document around
specific, important interfaces, such as BNR, Xfer, and the
communication agent interface (Brian and Rob are working on this).
The agent interface needs (1) progress, (2) intra method, (3) method
progress, and (4) inter method operations.

%Error reporting.  The ADI needs to know how to create MPI error codes.

%\section{ADI-3 Routines}
%\input adi3func.tex

%\section{ADI-3 Datastructures}
%\input adi3data.tex


\section{Integrating a New Device into the MPICH Build Tree}
This section describes how to add a device or method into the MPICH
build tree.  This section is intended both to describe the process of
adding a device and the rationale for the design of the
device-dependent modules.

\subsection{How To Interact With the MPICH Group on MPI Implementations}
\label{sec:collaborators}

Thus there are several options for those wishing to work with the
MPICH implementation, depending on their objectives.
\begin{enumerate}
\item Implement all of ADI-3 in his own way, using his
  own data structures, handling datatypes, etc., but taking advantage of our
  collective operations, for example, and argument-checking checking code in
  the MPI-over-ADI-3 implementation.
\item Implement only the routines in MPI Core, taking
  advantage of our data structures for communicators, datatypes, and other
  objects making up our Rhcv Device implementation.
%\item A collaborator might do something in between the above two options,
%  using only parts of the Rhcv Device implementation.
\item Implement a method for the multi-method device, using its internal
  interface and library routines, and not implement any of the ADI-3 functions.
\end{enumerate}

\input ../adiimplrules.tex

\section{Motivation}
\label{sec:motivation}

Why is the ADI so complex?  Why not define just a few functions?
Certainly such a device can be defined.  However, such a design trades
simplicity against performance.  
Consider the following scenarios:
\begin{enumerate}
\item Consider
\begin{verbatim}
    MPI_Irsend( MPI_BOTTOM, 1, my_indexed_datatype, ... )
\end{verbatim}
where the datatype describes 50MB of data, each entry of which is a
structure.
\item Consider
\begin{verbatim}
    MPI_Accumulate( MPI_BOTTOM, 1, my_indexed_datatype, ..., 
       my_vector_type, ..., MPI_MAXLOC )
\end{verbatim}
where \code{my_vector_type} is
a vector datatype of structures, and the amount of data is 50MB, and
the access is passive target (the target process makes no MPI calls,
but the operation must complete).
\end{enumerate}

Any of these may be implemented using the MPID CORE interface.
However, that interface may not provide the best efficiency.  Consider
the case of a simple send and receive of contiguous data.  In a system
providing cache-coherent shared-memory hardware, it may be possible to
implement this by having the sender directly manipulate the queues of
the receiver, including delivering the data.  The MPID CORE
implementation cannot express that approach, so a more powerful (but
more specific and thus less general) interface is also provided.  Of
course, for maximum flexibility, an implementor can bypass the ADI and
implement the MPI routines directly, perhaps taking advantage of the
parameter checking code in the MPICH2 implementation.

% Add manual entries to the table of contents
\mancontentstrue

\section{Overview of ADI3}
\input Overview.tex
\input OpaqOverview.tex

\subsection{Data Structures and Constants}
\input DSOverview.tex
%\input Constants.tex
%\input DyOverview.tex

\section{Summary of MPID Routines by Module}

See the table of contents for an up-to-date-list.
% %
% % I find this section very dangerous until and unless it is updated
% % automatically.  For example, MPID_Irecv was added some time ago, but
% % since it wasn't in the list, we talked about why it might be needed.
% %
% \textbf{THE FOLLOWING LIST IS CURRENTLY UPDATED BY HAND AND IS THUS OUT OF
%   DATE.  CHECK THE CONTENTS PAGE FOR A LIST OF ROUTINES}

% \subsection{MPID Core}
% \begin{verbatim}
%     MPID_Abort
%     MPID_Finalize
%     MPID_Flags_testsome
%     MPID_Flags_waitsome
%     MPID_Put_contig
%     MPID_Rhcv
%     MPID_Init
% \end{verbatim}
% %    MPID_Hid_Cancel (handler)
% %    MPID_Hid_Request_to_send (handler)

% \subsection{Dynamic}
% \begin{verbatim}
%     MPID_Comm_accept
%     MPID_Comm_connect
%     MPID_Comm_disconnect
%     MPID_Comm_spawn_multiple
%     MPID_Port_close
%     MPID_Port_open
% \end{verbatim}

% \subsection{Handlers}
% %\begin{verbatim}
%     Need enumeration of non-core handlers for \code{MPID_Rhcv}.  Perhaps these 
%     belong in various modules.
% %\end{verbatim}

% \subsection{Attributes}
% \begin{verbatim}
%     MPID_Attr_delete
%     MPID_Attr_find
%     MPID_Attr_list_walk
%     MPID_Attr_predefined
%     MPID_Comm_attr_notify
% \end{verbatim}
% %   MPID_Lang_t


% \subsection{Datatypes}
% \begin{verbatim}
%     MPID_Datatype_free
%     MPID_Datatype_incr
%     MPID_Datatype_new
%     MPID_Pack_size
%     MPID_Pack
%     MPID_Unpack
%     MPID_Segment_free
%     MPID_Segment_init_pack
%     MPID_Segment_init_unpack
%     MPID_Segment_pack
%     MPID_Segment_unpack
% \end{verbatim}
% %    MPID_Datatype
% %    MPID_Segment

% \subsection{Groups}
% \begin{verbatim}
%     MPID_Group_free
%     MPID_Group_incr
%     MPID_Group_new
% \end{verbatim}
% %    MPID_Group
% %    MPID_Lpidmask

% \subsection{Communicators}
% \begin{verbatim}
%     MPID_Comm_create
%     MPID_Comm_free
%     MPID_Comm_incr
%     MPID_Comm_thread_lock
%     MPID_Comm_thread_unlock
% \end{verbatim}
% %    MPID_Comm

% \subsection{Requests}
% \begin{verbatim}
%     MPID_Request_cancel
%     MPID_Request_send_FOA
%     MPID_Request_recv_FOA
%     MPID_Request_free
%     MPID_Request_iprobe
%     MPID_Request_new
%     MPID_Request_ready
% \end{verbatim}
% % MPID_Request

% \subsection{Communication}
% \begin{verbatim}
%     MPID_Flags_testall
%     MPID_Flags_waitall
%     MPID_Get_contig
%     MPID_Isend
%     MPID_Irecv
%     MPID_Irsend
%     MPID_Issend
%     MPID_Testsome
%     MPID_Waitsome
%     MPID_Memory_register
%     MPID_Memory_unregister
%     MPID_tBsend
% \end{verbatim}

% \subsection{Streams}
% \begin{verbatim}
%     MPID_Stream_iforward
%     MPID_Stream_irecv
%     MPID_Stream_isend
%     MPID_Stream_wait
% \end{verbatim}
% %MPID_Stream

% \subsection{Window Objects}
% \begin{verbatim}
%     MPID_Mem_alloc
%     MPID_Mem_free
% \end{verbatim}
% %    MPID_Win

% \subsection{Timers}
% \begin{verbatim}
%     MPID_Gwtick
%     MPID_Gwtime_init
%     MPID_Gwtime_diff
%     MPID_Wtime_init
%     MPID_Wtick
%     MPID_Wtime_diff
%     MPID_Wtime
% \end{verbatim}

% \subsection{Topology}
% \begin{verbatim}
%     MPID_Topo_cluster_info
% \end{verbatim}

% \subsection{Utility}
% \begin{verbatim}
%     MPID_Calloc
%     MPID_Free
%     MPID_Malloc
%     MPID_Memcpy
%     MPID_Strdup
% \end{verbatim}

% \subsection{Environment}
% \begin{verbatim}
%     MPID_MAX_THREAD_LEVEL
%     MPID_THREAD_LEVEL
% \end{verbatim}

% \subsection{Error Reporting}
% \begin{verbatim}
%     MPID_Err_create_code
%     MPID_Err_get_string
%     MPID_Err_set_msg
%     MPID_Err_add_class
%     MPID_Err_add_code
%     MPID_Err_delete_code
%     MPID_Err_delete_class
% \end{verbatim}

\section{MPID Core}
%\subsection{Data Structures}
%\input MPID_CORE-DS-list.tex
\subsection{Functions}
\input MPID_CORE-list.tex
%\input Handlers.tex

\section{mpiexec}
%\textbf{Need some discussion of mpiexec}
This section has not been written yet.  It will cover
\begin{enumerate}
\item How each device's \code{mpiexec} is built (since it may be a shell
  script or program)
\item How \code{mpiexec} is installed, and how any data files (e.g., lists of
  hosts) are created
\item Any commands or programs that must be run before \code{mpiexec} may be
  used, along with programs to undo that (e.g., \code{lamboot} and
  \code{lamkill}). 
\item How special information is passed between \code{mpiexec} and the MPI
  program, including how to set the attributes like \code{MPI_APPNUM} and
  whether processes have command-line arguments and environment variables
  provided by \code{mpiexec} (if not, \code{MPI_Init} may want to provide
  them). 
\end{enumerate}

In addition, there are other parts to the behavior of \code{mpiexec}
that are not covered in the MPI standard.  For example, how are the
return codes from individual MPI processes handled?  Some
possibilities are:
\begin{description}
\item The maximum value of all return codes.
\item The sum of all return codes.  Note that there may be
a limit of 255 imposed by the operating system.
\item The number of non-zero return codes.
\item A bit vector that indicates which processes had non-zero return
codes.
\item A signed character return code, along with an XML string
containing detailed information on each process (possibly using ranges
of processes for better scalability).  The XML string could be written
to a specified file, including \code{stderr}.
\end{description}

\code{mpiexec} also needs to provide an API that allows other tools such as
debuggers to locate all processes in an MPI job.

\section{Attributes}
\input AttrOverview.tex

\subsection{Keyval functions}
\input KyOverview.tex
\subsection{Data Structures}
\input Attribute-DS-list.tex
\subsection{Functions}
\input Attribute-list.tex

\section{Error Handlers}
\subsection{Data Structures}
\input ErrHand-DS-list.tex

\section{Info}
\input InfoOverview.tex
\subsection{Data Structures}
\input Info-DS-list.tex
\subsection{Functions}
\input Info-list.tex

\section{Datatypes}
Datatypes are a key part of MPI; they provide a great deal of expressive
power.  They also are difficult to implement efficiently; see 
\cite{gropp-swider-lusk99,Traeff:1999:FFE} for some techniques for speeding
the handling of datatypes.
In addition to the requirements described in Section~\ref{sec-rationale}, 
MPI-2 requires that it be possible to return the arguments that the user
used in creating the datatype.  This requires that, even if the datatype can
be simplified for the purposes of data motion (e.g., pack and unpack), 

Datatypes are constructed by the implementation of the MPI routines such as 
\code{MPI_Type_create_struct}.  That routine will call
\code{MPIU_Handle_obj_new} to acquire a structure and will then set the fields
in that structure appropriately. 

A critical part of the internal description of a datatype is the
`dataloop`, described by the 'MPID_Dataloop' structure.  
The 'dataloop' provides both the information necessary to
efficiently implement datatype pack and unpack operations (including
partial pack and unpack necessary for moving data in multiple pieces
implemented by the xfer interface)
and to retain a description of the arguments to the routine that
created the datatype, necessary to implement the MPI routines to
return the `envelope` and `contents` of a datatype.

\subsection{Data Structures}
\input Datatype-DS-list.tex
\subsection{Functions}
\input Datatype-list.tex

% starts as section Communication Buffer Management
\input SGOverview.tex

\subsection{Data Structures}
\input Segment-DS-list.tex
\subsection{Functions}
\input Segment-list.tex

\section{Groups}
\input LocalPID.tex
\subsection{Data Structures}
\input Group-DS-list.tex
%\subsection{Functions}
%\input Group-list.tex

\section{Communicators}
\subsection{Data Structures}
\input Communicator-DS-list.tex
\subsection{Functions}
\input Communicator-list.tex

%\section{Dynamic}
%\input Dynamic-list.tex

\section{Requests}
\subsection{Data Structures}
\input Request-DS-list.tex
\subsection{Functions}
\input Request-list.tex

\section{Communication}
\input CMOverview.tex
\input Communication-list.tex

\section{Streams}
\textbf{How do these relate to the xfer interface?  Are they
superceeded?  Are streams used to implement xfer?}
\input StmOverview.tex
%\subsection{Data Structures}
%\input Stream-DS-list.tex
%\subsection{Functions}
%\input Stream-list.tex

\section{Collective Computation}
Special structures are needed to support collective computation.
Because the same object is used for both the predefined operations
(such as \code{MPI_SUM}) and for user-defined operations.  The
predefined operations are also used in with 'MPI_Accumulate'.

%\input CollOverview.tex
\subsection{Data Structures}
\input Collective-DS-list.tex
%\subsection{Functions}
%\input Collective-list.tex

\section{Window Objects}
\label{sec:window-objects}
Window objects in MPI are used to support the remote memory access
(RMA) operations.  
\subsection{Data Structures}
\input Win-DS-list.tex
\subsection{Functions}
\input Win-list.tex

\section{Timers}
\label{sec:timers}
The ADI must define a timer because one is needed for \code{MPI_Wtime}.  In
addition, timers are a valuable feature for use \emph{by} the ADI and/or MPI
implementation for use in performance instrumentation.  To meet these needs,
the ADI defines 
%two kinds of timers, each of which generates a
a timer that generates a 
\emph{timestamp} rather than a time value in seconds.  
%One timer is local to a process. The other is global to all MPI processes, and
%is intended to provide a time value that can be compared between different
%processes.  
The timer is local to a process (no global timer).
%Question: Should the basic timer be part of MPID CORE?  In some sense, it
%should.  Yet we could use the OS-supplied timers by default on most platforms.

To simplify the support of different timers and in particular to
support lightweight, high-resolution, machine-specific timers, the
timer module has its own \code{configure} and include files.  The
\code{configure} in the timer module copies the appropriate
\file{mpichtimer.h} file into \file{mpich/src/include} for use by the
MPI implementation.
%%%
%%% Global timers commented out because they are too far down the
%%% priority list.

\subsection{Functions}
\input Timer-list.tex

\section{Topology}
\input TopoOverview.tex
\subsection{Functions}
\input Topology-list.tex

\section{Utility}
\subsection{Functions}
\input Memory.tex
\input Utility-list.tex 

\section{Environment}
\subsection{Data Structures}
\input Environment-DS-list.tex
%\subsection{Functions}
%\input Environment-list.tex

\section{Error Reporting}
MPI allows separate error codes and classes.  An error \emph{class} is
a predefined value that roughly corresponds to a Unix error code
(e.g., \code{E_NOMEM}).  From a user's perspective, the major problem
with a simple error code (class in MPI terms) is that it is a
\emph{generic} message that gives no information about the particular
cause of the error.  For example, the Unix error code \code{E_NOMEM}
only tells the user that no memory is available.  The user might
prefer a message that included the amount of memory requested, such as 
\begin{verbatim}
Requested memory unavailable (104732811 bytes in file getmem.c, line 137).
\end{verbatim}
The MPI error code provides a way to provide this kind of detailed
information.  An MPI error code may be though of as an
augmented error class that allows the MPI library to provide more
detailed information about an error.  MPICH uses error codes for this
purpose.  

In MPICH, an error code has three components:

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
instance id&base code&base class\\
\hline
\end{tabular}
\end{center}
where the fields are
\begin{description}
\item[\texttt{base class}:]The MPI error class, from the defined
classes.  There are about 45 predefined classes.

\item[\texttt{base code}:]Specifies a more specific error message than
the \code{base class} but which contains no instance-specific data.
For example, within \code{MPI_ERR_ARG}, there are different base codes
for negative arguments, out-of-range arguments, etc.

\item[\texttt{instance id}:]Used to indicate an instance-specific message
\end{description}

Values of zero for the \code{base code} and \code{instance id} are
always valid; in that case the error code is simply the error class.

The values encoded in \code{instance id} are defined by
\code{MPID_Err_create_code}; the routine \code{MPID_Err_get_string}
understands the \code{instance id} field.  The MPICH-1 code contains a
partial implementation of this approach.
%One possible implementation
%of this is described in (technical note not yet written, but described
%in ALICE brown bag talk and Coding Standards document), and is
%included with the MPICH implementation.

Messages are \code{char *} instead of \code{wchar_t *} because (a)
strings in C are \code{char *} and error strings in MPI are \code{char
*}.  However, we'd like to support Unicode messages.  We may just
return \code{char *} always, and allow the user to interpret the
message as either \code{char} or \code{wchar_t}.

MPI-2 provides an interface that allows users to add their own error
codes and classes.  The standard is silent on the number of codes and
classes that the user may add.  In order to provide a reasonable
balance between effective, instance-specific messages and user-defined
error codes and classes, we will allow 255 classes (not counting
\code{MPI_SUCCESS} and 255
codes within each class.  This includes both the predefined codes and
classes and the user-defined codes and classes.

\input Error-list.tex

\subsection{Implementation of Error Reporting}
\label{sec:error-report-impl}
The MPICH-1 error reporting routines use the X/Open \code{catgets} message
catalog routines.  These have the advantage of being fairly common and
portable.  Unfortunately, they are a nightmare to use because messages must be
managed as numbers (actually, a \emph{pair} of numbers) that must be kept
consistant with a central database.  An 
alternative interface, used by GNU and Sun, is the \code{gettext} interface.
This identifies messages by a string rather than a number.
Because there are portable implementations of \code{gettext}
(\url{http://www.gnu.org/software/gettext/gettext.html}) and because it is
easy to implement a version of \code{gettext} that does not support multiple
languages, we plan to implement the error handling on top of the GNU
\code{gettext} tools.  The GNU \code{gettext} distribution includes utilities
and \code{emacs} modes for creating and maintaing files of messages.

Here is a short primer on using the \code{gettext} routines.  Note that for
the most part these will not be used directly in the MPICH code; rather, they
may be used as part of the implementation of the error reporting ADI3
routines.

The initialization is performed with this code:
\begin{verbatim}
   /* Initialization */
   /* GNU recommends LC_ALL but other systems may prefer LC_MESSAGES */
   setlocale( LC_ALL, "" );
   /* LOCALEDIR is a const char * to the directory containing the
      messages.  It should come from the runtime parameter system */
   bindtextdomain( "mpich", LOCALEDIR );
   textdomain( "mpich" );
\end{verbatim}

Any message is translated by using \code{gettext}:
\begin{verbatim}
    printf( gettext( "Aborting Program with code %d\n" ), code );
\end{verbatim}
The message translations are maintained in a \code{PO} file (for ``portable
object'').  These files are converted into \code{MO} files (for ``machine
object'') by a message compiler (\code{msgfmt}).  These files are placed in
the \code{LOCALEDIR} with \file{/usr/share/locale/<LANG>/<PACKAGE>.mo}
For example, in Linux, they might be installed in
\file{/usr/share/locale/en_US/LC_MESSAGES/mpich.mo}.   For Linux, the default
\code{LOCALEDIR} is \file{/usr/share/locale}.  
Question: these directory names aren't consistent (the documentation is
poor).  What should they be?

\subsection{Error Handling}
\label{sec:error-handling}

When an error occurs, it may be necessary to take error recovery
actions.  In many MPI-1 implementations, error actions are often
limited to (a) return an error code or (b) abort MPI program.
Case (a) is taken for simple errors such as invalid parameters to a
routine while case (b) is taken for cases such as communication
failure.  

While these are acceptable responses to errors in an MPI
implementation, we would like to support a more flexible error
handling approach, without adding complexity to all of the code.
\begin{enumerate}
\item Local, non-fatal errors.  Return an error code using the error
code routines.
\item Local, fatal errors (such as SEGV).  Call \code{MPID_Abort}.
\item Nonlocal, non-fatal errors.  There are two cases:
   \begin{enumerate}
   \item Communication failure to a process.  Call
       \code{MPID_Err_link}.
   \item Notification received that communication to a process in a 
       communicator shared with this process has failed.  Call 
       \code{MPID_Err_partner}.
   \end{enumerate}
\end{enumerate}
By default, \code{MPID_Err_link} and \code{MPID_Err_partner} would simply
call \code{MPID_Abort}.  However, a ADI implementation that is
tolerant of communication failure can use these to update the status
of communication connections and to mark communicators as
``damaged''.  These routines have not yet been precisely defined, and
should not be used in MPICH-2.

%\input Error-handling-list.tex

\section{Miscellaneous}
\label{sec:misc}

\subsection{Message Queue Implementation}
The tests for message-matching can be done by using a single 64-bit
integer compare (possibly with a mask applied).  For special cases
(more limited tag ranges, communicator context ids, and numbers of
processes), even 32 bits is enough.  The generic queue code will allow
the use of \code{int64_t} as an option to simplify and speed the
search for messages matching a specified communicator, tag, and rank
tuple.

\subsection{Flow Control}
\label{sec:flow-control}
These routines provide support for flow control on communications.
They should be used if no other mechanism provides flow control.

\makeussubscript
There are several different approaches for flow control.  All of these
mechanisms make use of a system that counts the amount of resources.
These are usually managed one per communication link.  In the typical
case, two values are maintained for each communication link.  The
sender keeps a count of the number of messages (or bytes or packets
etc.) sent ($n_{sent}$), and the receiver maintains the number of
message buffers 
in use ($n_{inuse}$).  The value of $n_{inuse}$ increases each time a
message is received and decreases each time a message is consumed
(e.g., because it matches a posted receive). 
The flow control approaches differ in how these values are updated. 

\begin{enumerate}
\item Occasional acks update the resource count.  One approach is a
high-watermark system.  Once $n_{inuse}$ falls below $n_{high}$, an
update is sent onces $n_{inuse}$ rises above $n_{high2}$. 

\item Updates on every packet.  Every packet from the receiver
contains an increment to be applied to $n_{sent}$.  For symmetric
communication patterns, no separate ack packet is needed.  Note,
however, that an ack packet is sometimes needed, following the
high-watermark approach above.  This can happen if, for example,
messages are sent in only one direction.  

This method can reduce the number of flow-control packets sent in many
cases.  It is particularly appropriate for communication systems that
send fixed (or minimum) sized packets, so that the extra space
required (only a byte or two in any event) has no impact on the time
to move the packet from one process to another.  

\item Negative acks.  Messages are sent to indicate that a message was
\emph{not} accepted. 

\end{enumerate}
\makeustext

Question: which should we implement?  Both of the positive ack
strategies? 

The likely routines are
\code{MPID_Flow_init} and \code{MPID_Flow_finalize} to create and
release the data structures and routines/macros \code{MPID_Flow_check}
to check that a message can be send, \code{MPID_Flow_ack} to process
an ack packet, \code{MPID_Flow_recv} to update 
\makeussubscript$n_{inuse}$\makeustext\ and
\code{MPID_Flow_send} to update \makeussubscript$n_{sent}$\makeustext.

A device (or communication method) is free to use these routines or to
use any other (correct) approach to flow control.  These routines will
be provided to make it easier for new communication methods to be
created that implement the necessary flow-control.

\section{Extensions}
\input Extension-list.tex

\subsection{Other Extensions}
In some applications, it isn't necessary to communicate floating point values
to their full precision.  Some groups (see, for example,
\url{http://nemo.physics.ncsu.edu/~briggs/software}) have experimented with 40
and 48 bit versions of 64 bit floating point data.  For slow networks (and
slow memory systems), this can lead to significant communication performance
improvements.  This may seem like a niche extension, but MPI almost specifies
this for I/O in the form of
\code{MPI_Register_datarep}\findex{MPI_Register_datarep}.   

\openin\testfile{None-list.tex}
\ifeof\testfile\else
\section{Miscellaneous}
\input None-list.tex
\fi
\closein\testfile

\appendix
\section{Rationales}
\label{sec:rationales}
This appendix contains rationales and discussion on some of the
decisions made in the design of the ADI.

\subsection{Opaque Handles and Objects}

The ADI3 design provides only one level of isolation of objects: to
the user, the handles to the objects are opaque (in fact, they are
almost always an integer).  To all parts of the \mpich\ implementation,
the objects are well-defined structures.  An alternative design would
use more modular data encapsulation, so that only those modules that
needed the internals of the object would have the definition of the
structure.  For example, for the List type, instead of \code{MPID_List
*} we could use \code{MPID_List_t}, defined as
\begin{verbatim}
 typedef struct MPID_List *MPID_List_t;
\end{verbatim}
so that the form of \code{MPID_List} is isolated to the list
management routines.
This uses the fact that if the typedef is to a pointer to a structure,
the internals of the 
structure can be left unspecified except to the routines that actually 
manipulate them.  In the \code{MPID_List} case, this would allow the
implementation 
to use simple lists, hash tables, or HB trees, or skip lists without 
affecting or needing to recompile any code that simply refered to
\code{MPID_List}. 
However, we decided, at least for now, to leave \code{MPID_List} and
most other objects defined for all parts of the implementation.  If
there is a need to allow for multiple implementations of some
structure, we may revisit this.  

\subsection{Attributes and Info}
Earlier versions mused out loud about using the same structures to
implement attributes and \code{MPI_Info} because they are both lists
of key/value pairs.  This isn't a good idea because both the key and
values are so different in the two cases.

\subsubsection{Protecting Function Pointers.}
  Because the keyval structure contains pointers to user-functions,
  there is always 
  the danger that a user-error that overwrites the memory locations storing 
  the function pointers.  Should there be sentinals around either the entire
  keyval entry or around each function pointer that would be tested before 
  invoking the function?  
  For example, we could define
\begin{verbatim}
  typedef struct { 
      unsigned long sentinal1;
      void (*f)(void);           // pointers to functions are different from
                                 // pointers to nonfunctions 
      unsigned long sentinal2;
  } MPID_Protected_function_ptr;
\end{verbatim}
  and compute the sentinals based on the value of the function pointer.

\subsection{Generalized Waitsome}
We can generalize \code{MPID_Flags_waitsome} by adding a min and max
count to return.  This allows us to combine all 
wait/test functions and add some useful generalizations.  For
example,

\begin{center}
\begin{tabular}{lll}
      \code{min_count}  & \code{max_count}&    equivalent routine\\\hline
      0          & 1        &    test or testany\\
      0          & count    &    testsome or testall (what is the difference?)\\
      1          & 1        &    wait or waitany\\
      1          & count    &    waitsome\\
      count      & count    &    waitall
\end{tabular}
\end{center}
  This allows the generalization:
\begin{center}
\begin{tabular}{lll}
      \code{min_count}  & \code{max_count}&    equivalent routine\\\hline
      0          & 4        &    testforatmost(4)
\end{tabular}
\end{center}
This may be useful for some uses where the flags to complete on must be 
copied first into a temporary (allocated on the stack) variable.

%  Votes: Rusty votes no.

\subsection{Predefined Attribute Values}
An earlier version of this interface defined a single routine to return the
value of each attribute (e.g., \code{MPI_WTIME_IS_GLOBAL} or
\code{MPI_IO}).  This interface is not the correct one because it is
not modular.  In particular, separate modules such as the timer module
have difficulty providing the correct value with this interface.
Rather than defining a separate routine for each attribute value, we
choose to make these values (all integers) available as part of the
\code{MPIR_Process} per-process data structure.  

% /*@
%   MPID_Attr_predefined - Return the value of a predefined attribute

%   Notes: 
%   This is the wrong interface.  Each subsystem should provide the values
%   separately.  Question: is it better just to include these values as
%   part of the PerProcess block?  Then there is no need for this routine,
%   and each system would just access the PerProcess block directly.

%   Input Parameter:
% . keyval - A predefined keyval (the C version).  Note that these are part of 
%   'mpi.h', and thus are known to the device.

%   Notes:


%   Return value:
%   Value of the attribute (the value itself, not a pointer to it).  For example,
%   the value of 'MPI_TAG_UB' is the integer value of the largest tag.  

%   Module:
%   Attribute

%   Question:
%   The value of 'MPI_WTIME_IS_GLOBAL' is better known by the timer.  Should
%   we let the timer package provide this value?  Similarly, the value
%   of 'MPI_LASTUSECODE' is known by the error reporting module.  One 
%   possibility is to require each of the modules to have an initialization 
%   step, and for each modules'' initialization module to set the value for the
%   appropriate attributes.

%   @*/
% int MPID_Attr_predefined( int keyval )
% {
% }
\subsection{Streads and Xfer}
The stream interface originally proposed was reasonable for
implementing MPI's blocking collective with the more general
scatter/gather bassed algorithms.  However, the stream API placed the
control of scheduling of the operations outside of the device; for
example, a \code{MPID_Stream_wait} routine was provided to let the
code that was using streams wait for completion of the current data
transfer.  This is not a good fit to nonpolling devices, and even for
polling devices, makes it difficult for the device to efficiently
schedule operations.  Thus, the xfer interface defines a data transfer
\emph{program} and lets the device execute that program.

\let\SaveBibliography=\thebibliography
\def\thebibliography#1{\SaveBibliography{#1}\addcontentsline{toc}{section}{References}}
\bibliography{/home/MPI/allbib}
\bibliographystyle{plain}

% Index
%\openin\testfile{adi3man.ind}
%\ifeof\testfile\else
\let\SaveIndex=\theindex
\long\def\theindex#1{\SaveIndex{#1}\addcontentsline{toc}{section}{Index}}
\input adi3man.ind
%\fi
%\closein\testfile

\end{document}
