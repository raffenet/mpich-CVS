%
% This file contains the discussion of the design of the ADI.  It is
% meant as a motivation for the design, explaining why particular
% choices were made. 
% It is included into goals.tex and adi3new.tex
%

\section{Introduction}
The goal of the MPICH-2 ADI-3 is to provide routines to support MPI-1
and MPI-2 operations.  The target systems include clusters connected
with either conventional networks or networks that support remote
memory access such as Infiniband, large symmetric multiprocessors
(SMPs), and experimental systems (particularly in support of research
into MPI implementations).  

\subsection{Other Work}
\label{sec:other-work}

This section has not been written yet.

This section will briefly review other MPI implementation designs.  

Other implementations of MPI include ones from IBM \cite{mpi-ibm},
Sun \cite{hat01:sun-mpi}, MPI Software technology \cite{mpi-softtech} and
Critical Software \cite{mpi-critical-software}.  The IBM, Sun, and MPI
Software technology versions offer support for \code{MPI_THREAD_MULTIPLE}.

Several implementations have been developed that rely on remote memory
operations, including IBM LAPI \cite{mpi-LAPI99}, DEC (now HP) memory
channel \cite{DEC-memchannel2}, Infiniband
\cite{mpi-infiniband}, and BIP \cite{mpi-bip}.  

Several implementations of the MPI one-sided operations have been
described; see \cite{mpi-sun-rma}, \cite{mpi-fujitsu-rma}.

The major cluster implementations are MPICH
\cite{gropp-lusk-doss-skjellum:mpich} and LAM/MPI \cite{lam,lam-www-iu}.  

Another implementation is MPI/Pro from MPI Software Technologies;
\cite{mpi-rossum} describes the implementation of one version of this
product. 

An example of the use of both lock-free shared-memory operations and threads
as MPI ``processes'' is presented in \cite{tang00:mpi-impl}.  

\subsection{MPI Overview}
In order to better understand the design of the ADI, we first review
MPI communication.  MPI-1 defined both point-to-point and collective
message-passing.  In both of these, the sender and the receiver
actively participates in the communication in the sense that
communication does not occur until an MPI call is made to initiate it
and the data is not guaranteed to be delivered until an MPI call is
made by the destination process.  In fact, because the destination in memory of
the data is described by an MPI call made by the destination process, the
data cannot be delivered to its final (user-specified) destination
until the matching MPI call is made by the destination process.  This feature
has led many (but not all) MPI implementations to adopt a
\emph{polling}\index{polling} mode of communication where communication for MPI
happens only within MPI calls, not asynchronously.\footnote{The only
  exception to this is cancelling of MPI Send operations; a strictly
  conforming implementation of this requires a guarantee that the
  remote process will respond without requiring an MPI call.  Because
  of the extremely rare use of this feature, most polling-mode
  implementations do not support this case.} An alternative
approach uses either one more separate threads or an interrupt to
cause communication to happen outside of MPI calls made by the
user\footnote{MPICH-1 supported both modes but most implementations chose the
  polling mode.  All ADI-2 implementation distributed with MPICH-1 used
  polling mode; however, some built by outside groups, such as the version for the Intel TFLOPS system, did \emph{not} use polling mode.}.

MPI-2 introduced additional operations including remote memory access (RMA),
dynamic process management, and parallel I/O.  Remote memory
access (also called one-sided communication) provides a way to express
put, get, and accumulate operations into memory in a remote process.
In MPI, these operations are all nonblocking; to ensure that these
operations are locally complete, an additional MPI routine must be
called.  MPI provides two ``flavors'' of RMA completion: \emph{active
target}\index{active target}
and \emph{passive target}\index{passive target}.  In active target,
the target process of an 
RMA operation (that is, the process that did \emph{not} initiate the
RMA operation) must call an MPI routine before the RMA completes.  The
simplest such routine is \code{MPI_Win_fence}; this is a collective call over
all 
processes associated with the RMA window and is similar to a barrier.
The other is the more esoteric ``scalable completion'' routines, which
are called by a group of processes.  In both of these cases, an MPI
implementation may rely on the target processes calling an MPI
routine, and thus these may (but are not required to) use a polling
implementation.   Unlike some other RMA APIs, MPI active target
RMA operations may be applied to any memory belonging to the process.

The other form of RMA completion is handled by calls made only by the
originating process.  This is called passive target RMA.  Because the
target process is not required to make any MPI calls, this kind of RMA
requires either very capable hardware that can handle all MPI RMA
operations or the use of a non-polling agent at the target process, or a
combination of these.
Because these operations can be more difficult to implement
efficiently, MPI allows an MPI implementation to require that passive
target RMA operations be allowed only on memory allocated by
\code{MPI_Alloc_mem}.  

MPI-2 dynamic process management allows an MPI application both to
create new processes and communicate with them and to connect two
already running MPI programs together.  The number of processes in an
MPI program can thus change over the lifetime of the program, though
the MPI routines to create or connect to processes are collective over
a communicator, allowing an MPI implementation to ensure that these
operations are handled in a scalable fashion.

Section~\ref{sec:basic-design} describes some of the design choices that these
operations suggest.  These choices are not the only ones possible, but
we believe that they provide a consistent and efficient way to realize
the communication defined by MPI.

\subsection{A Layered Approach}
The ADI described here is full-featured.  This allows an implementor
to take advantage of the opportunities for more efficient
communication.  However, to keep this flexibility from becoming a
burden, the design of the ADI is also \emph{layered}: the more
advanced features can be emulated by the more basic features.  The
implementation of the ADI distributed with MPICH will include code to
provide these more advanced features in terms of the more basic
operations, allowing an implementor to quickly create a working
implementation of the ADI and providing the opportunity to later
enhance the performance by selectively replacing some of these
emulations.  Section~\ref{sec:layers} describes this in more detail.

The ADI described here resembles the communication routines of the MPI
standard.  The differences are
\begin{enumerate}
\item The MPI objects such as requests and communicators are not
opaque objects; instead, the ADI uses pointers to structures with
defined fields.

\item Checking for correct parameter values is not performed by the
ADI routines; the implementation of the MPI routines can make these
tests before calling the ADI routines.

\item Completion of MPI operations (e.g., \code{MPI_Wait}) is handled
by a combination of a completion counter in requests and ADI calls to make
progress, rather than through calls similar to the MPI wait and test
calls.  There is no \code{MPID_Wait} or \code{MPID_Test} operation.

\item RMA (remote memory access) operations are complemented by a
special interface for low-latency operations, particularly in the
passive target case.

\item Collective operations are built out of point to point operations
(though provision is made to replace each collective operation with an
optimized function).  An enhancement is planned that allows the use of
pipelined store and forward and scatter/gather (to collections of processes)
communication.  

\item Dynamic process operations use a similar interface to MPI, but the
process of building the new intercommunicator is made more explicit
through the use of \emph{virtual connections}\index{virtual connections}.
\end{enumerate}

In addition, the ADI is \emph{not} responsible for the construction
and management of other MPI objects such as datatypes, attributes,
error handlers, and reduction operations.  However, hooks are provided to
allow the MPI level to notify the ADI of changes in the other MPI objects.

Most implementors will choose to start with a simple interface based
on only nonblocking \code{readv} and 
\code{writev}-like operations.  This replaces
the ``channel'' interface defined in ADI-2.  An implementation of
ADI-3 is provided with MPICH that is built on this simple
interface; the channel interface and this implementation are described
in \cite{channel3-tcp}.

Another interface that is under development and that will be
documented separately is the method interface.  This defines 
a set of operations that are needed to implement communication on a
single connection.  This interface allows multiple communication
methods to be used in a single MPI program, such as TCP, VIA, and
shared memory.  

\section{High-level Overview}
\label{sec:high-level}

The ADI is relatively rich in functionality.  Before diving into some of the
details, we provide a brief overview of the ADI.
Communication in the ADI takes place between processes.  We call the object
that describes the communication between two processes a
\emph{connection}\index{connection}
and show a simplified block diagram of the objects used by the ADI in
Figure~\ref{fig:adi-overview}.
On each connection, we assume that the low-level communication is
\emph{nonblocking}, thus there are queues of pending send operations and
ordered lists of receive operations.  The lines show communication paths; note
that data may be moved from user memory to user memory without passing through
the send or receive agents.  This allows the ADI to support so-called
zero-copy\footnote{This is zero copy as that term is understood by
  network designers.  This really mean zero \emph{extra} copies, and
  does not mean that the data is not copied from the source to the
  destination process.} methods.

Because the ADI implementation is connection oriented, all details of the data
transfer are hidden within the connection.  This makes it relatively easy to
support multiple communication methods between different processes; for
example, the message format used with TCP communication need not look anything
like the message format used by Infiniband communication within the
same device. 
Similarly, flow control is handled on a connection basis, allowing the use of
whatever method is appropriate for each communication method.
%
% Brian notes that connection oriented is a constraint.  Bill is leaving it in
% because I don't think that it is much of a constraint, and MPID_Comm
% contains a (pointer to a?) MPID_VC array.  

While the term ``connection'' is used, there is no requirement that an MPI
program create all possible connections or use a connection-oriented low-level
protocol.  These are really \emph{virtual connections}, with meaning only for
the ADI and MPI layers.

\begin{figure}
\centerline{\resizebox{5in}{!}{\includegraphics{adi-ov.eps}}}
\caption{Simplified block diagram of the communication paths on a connection
  between two processes.  The dashed lines separate the four communication
  types described below.} 
\label{fig:adi-overview}
\end{figure}

It is important to bear in mind the difference between the operations that are
sufficient to provide the MPI semantics and the operations that may be
necessary to provide a higher-performance match between facilities
provided by the hardware and software.  The ADI endeavors to provide
an effective compromise between a minimal set that can provide the
full functionality and a richer (and larger) set that can exploit the
capabilities of a wide range of systems.  To bridge the gap between
these, the MPICH ADI implementation provides some implementations of
these higher-performance interfaces in terms of a smaller set of
operations.  (In ADI-2 used in MPICH-1, this smaller set was called
the ``channel interface''.)  A more complete discussion of this is
presented in Section~\ref{sec:layers}.

\section{Basic Design Rationale}
\label{sec:basic-design}
In this section, the basic operations that MPI requires of an ADI are
described, as well as the design decisions that we have made based on these
operations.

\subsection{Communication Types}
\label{sec:comm-types}

While all MPI interprocess communication (including MPI-1 and MPI-2)
can be supported by a single, suitably powerful mechanism such as
active messages, the MPI communication semantics described above
suggest four separate types of communication operations.  These are:

\begin{enumerate}
\item Two-party, point-to-point communication.  This is the classic
send-receive operation.  This typically involves coordination between
the sender and the receiver, handling such items as flow control,
rendezvous messaging, and eager message delivery.  Many low-latency
implementations of this kind of communication rely on \emph{polling}
to advance the communication (make \emph{progress}\index{progress} in
MPI terms). 
Others may use a separate thread or an interrupt-driven mechanism (or
a hybrid of both polling and non-polling). 
Because MPI semantics support nonblocking communication of arbitrary
message sizes, any low-level support for communication in MPI must
provide nonblocking, point-to-point communication.  This is most
easily accomplished if the low-level communications is also
nonblocking.

\item Communication that has the property of local-completion. That
is, a communication operation that must complete independently of any
explicit action by the target (destination) process.  This is required
in MPI-1 for the implementation of \code{MPI_Cancel} (in the send
case in most implementations) and is useful for \code{MPI_Abort}.
In MPI-2, local completion is also needed for \emph{passive target}
remote memory access (RMA) operations.  This kind of communication is
typically implemented through the use of active messages or remote
service requests (without polling).

\item Communication for active-target RMA.  In MPI, active target RMA
operations include remote put, get, and accumulate.  These operations
are completed by either an \code{MPI_Win_fence} call made by all processes
in the MPI window object or by the combination of
\code{MPI_Win_complete} and \code{MPI_Win_wait} at the origin (process
that initiating a put, get, or accumulate) and target (process
containing the memory accessed by a put, get, or accumulate)
processes.  This form is similar to the two-party, point-to-point
communication because it can be implemented by using a pure polling
interface. This communication form is separated from the point-to-point
mode because the hardware in some systems allows some of the
active-target RMA operations to be implemented directly by hardware or
low-level software.

\item Communication for passive-target RMA.  These operations
must complete locally.  If these operations must be implemented by
communicating with an agent at the remote process, then some form of
non-polling agent is required, such as an interrupt-driven active
message or a separate communication thread.  As in case three, this
communication form is separated from case 2 (local completion for
MPI-1) because the hardware in some systems allows passive target RMA
operations to be implemented directly.  We expect some systems to
provide an extensive set of operations (e.g., direct access to memory
on an SMP through a shared memory segment), others to provide more
limited access (e.g., remote DMA through special network support), and
others to be implemented on top of a non-polling communication layer
such as active messages.  The ADI design is intended to provide a
common set of entry points independent of the capabilities of the
underlying system.

\end{enumerate}

The ADI design makes no explicit choice between polling and
non-polling implementations.  Instead, it defines several kinds of
polling \emph{points}\index{polling!points} but allows a purely non-polling
(interrupt-driven or separate communication thread) implementation as well.

The four types of communication, of course, can be implemented by a
single, suitably powerful abstraction.  However, achieving high
performance (particularly low latency) requires abstractions that are
close to the operations that are efficiently implemented in hardware.
The emergence of remote access-style operations in networks
\cite{unet,via,infiniband} encourages their use as primitives
(communication types 3 and 4).  Efficient handling of message-passing,
particularly the need to manage the flow of point-to-point
communications and scalable collective communications on top of more
conventional two-sided communications such as TCP suggests
communication type 1.  Finally, the need to handle some MPI-1
operations that are infrequent and not performance sensitive, but must
be handled reliably, suggests communication type 2.  The top level ADI
interface provides direct access to these four types of communication.  Of
course, the implementation of the ADI may implement some of these
communication types in terms of others, such as reducing all four types to
type two (active messages).  

\begin{figure}
\begin{verbatim}
(sketch of figure to be completed later)
1 can implement 3
2 can implement 4
1 can provide emulation of 2, but only approximately
\end{verbatim}
\caption{Sample dependencies between communication approaches}
\label{fig:comm-depend}
\end{figure}

\subsection{Additional Goals}
To ensure that short messages have the lowest possible latency, the
common cases should have direct paths to the low-level data transfer
operations.  In particular, the MPI and API layers
should allow operations to to complete without requiring the creation
of intermediate data structures.  For example, sending a single word
(particularly from a blocking \code{MPI_Send} operation) should not
require creating and initializing internal data structures.  However,
to maintain a simple code base, we will strive to use common code.
This suggests that a basic operation is a simple ``attempt to
send;'' this allows the ADI to attempt to send a short data message
and return success without creating, for example, a queue element that
holds a pending communication operation.  Only if the data cannot be
communicated immediately will the ADI create an intermediate data
structure to hold the state of the incomplete communication.  

Thread safety is another goal of the MPICH2 implementation.  There are
really two separate situations.  One is the use of multiple threads by
the user's code, for example, in a \code{MPI_THREAD_MULTIPLE} mode.
Another case is the use of a single thread by the user but multiple
threads by the MPICH2 implementation.  In addition, because thread
safety introduces extra overhead to ensure that shared data structures
are updated consistently, MPICH2 can be configured for both compile
time and runtime specification of the level of support for user threads.

\subsection{Other Relevant MPI Issues}
MPI defines a number of objects such as requests, windows, and
communicators.  In many cases, these objects are natural choices for
use within the ADI.  Using these objects directly, rather than defining
different objects for use by the ADI and translating between the MPI and ADI
versions, avoids unnecessary 
overhead within the device.  Let us look at several of these objects.

\subsubsection{Structures Involved in Communication}

\paragraph{MPI Requests.}
The use of nonblocking operations to implement the low-level
communication requires an object to hold the current state of the
communication and to record completion of the operation.  The natural
place to store this within the MPI request.  Pending send operations must
also be saved in a first-in-first-out queue (to maintain the message
ordering guaranteed by MPI); this suggests that the queue contain MPI
requests.  On the receive side, the message-matching defined by MPI
suggests saving the requests in an ordered list.  Note the asymmetry
between sends and receives.  On the send side, requests are placed in
a strict FIFO queue for each communication path (to maintain ordering
of messages).  On the receive side, requests for unmatched receives
are kept in an ordered 
list, and this list, because of the wildcard source receive
(\code{MPI_ANY_SOURCE}) is (logically at least) shared by all
communication paths.  Similarly, requests for unexpected messages
(messages sent but for which no matching receive has yet been issued)
are kept in an ordered list.
Requests are also the logical place for the data structures relevant to packing and
unpacking from complex datatypes to simpler layouts such as contiguous
data buffers.  This is discussed in more detail under MPI datatypes.

\paragraph{MPI Datatypes.}
MPI communication can be specified using datatypes that describe
complex layouts in memory.  An MPI implementation must convert these
descriptions into data layouts that can be conveniently moved by the
low-level communication layers.  Such layers typically support only
contiguous memory regions or Unix ``io vectors'' (\code{struct
iovec}); MPI provides more general forms of data layouts.  However,
while the MPI datatypes are sufficient to express most forms of
communication, there are no routines defined by the MPI standard to
pack or unpack only a 
fraction of a datatype.  For example, there is no MPI-defined method
to pack as much as fits into a fixed sized buffer and return enough
state so that a subsequent pack can pick up where the last left off.
Such an operation is needed for any algorithm that packetizes data or
that pipelines data transfers.  Because this operation is needed both
to handle packing noncontiguous data into temporary buffers needed by
low-level communication routines (such as TCP \code{write} or
\code{writev}) and by high-performance algorithms for collective
communication, we have introduced a new data structure that is used to
pack and unpack buffers described by MPI datatypes.  This new
structure is called a \emph{segment}\index{segment} and is stored in a
structure 
type named \code{MPID_Segment}\index{MPID_Segment}.  Segments are
discussed in more 
detail in Section~\ref{sec:segment}.

Thus, to handle the need to pack and unpack data, MPI requests also
contain a segment.  Combined with the datatype and the user-buffer,
this gives enough information to move the data to and from the user
buffer, even if an intermediate buffer is needed.  Note that where
possible, no intermediate buffer is used.  For contiguous data, and
for more general data formats that the underlying communication level
supports, data can be moved without using any extra buffers.  Segments
are required to handle the general case.  

\paragraph{Consequences.}
These considerations suggest that the MPI request (more
specifically, the internally-defined structure to which an MPI
request refers, which is \code{MPID_Request}) is the key object.  The
\code{MPID_Request} is used to store the progress of communication and
order communication between processes.  The ADI will use the request
as its basic object.

The
consequence of this is that the ADI point-to-point communication routines
should usually return a request.  The only exception is that blocking
communication routines should not return a request if the
communication is already complete.  This allows
the blocking communication routines to
return completion without ever creating and managing a request.
This suggests that the ADI interface for point-to-point communication
look something like the following: 

\begin{small}
\begin{verbatim}
MPID_Send( buf, count, datatype, tag, <communicator info>, &request )
MPID_Isend( buf, count, datatype, tag, <communicator info>, &request )
MPID_Issend(...)
... similarly for other point-to-point functions
\end{verbatim}
\end{small}

\noindent
The exact form of the arguments that specify the communicator
information will be discussed later.
This allows a ``blocking'' send to return a request if the operation
has not completed, giving the calling routine more control over
what steps to take to complete the request.  It sets the request
pointer to \code{NULL} if it was able to complete without creating a request.

% An alternative design would have fewer routines but with additional
% arguments.  This design was chosen because it both provides a shorter
% code path (since the MPI send mode is known at compile time) and
% because it is easy to use C preprocessor macros to convert this form
% into calls to a single, parameterized routine but the reverse
% direction cannot be so implemented.
% (e.g., could have a single MPID_Send(..., mode, is_immediate )
% instead of multiple routines).

The blocking receive case is similar to the blocking send case.  If,
when the receive routine is called, the data is available, no request
should be created (the cost of creating a request isn't the real
issue, it is the cost of initializing and managing the request).  

Recall
that in the receive case, there are two kinds of receives: posted (but
unmatched by an incoming send) and unexpected (sent but unmatched by a
receive).  For thread-safety\index{thread safety!issues}, operations on these
two lists must be 
made atomically.  These operations include 
\begin{itemize}
\item check posted and return if found; else insert into unexpected queue
%(for incoming messages)
\item check unexpected and return if found; else insert into posted queue
% (for posting a receive)
\end{itemize}
The MPID request is the appropriate list element to use in
constructing these structures.

While the request is allocated by the ADI, \code{MPID_Datatype}s are
allocated by the MPI implementation.  In fact, most of the MPI
objects, except for requests, will be allocated by the MPI
implementation rather than within the ADI.  This is an arbitrary
choice; for greatest generality, the ADI could be responsible for
allocating all MPI objects.  However, we believe that most users of
MPICH and ADI-3 will not need that flexibility, and managing the
objects within the MPI layer instead of the ADI layer simplifies the
implementation of the ADI.  However, to make provision for any
ADI-specific features that must be associated with an MPI object, the
definition of the structure associated with each object includes
\begin{verbatim}
    MPID_DEV_xxxx_DECL
\end{verbatim}
where \code{xxxx} may be \code{COMM}, \code{DATATYPE}, etc.  This
provides a simple way to extend the objects defined by the MPI layer
without forcing the ADI to provide a complete implementation.
Whenever an object is created or destroyed, the MPI layer can call a
\emph{hook}\index{hook routine} routine with a name of the form
\begin{verbatim}
    MPID_Dev_xxxx_create_hook( pointer to object, ... )
    MPID_Dev_xxxx_destroy_hook( pointer to object, ... )
\end{verbatim}
The other parameters will be defined as it becomes clear what is needed.  For
example, in the case of communicator creation, a pointer to the old
communicator may be needed.

These routines are called after all other creation operations take
place and before any of the destroy operations take place.

\textbf{These calls have not yet been implemented}

\subsubsection{Communication Contexts and Groups}

MPI Communicators describe both a collection of processes (an
\code{MPI_Group}) and a unique communication context.  As described
later, in MPICH and ADI-3, the communication context is encoded as an
integer.  The target process of communication in MPI is described by a
rank in a group associated with a communicator.  While this suggests
that MPI groups are fundamental data structures, in MPICH-2, groups
are not used in the ADI at all.  Instead, each communicator maintains
an array of \emph{connections} that are indexed by the rank.
Using these connections, the MPI implementation provides complete
support for the MPI group 
operations (e.g., \code{MPI_Group_union}).  Explicit MPI Groups are
not used by the ADI.

% The following comment was made when I thought that we would just use the
% rank to index into the connection table.  Instead, a multimethod device
% should trap the any-source case and call a special routine.
% (there is
%a special connection for receives with rank \code{MPI_ANY_SOURCE}).

\subsection{Completing Point-to-Point Operations}

To complete some particular MPI communication (described by a request,
such as in a call to \code{MPI_Wait}, it is necessary to have the ADI
respond to \emph{any} pending communication.  Thus, it is not
necessary to provide the ADI with a collection of request to test or
wait on.  Instead, we merely need to ask the ADI to try to make
progress on communication and then check (using the completion counter
in each request) whether any particular MPI requests have completed.
In the absence of threads, a simple interface would look like
\begin{verbatim}
   MPI_Waitsome( ... )
   {
   while( no completed requests found ) 
       for (i=0; i<count; i++) {
           if (any requests done, return those as complete)
       }
       MPID_Make_progress( TRUE );
   }
\end{verbatim}
However, if there are multiple threads, particularly if there are
separate threads that can complete communication\index{thread safety!issues},
then this code has a race condition, caused by the API for progress.
Instead, a slightly more complex interface is needed to
eliminate any race conditions.  For example,
\begin{verbatim}
    MPI_Waitsome( ... )
    {
      while (1) {
        MPID_Progress_start( );   // Notes that we are about to
                                  // check ready flags.  No completion
                                  // counters will be set to complete
        for (i=0; i<count; i++) {
            if (any request done) { save info on request }
        }
        if (no requests done) 
            MPID_Progress_wait();
        else {
            MPID_Progress_end();
            break;
        }
      }
    }
\end{verbatim}
The interface for test operations is similar, except that
\code{MPID_Progress_wait} is replaced with \code{MPID_Progress_test},
and no outer loop is needed.

In a polling implementation, the ``start'' and ``end'' calls are
no-ops and the ``wait'' and ``test'' calls are blocking and
nonblocking polling calls respectively.  In a nonpolling
implementation, the ``start'' and ``end'' calls may set and clear a thread lock
or access lock, and the ``wait'' and ``test'' calls may yield to a
communication thread (in addition, the wait version could wait to be
signaled through a condition variable).  This interface allows us to
use the same code for completing MPI nonblocking operations
independent of the choice of polling, nonpolling, threaded, or
nonthreaded implementations of the ADI.

\subsection{Supporting Collective Operations}
When implementing collective communication algorithms, the ability to
both store and forward data is important for performance.  For
example, when complex MPI datatypes are used, it may be necessary when
receiving data to first receive into a temporary buffer and then
unpack that data into the user's buffer.  Forwarding this same data on
(for example, within a broadcast) in a separate MPI send operation
then requires repacking the data from the user into a temporary
buffer.  To enable an ADI implementation to avoid this cost, and to
make it easier to efficiently write the collective communication
algorithms, the ADI will provide a variety of store and forward, scatter,
and gather operations.  Note that these operations can be emulated
using only point-to-point; as described above, these can be built on
top of simpler, point-to-point communication.  These interfaces are
still under design.

%If MPI providing only communication of contiguous (or simple io
%vector) blocks of data, it might be possible to 

\paragraph{Consequences.}
Determining completion in the store and forward or multisend cases may involve
more than one communication 
operation and possibly multiple communication methods.  This argues
that the status of a communication be tracked with a completion
counter rather than a simple flag, since multiple communication
operations may be working with the same data buffer.  

To best exploit the fact that the same data is both being received and
sent, the ADI should be able to provide pointers to ``good'' memory
for these operations.  For example, such memory may be in a special,
pinned page or within a sophisticated NIC.  

The algorithms for efficient collective communication provide some
information on the kinds of multi-party operations that are required.
The ADI does not support the most general of these operations; the
goal is to allow an MPI implementation to efficiently and correctly
support the more common collective communication operations such as
\code{MPI_Bcast}, \code{MPI_Scatter}, and \code{MPI_Allgather}.

The very first version of ADI-3 does not include these more general
multi-party communication operations.  It is the intent of ADI-3,
however, to develop an efficient method for describing and
implementing the operations needed for the MPI collective operations.
We may also consider special support for collective argument checking,
that is, checking that the parameters to a collective MPI routine are
consistent amoung all of the involved processes, perhaps using a
special header format or layered header format.

\subsection{Data Segments}
\label{sec:segment}

MPI datatypes are very general; a single instance of a datatype can
describe an arbitrarily large amount of data that is not necessarily
contiguous in memory.  Further, MPI datatypes can be very concise;
a vector datatype of a structure (that itself is not contiguous)
describes a very complex memory layout with just a few words of
memory.  Further, MPI dataytpes may be described using only five
different basic loop types \cite{datatype1,datatype2).
Because few if any low-level communication layers support the
full generality of MPI datatypes, it is sometimes necessary to pack
and unpack data to intermediate buffers.  In addition, it may be
necessary to pack or unpack a single MPI datatype with multiple calls;
this operation is not supported by the \code{MPI_Pack} and
\code{MPI_Unpack} routines.  For example, the code
\begin{verbatim}
    MPI_Type_vector( 1000000, 1, 237, MPI_DOUBLE, &newtype );
    MPI_Type_commit( &newtype );
    MPI_Send( buf, 1, newtype, ... );
\end{verbatim}
passes a single instance of an MPI datatype that describes 8 MB of
data, even though it is completely described by the base address and
four integers, one of which is the size of a \code{double}.
Further, because this is a vector type, it cannot be
represented efficiently with a \code{struct iovec}.  If the
underlying communication layer can only send a maximum of 64K at a
time (for example, using a shared memory pool or a remote-memory
communication area), it is necessary to incrementally pack this
datatype, 64K at a time, into a temporary buffer.  The segment
routines provide this capability.

These routines are also needed for some implementations of the
collective communication routines.  Many of the better (and in some
cases, the best) algorithms for the collective operations in MPI can
be cast, at least for systems that are homogeneous is data
representation, in terms of operations that view the data to be
communicated as a contiguous range of bytes and that send different
parts to different processes.  To implement these algorithms for MPI
requires handling MPI datatypes; even in a single collective
operation, some MPI processes may provide a simple, contiguous data
buffer while others specify a complex datatype.  To implement these
algorithms requires a method to extract parts of a data buffer, viewed
as a range of bytes.  This is a simple variation on the routines that
can incrementally pack (and unpack) a data buffer described by an MPI
datatype.  


\textbf{Question: We need to tie down the details of the segment
creation and pack/unpack operations.}

Issues with the segment routines:
\begin{enumerate}
\item Who allocates storage for the segment?  That is, where does the
contiguous buffer come from?  In many cases, it would be nice if it
was memory that was convenient for the ADI-3 device.  In the case of
shared memory or RMA-networks, using special memory saves a memory
copy.

\item Who sets the amount of data to pack or unpack in each call?
The specific concern here is to not stop in the middle of a natural
data item, e.g., 3/8ths of the way through a double.  The design here
allows the routines to make slight adjustments in the amount of data
packed or unpacked in order to stay on ``natural'' boundaries.  

\item How much do these routines need to know about MPI Datatypes?
We'd like them to be generic so that they could be shared with other
projects such as PVFS and HDF5.  
\end{enumerate}


\subsection{Remote Memory Access}
\label{sec:rma-design}
The MPI remote memory access model was deliberately designed to have
very loose (but precise) synchronization requirements and to make
minimal \emph{demands} on 
the 
underlying hardware.  For example, it is possible within the MPI model
to support non-cache-coherent systems (such as the NEC vector
supercomputers)\footnote{The ADI-3 design, however, does assume cache
coherence with local memory; that is, the memory system on each node
is cache coherent.}.  However, the model also \emph{allows} an MPI 
implementation to exploit special hardware capabilities.  

%
% The following paragraph is intended for readers that are not familar
% with the details of MPI RMA.
The MPI specification is very careful in describing when a process's
memory window is accessible to other processes (the \emph{exposure
epoch}\index{exposure epoch}\index{epoch!exposure}) and when a process
may be performing RMA operations (the 
\emph{access epoch}\index{access epoch}\index{epoch!access}).
Understanding these is necessary in developing 
a correct MPI implementation.  However, these concepts are
deliberately made as general as possible to allow the greatest
flexibility to an MPI implementation.  In the discussion below, we
will usually not refer to the access or exposure epochs.  However, if
there are questions as to what the terms ``synchronization'' or
``completion'' mean in the RMA context, consult the discussion of the
RMA epochs in the MPI standard.

The MPI specification has a number of ``as if'' rules, such as ``as if
only one process accesses a memory window at a time''.  Naturally, if
an implementation can perform an operation more efficiently without
violating such ``as if'' rules, the implementation is free to do so.
An example of this is passive target updates to disjoint regions in a
memory window; 
the ``as if'' rule says that these must appear ``as if executed
sequentially,'' but an implementation can allow concurrent updates if
they are known a priori to be disjoint.  This suggests that operations
be aggregated so that the range of affected bytes within the target
window is known.  Fortunately, the MPI specification allows such aggregation.

\subsubsection{RMA Aggregation}
One of the most misunderstood parts of the MPI RMA specification is
the issue of when operations take place, particularly with the poorly
named \code{MPI_Win_lock} and \code{MPI_Win_unlock} routines.  MPI RMA
allows the implementation considerable latitude in the timing of
operations.  An important case in point is aggregation (combining) of RMA
operations.  The approach of aggregating RMA operations has been
developed in the BSP approach, and the MPI specification was designed
to support this technique.  For example, the following sequence of MPI
calls

\begin{small}
\begin{verbatim}
    MPI_Win_lock( MPI_LOCK_SHARED, rank, 0, win );
    MPI_Accumulate( &one, 1, MPI_INT, rank, 0, 1, MPI_INT, MPI_SUM, win );
    MPI_Win_unlock( rank, win );
\end{verbatim}
\end{small}

\noindent
can be converted into a single, atomic update operation on some
systems (particularly on those that have no direct access to remote
shared memory, such as a system that only supports TCP communication).

To allow low-latency implementation of single-element remote updates
(e.g., put or accumulate), the ADI design allows the MPI
implementation to perform aggregation of RMA operations without
calling the ADI.  This eliminates a layer of function calls in these
simple cases\footnote{In addition, the MPI functions could be inlined by a
  suitably sophisticated compiler,
removing all function calls.}.  
%Of course, for some operations and for
%some devices, ...
The ADI provides a set of definitions
that are used to decide the aggregation threshold, in terms of number
of bytes and operations.  The device can also specify that no
aggregation is done at the MPI level, giving the ADI greater control
at the cost of additional function calls.
\textbf{How does the device specify this?}
%
% Still to do: describe the data structure used to aggregate
% operations.  This may limit the operations aggregated to those that
% use simple datatypes (e.g., only the predefined types or only simple
% strided or short index types).
%

\subsubsection{Nonblocking RMA and Remote Completion}
% Also yet to do: how to indicate remote completion.  We should be
% careful to give the device a great deal of flexibility here, as the
% specific choices of remote flags and counters can create
% performance issues that are not present in MPI.
The MPI RMA operations are nonblocking.  Thus, there must be some way
to indicate both local and remote completion.  MPI provides users
with three different mechanisms for marking completion in their code:
\begin{description}
\item[Fence.]This is essentially a barrier synchronization, similar to
the Cray SHMEM routine \code{shmembarrier}.  To allow the ADI to exploit
hardware and 
software features similar to those used by Cray \code{shmembarrier} in the
Cray T3D and T3E, the ADI provides a similar routine
(\code{MPID_Win_fence}), with the 
difference that it applies to MPI window objects on arbitrary groups
of processes. An ADI implementation for a system that
provides an efficient fence operation only on all processes can, of
course, test for that case and execute different code when not all
processes are involved in the MPI window object.

\item[Passive.]This is a kind of two-party synchronization since only
the origin and target processes are involved, rather than all members of the
group of the MPI window object.  The ADI provides a
simple completion counter variable that is zero on completion; this allows the
flag to be a counter that contains the number of uncompleted
operations or a simple boolean that indicates whether the operation is
complete.  The MPI calls that express this kind of synchronization are the
misnamed 
\code{MPI_Win_lock} and \code{MPI_Win_unlock}.  Note that calls by a
process to \code{MPI_Win_lock} for its own rank (i.e., ``lock my
window'') are different in behavior from calls to a remote process
because the local process may use non-MPI operations to access the
memory window (e.g., through simple references or assignments).  ADI-3
assumes a cache-coherent memory system, which allows some important
simplifications in handling these operations.

\item[Scalable Multiparty.]In this mode, not all processes in the
window are involved (unlike the fence case), but both origin and
target processes make calls to indicate when operations must complete.
The MPI calls used to express this kind of synchronization are
\code{MPI_Win_post}, \code{MPI_Win_start}, \code{MPI_Win_complete},
and \code{MPI_Win_wait}.
This form can also be handled with counters that are updated by each
partner process.  Efficient handling of this approach remains a
research issue, however.
\end{description}

\subsection{Contexts for Collective, File, and Window Operations}
\label{sec:comm-for-coll}
MPI requires that different kinds of communication be
non-interfering.  That is, communication for collective operations
such as \code{MPI_Bcast} and point-to-point operations such as
\code{MPI_Send}, even on the same communicator, must not interfere
with each other.  With MPI-2, this is extended to operations on MPI
File and Window objects, both of which may involve some communication
by the implementation.

MPI also requires that communication within different
communicators be noninterfering.  Many implementations achieve this by
using a hidden (to the MPI user) \emph{context id}\index{context id},
which is simply an 
integer that is communicated along with the tag, source, etc.  
One approach that can be used to ensure that
communication of different types be non-interfering is to use
different communicators; in MPICH-1, each communicator was created
with a second, hidden (from the user) communicator that was used for
communication implementing collective operations.  

However, this approach has a number of drawbacks, particularly in
organizing the code.  In addition, with files and windows as well,
three hidden communicators (in addition to the one used for point-to-point
communication) might be needed\footnote{An alternative is
to perform a shallow duplicate (without invoking attribute copy
functions) of the communicators passed into the file and window
creation routines.}.  Finally, the implementation of intercommunicator
collective operations, also introduced in MPI-2, adds additional
complexity.  In MPICH-2, we take a different route.  Instead of
creating hidden communicators, we allocate context ids in groups of
four.  The API routines for communication take an explicit context id
offset as a parameter.  These offsets have the following values.  For
intracommunicators: 
\begin{description}
\item[0]--point to point communication, 
\item[1]--collective communication, 
\item[2]--communication for files, 
\item[3]--communication for window objects.  
\end{description}
For intercommunicators: 
\begin{description}
\item[0]--point to point, 
\item[1]--collective in group A, 
\item[2]--collective in group B, 
\item[3]--collective over both groups.
\end{description}
(Note that a single context value for ``collective in local group'' is
adequate; however, having separate context values provides stronger
separation and aids in debugging.)
%
% A single context for file and window may not be enough.  However, it
% may, given the collective operation of the calls within the IO and
% RMA sections.

\subsubsection{Context Id Generation}
\label{sec:context-id-generation}
Because libraries are encouraged to use it, the operation
\code{MPI_Comm_dup} should be efficient.  In particular, 
where possible, it should be a local operation (involving no
communication).  This is possible, at least for the first few dup's of
a communicator, if each communicator caches some extra context ids
when it is created.  The MPI implementation will maintain a cache of
context values in a communicator; communicators created by dup'ing
that communicator will take a value from the cache.  If the cache is
empty, the MPI level will make a collective call to the ADI-3 routine
that returns context ids.  To simplify this, the ADI-3 routine returns
a single value; the MPI level will multiply this by a fixed constant
size to create a sequence of consecutive context ids.  In addition,
the MPI level will keep track of returned context ids (made available
by freeing a communicator) so that the ADI can be told when a context
id is available again.

In the current implementation, context id generation is provided by a
utility routine above the level of the ADI.  Because there are opportunties
for implementations on hardware that provides remote memory operations,
such as accumulate, future versions of the ADI may allow the ADI
implementation to replace the default context id generation.

\subsection{Dynamic Processes}
There are two major goals here for the ADI:  scale to large but not
necessarily enormous numbers of processes and maintain modularity so
that it is relatively easy to maintain and extend the implementation.
A secondary goal is to structure the code so that initialization and
rundown are efficient and do not spend a great deal of time setting up
facilities that a program never uses.

MPI-2 adds dynamic process management.  A consequence of this is that
there are no absolute and global process ids.  This observation
suggests that all communication be considered locally in terms of
(possibly virtual) connections to processes.  Unlike ADI-2, there are
no data structures that map a rank in a communicator into a ``global
rank'' (i.e., rank in \code{MPI_COMM_WORLD}).  Instead, communicators
have arrays of virtual connections that are indexed by the rank.

There is a kind of local process id that is used by the MPI group
operations.  However, it is not critical item and, since the MPI group
operations are implemented above the ADI, we do not discuss them here.
The one exception is the routine to provide a mapping from any
rank in a communicator to a value that uniquely specifies an MPI
process \emph{relative only to the calling process}.  That is, we do
not need a value that is globally correct, only one such that if two
ranks in two different communicators \emph{on the same process} refer
to the same MPI process, then this mapping will give the same value on
that process.  Such values are called ``local process ids''.

\section{ADI Layers}
\label{sec:layers}

It is expected that many implementations of the ADI will be \emph{layered},
building the four types of communication described in
Section~\ref{sec:comm-types} in terms of simpler communication methods.
This section outlines several possible implementations.

\subsection{Socket (TCP) communication}
Socket-based communication provides a two-party communication similar
to the type 1 communication in Section~\ref{sec:comm-types}.  A simple
implementation can map all four kinds of communication into simple
\code{read} and \code{write} calls as follows (with some caveats):
\begin{enumerate}
\item Point to point communication is a close match.  Some care is
needed to handle flow control.

\item Local completion communication is more difficult.  A simple (and
not entirely correct) approach is to simply convert these operations
(e.g., \code{MPID_Cancel_send}) into a message that is sent on the
same socket as the type 1 point to point communication.  All that this
requires is that messages sent on the socket connecting two processes
include a header that describes the type of message (e.g., MPI message
envelope, MPI cancel message, MPI cancel acknowledgement, etc.).  For
those familar with the channel device in ADI-2, these are just the
packet types.

\item Active target RMA.  This can also be converted into simple
messages sent on the same socket.  Note that there are some
complications in handling complex MPI datatypes.

\item Passive target RMA.  Just like active target RMA.  Note,
however, that for correct behavior, the receive agent must be called
at least occasionally, even if the target process makes no MPI calls.
\end{enumerate}

The progress engine is implemented by using \code{select} or \code{poll}, and
is a pure polling approach. 

To correctly handle the local completion (type 2) and passive target
(type 4) communication when using sockets, there must be an
asynchronous communication agent.  One easy way to do this is to take
the progress engine and place it in a separate thread.  The operating
system then guarantees that the progress engine is called often enough
to ensure that the locally completing communication operations make
the necessary progress.  However, to illustrate the advantages of the
ADI design (in terms of four separate communication types), consider a
different version where there are two sockets   
between communicating pairs of processes.  The first socket is used in
a polling mode for point-to-point and active target communication
(types 1 and 3).  This provides the low-latency associated with
polling models.  The second socket is used for the locally-completing
communication (types 2 and 4), and uses a separate thread, process, or
\code{SIGIO} to ensure local completion.  This approach provides correctness
with the MPI progress model without sacrificing the low latency of the
polling approach for the more common operations.  Note that such an
implementation must still guard some data structure updates where
different communication threads might update the same data structure.

Of course, other approaches are possible, including one that uses a
mix of polling and nonpolling even on type 1 and type 3 communication.

\subsection{Remote Put}
\textbf{Not yet written}

\subsection{Shared Memory}
\textbf{Not yet written}

% \textbf{Old text describing the plan for this section}
% This section describes 3 layered implementations.  First, just
% implement the TCP part.  Build the rest in terms of special messages.
% The TCP implementation has two important sub cases: pure polling
% (doesn't implement cancel-send; passive-RMA) 
% and mixed (separate receive-agent thread for those messages).  

% Second, include VIA-style remote put, only for contiguous put.  Show
% growth direction.  Passive RMA in terms of the AM handler.

% Third, direct shared memory for passive operations: direct operation
% on memory.

\section{Summary}
\label{sec:adi-summary}
This section provides a short summary of the ADI routines.  Complete details
are provided in the ADI-3 reference manual.

\subsection{Point to point communication}
Each simple MPI communication operation has its counterpart here:
\begin{verbatim}
MPID_Send
MPID_Ssend
MPID_Rsend
MPID_Isend
MPID_Issend
MPID_Irsend
MPID_Recv
MPID_Irecv
MPID_Request_free
MPID_Iprobe
MPID_Probe
\end{verbatim}
Note that these do not include the buffered send nor the two send-receive
operations (\code{MPI_Sendrecv} and \code{MPI_Sendrecv_replace}).
Send-receive operations are described in terms of separate send and receive
operations; buffered send may use the optional \code{MPID_tBsend} or may
simply use \code{MPID_Isend} with the sample implementation described in the
MPI-1 standard.

Several MPI routines are represented by slightly different routines.
These include 
\begin{verbatim}
MPID_tBsend        - Used to optimize buffered sends (MPI_Bsend etc.)
MPID_Cancel_send   - Separate routines for cancelling sends and receives
MPID_Cancel_recv
\end{verbatim}

In addition, the persistent communication routines have MPID equivalents:
\begin{verbatim}
MPID_Send_init
MPID_Ssend_init
MPID_Rsend_init
MPID_Recv_init
MPID_Startall
\end{verbatim}
These are provided so that the device can effectively manage the
persistent request, which may require allocating a device-specific
request either when an init routine (e.g., \code{MPID_Send_init}) is
called or when the request is started with \code{MPID_Startall}.  

Note that there is no direct access to the message queues by the MPI
layer.  We recommend that the device implement the message queue
interface defined for external tools and used by Totalview.  This
interface is described in \cite{pvmmpi99-totalview} and the files
\file{mpich2/src/mpi/debugger/} in MPICH2.

\subsection{Completion of Point to point communication}
Completion of nonblocking (or incomplete in the case of a request
returned by \code{MPID_Send}, \code{MPID_Ssend}, \code{MPID_Rsend}, or
\code{MPID_Recv}) operations is handled by calling the progress engine routines
and checking the completion counter value in a request.  The progress engine
routines are
\begin{verbatim}
MPID_Progress_start
MPID_Progress_end
MPID_Progress_test
MPID_Progress_wait
\end{verbatim}
This set of routines is required in order to provide an
interface to the request's \code{completion counter} value that is thread-safe.
In addition, there is a routine that indicates \emph{polling
  points}\index{polling!points}; 
places in the MPI code where a polling implementation should check for
incoming messages.  This routine is nonblocking and is 
\begin{verbatim}
MPID_Progress_poke
\end{verbatim}

%% \subsection{Data Segments}
%% The routines to incrementally pack and unpack a data buffer given a
%% user buffer and an MPI datatype are:
%% \begin{verbatim}
%% MPID_Segment_pack_init
%% MPID_Segment_pack
%% MPID_Segment_unpack_init
%% MPID_Segment_unpack
%% MPID_Segment_free
%% \end{verbatim}
%% These are used to implement both the \code{MPI_Pack} and
%% \code{MPI_Unpack} routines as well as in the default implementations
%% of the MPI collective communications routines.  Most implementations
%% of the ADI-3 will also use these routines to handle packing and
%% unpacking messages defined by MPI datatypes.

%% Comments:  For some MPI datatypes, it is sometimes possible to
%% describe the buffer with a Unix-style \code{struct iovec}.  Many
%% devices provide low-level and sometimes even efficient for
%% \code{iovec} descriptions.  One unanswered question is whether we
%% should have a \code{MPID_Segment_pack_iovec} or whether there should
%% be a way to check whether the Segment routines are needed at all
%% (i.e., they aren't needed for datatypes that are contiguous or that
%% are described efficiently by a \code{struct iovec}).  

\subsection{Starting and Stopping}
The routines to start and stop the ADI match the MPI counterparts.
Note however that there are no calls to implement ``is initialized'';
this is managed entirely at the MPI level.
\begin{verbatim}
MPID_Init
MPID_Finalize
\end{verbatim}

\subsection{RMA}
The RMA routines haven't been set yet.  However, they will include
MPID versions of put, get, and accumulate, along with a few calls to
handle the aggregated operations (e.g., lock/accumulate/unlock).
In addition, we expect to separate the \code{MPI_Win_lock} and
\code{MPI_Win_unlock} into two kinds of operations: non-local, where
the operation is really a start/end RMA operation, and local, where it
really is lock/unlock.

One question is whether there should be support at the MPI layer for
caching data type descriptions (e.g., for complex MPI datatypes to be
applied at the target process) and the corresponding ADI routines, or
whether this should be handled entirely by the ADI.

The ADI routines that support RMA include
\begin{verbatim}
MPID_Win_put
MPID_Win_get
MPID_Win_accumulate
MPID_Win_do
MPID_Win_fence
MPID_Win_start
MPID_Win_end
MPID_Win_local_lock
MPID_Win_local_unlock
\end{verbatim}
\code{MPID_Win_do} is used to send aggregated operations to the ADI.  This
provides a pointer to a description and an indication of whether this starts,
ends, or continues a sequence of RMA operations.  \code{MPID_Win_start} and
\code{MPID_Win_end} are used for nonlocal uses of \code{MPI_Win_lock} and \code{MPI_Win_unlock}.

We may want \code{MPID_Win_do_put}, \code{MPID_Win_do_get}, and
\code{MPID_Win_do_accumulate} instead of a single \code{MPID_Win_do}.

\subsection{Dynamic Processes}
The MPID routines are similar to the MPI routines.  Undecided: does
the MPI layer or the ADI layer setup the communicator?  The ADI layer
needs only provide the pointers to the connection structure; the MPI
layer could build the communicators and access the context id values.

\subsection{Device Hooks}
To allow the device to track the creation and destruction of MPI
objects (other than requests), the device may define hook routines.
These have the form
\begin{verbatim}
MPID_Dev_xxx_create_hook
MPID_Dev_xxx_destroy_hook
\end{verbatim}
where \code{xxx} is the object type, such as \code{comm},
\code{datatype}, \code{group}, etc.  Many devices will define these as
C preprocessor macros that expand to nothing, thus eliminating any
function calls.
