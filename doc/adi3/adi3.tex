\documentclass{article}
\let\file=\texttt
\let\code=\texttt
%\usepackage{times} % Necessary because acrobat can't handle fonts properly
\usepackage{epsf}

\def\questions{\ifvmode\else\par\fi\paragraph*{Questions:}}
\def\URL#1{\texttt{#1}}
\let\note=\marginpar

\begin{document}

\title{A Third Generation Abstract Device Interface for MPICH}
\author{David Ashton, William Gropp, Ewing Lusk, and Debbie Swider}
\maketitle

\pagenumbering{roman}
%\setcounter{page}{3}
\pagestyle{plain}
{\parskip=0pt
\tableofcontents

\bigskip
}
\bigskip

\pagestyle{plain}
\clearpage

\pagenumbering{arabic}
\setcounter{page}{1}


\section{Introduction}
The MPICH implementation of MPI has used an abstract device interface (ADI) to
provide an easy way to port MPI to new platforms.  The first version of the
ADI was based on a message-passing model, and was designed to allow MPICH to
make use of underlying message-passing systems such as Intel's NX or IBM's
MPL.  The second version of the ADI was a relatively minor change intended to
allow for more experimentation and to allow the device more control over the
formats of datatypes and communicators.  The Globus device \cite{mpich-globus}
made use of these features.

With the success of MPI, most other message-passing systems (for scientific
computing) have disappeared.  Many massively parallel computer vendors have
also disappeared, and the remaining ones all have their own implementations of
MPI.  At the same time, high-performance interconnects that provide some form
of OS-bypass have emerged (Myrinet), along with proposed commodity
standards (VIA, Infinaband).  There is increasing interest in multithreaded
MPI applications (in fact, several vendor MPI's already support multithreaded
applications) and in large clusters of SMPs.  Support for MPI-2, including
process creation, one-sided communication, and high-performance I/O (though
partially addressed with ADIO \cite{ADIO}) is also needed.
In order to support research
into MPI implementations, MPICH needs a new ADI that addresses the new
features of MPI-2, high-performance user-mode interconnects, multithreaded
environments, and experiments in performance enhancements for both
point-to-point and collective operations.  

\subsection{Related Work}

This section will cover other MPI implementation work, including:

Tony and Boris argue for a single protocol that moves small blocks fast is
better than a multi-level protocol \cite{techreport}.

Several groups have looked at receiver rendezvous; this may be particularly
helpful for wide-area networks.

NEC (as well as us) have looked at datatype optimizations.

The LAPI-based implementation uses remote-memory-access.

\section{Requirements}
This section lists requirements that we address in the ADI-3 design.

\subsection{General Capabilities}
\begin{enumerate}
\item Thread-safe
\item Full MPI-2 (and MPI-1, including cancel for sends)
\item Support multimethod, including IMPI
\item Uniform configuration support (e.g., environment variables for tuning)
\item Complete and deep instrumentation (e.g., ADI instrumented)
\item SMP friendly (e.g., no busy waits)
\item Support external scheduling systems
\item Support overlap of communication and computation (using threads or
  asynchronous operations)
\end{enumerate}

\subsection{Performance Issues}
\begin{enumerate}
\item Collective implementations should allow pipelining optimizations (store
  and forward).  This optimizes the long-data case
\item Collective short-cuts for join/copy/concatenate; this optimizes the
  short-data case.
\item Collective operations can exploit support for low-level multi-party
  operations such as IP (unreliable) multicast
\item Access to network topology
\item Make use of system network topology
\item Scalable and low latency (in shared-memory case, this means no locks)
\item Direct (and short) path for common operations
\item Asymptotically fast bandwidth
\item Fast and efficient non-contiguous datatypes
\item Fast one-sided support
\item No performance penalty for thread-safety in single-threaded case
\item Are there opportunities for I/O communication?
\end{enumerate}

\subsection{Job Startup and Rundown and Abort}
\begin{enumerate}
\item Abort must be rock solid.
\item Process creation through an API that allows use of third-party job
  managers and schedulers.
\item Scalable delivery of executable to hosts when (a) executable is not on a
  shared system and/or (b) it is preferable to run from a local disk.  Should
  there be version   control when running the same, unchanged, executable?
\item What info values should be standardized?  For queuing systems, we could
  consider host, user, queueclass.  For interconnection, proxy would be
  useful.  (Some of these have been used by STAMPI
  \URL{http://guide.tokai.jaeri.og.jp/program/eng/software}).
\item All resources must be released at the end of a job, even if it is killed
  with an uncatchable signal.  This is so difficult as to not even be strictly
  possible (deficiencies in the OS design), but we can reduce the windows of
  vunerability by logging all permanent resources (e.g., process ids, SYSV
  memory segments and semaphores, files, IP ports) into a file or database.
\end{enumerate}

\subsection{Interoperability (IMPI and Java)}
\begin{enumerate}
\item We would like to support IMPI which is a TCP protocol with specific
  requirements for job start, connection, and data transfer, without impacting
  performance when IMPI is not used.
\end{enumerate}

\subsection{Communication and Flow Control}
\begin{enumerate}
\item Communication buffers must be managed to avoid out-of-memory errors
  because of buffer limits.
\item Support pipelined collective algorithms (this requires a byte-stream
  data model for implementing the collective algorithms, not MPI
  point-to-point) 
\item Support for topology information (e.g., the MPID\_Depth and
  MPID\_Clusterid in the current experimental WAN version)
\item Fast datatype support
\item There has been some work on different approaches, such as having
  receives with a particular source send a ready-to-receive.
\item Support for error detection and reporting on non-local events.  For
  example, a mismatch in collective communication parameters, or a failed
  ready-send. 
\end{enumerate}

\subsection{Control and Customization}
MPI attributes on communicators (both MPI\_COMM\_WORLD and specific
communicators) provide a clean way to pass information to the ADI without
introducing a new API.  Such attributes could control debugging, packet sizes
(payloads), flow-control thresholds, and the like.

Question: where are the keyvals defined?  Since these might be
device-specific, how are they added to \file{mpi.h} (see signals below)?  

For MPI-3: an \code{info} on communicator creation would be better than an
attribute. 

When an attribute is change, the ADI needs to be notified.  The ADI also needs
to make its own changes (e.g., to use the attribute to report data back to the
user).

\begin{description}
\item[AttrSet()]
\item[AttrGet()]
\item[AttrChanged()]
\end{description}

\subsection{Device-Specific Issues}
How are device-specific parameters set and communicated to the device?  How
are parameters made available to an interrogator?

\subsubsection{TCP}
\begin{enumerate}
\item Lost connections must be restartable.  This will require keeping track
  of message progress in case a connection is dropped during a message
  transfer. 
\end{enumerate}

\subsubsection{Shared Memory}
\begin{enumerate}
\item Support MIMD as well as SPMD
\item Spin wait control
\item Sends must pipeline transfers when appropriate (multibuffer)
\end{enumerate}

\subsubsection{VIA}
\begin{enumerate}
\item Support pinning of memory locations, both on request (e.g., for
  MPI\_Send\_init), and adaptively (i.e., recognize a loop of requests).
\end{enumerate}


\section{Technology Issues}

\subsection{Shared Memory}
\begin{enumerate}
\item Memory synchronization and ordering.  We want to make this as
  light-weight as possible, issuing assembler instructions when possible
\item Lock-free code: there quite a bit of literature on lock-free systems.
  We want to avoid locks as much as possible.  We probably want to use
  abstractions like queue insert and remove for various kinds of queues
  (multi-reader/multi-writer, single-reader/single-writer).  We may need to
  implement these with operations like load-link/store-conditional (note that
  compare-and-swap is what much of the literature uses).  An issue in using
  LL/SC is fairness (these algorithms aren't wait-free, just lock-free).
\end{enumerate}

\section{Design}

This section contains ideas.  As we evaluate them, we will keep the reasons
why design choices were made.

There are two major goals of the ADI that are in tension.  One is to provide
an easy-to-port interface; the other is to provide a general, highly capable
interface that provides very high performance.  The first requirement is most
easily met by defining a simple remote-memory access interface (similar to the
``channel'' interface in ADI-1 and ADI-3).  The second requirement is best met
by providing a full message-passing interface, complete with collective
communications routines.  

To provide for both of these goals, we specify three interfaces:
\begin{enumerate}
\item A remote-memory interface that replaces the channel interface. This is
  called the RMQ interface (remote memory and queuing).
\item A high-level, message-passing style interface, similar to the ADI-2
  interface.  This is the ADI (ADI-3) interface.  The major difference between
  ADI-3 and MPI is that the data structures are exposed (not opaque), and that
  the operations are cast as queue operations (inserts and deletes).
\item Definition of the data structures that transfer data between the MPI
  calls and the lowest level of the device.  This provides a vertical
  integration of the data structures, and is a new feature in ADI-3.  This is
  called the VDSI (vertical data structure interface)
\end{enumerate}
Only the last two interfaces are required.  The RMQ interface also defines a
companion VDSI; the data structures in the VDSI are designed to fit closely
with the RMQ.

This design allows us to eliminate many (all?) of the data structure
translations 
that exist between the levels of the device.  In addition, these help define
the ADI-3 interface.  A diagram of the layers (and cut throughs) is shown in
Figure~\ref{fig:layers}.

\begin{figure}
\centerline{\epsfbox{layers.eps}}
\caption{Layers in ADI-3 design}
\label{fig:layers}
\end{figure}

\subsection{Vertical Data Structure Interface}
There are five basic objects: Requests, Datatypes, Communicators, Windows, and
Files.  An ADI implementation may choose to define all of these datatypes, or
it may choose to use the ones that are defined.  

\subsubsection{Requests}
Requests are queuable objects \note{should everything be queueable?}.  In
addition, the VDSI should define requests so that data can be transfered
directly to/from the request to the communications media.  For example, in a
shared-memory system, the Request should have the message-matching information
stored on a cache-line, and the communication operation should involve just
copying that cacheline.  In a TCP environment, the request should contain all
of the data that is needed for the envelope, in a convenient form.  In a
mixed-method environment, the Request might be defined in the form best for
the fastest (lowest latency) method, and translated as necessary for the other
methods.  

The operations on a Request are
\begin{description}
\item[Request RequestAlloc]
\item[RequestFree( Request * )]
\end{description}

\subsubsection{Communicators}
Communicators are vital for the ADI but are also manipulated by a large range
of MPIR functions.  For this reason, the communicator structure is defined in
large part by the MPIR routines.  An ADI implementation may extend this
definition (adding fields to the structure) if necessary.

The operations on a Communicator are
\begin{description}
\item[Comm CommAlloc]
\item[CommFree( Comm * )]
\end{description}

\subsubsection{Datatypes}
Because datatypes need to be manipulated by the MPIR routines, their
definition, like that of communicators, is more constrained.

Since many (most) uses of datatypes are for the predefined datatypes, we may
want to represent a datatype as 
\begin{verbatim}
typedef struct {
    int index:24;
    int basesize:8;
} Datatype;
\end{verbatim}
Then a non-zero basesize can be used to indicate that the datatype is
contiguous and of the number of bytes specified by that size.  If additional
information is required (e.g., for heterogeneous communication or for
non-contiguous types, the index is used to access an array of more complex
structures containing details about the datatype).

The operations on a Datatype are
\begin{description}
\item[Datatype DatatypeAlloc]
\item[DatatypeFree( Datatype * )]
\item[DatatypeCommit( Datatype )]
\end{description}

\subsubsection{Windows}

The operations on a Window are
\begin{description}
\item[Window WindowAlloc]
\item[WindowFree(Window *)]
\end{description}


\subsubsection{Files}

The operations on a File are
\begin{description}
\item[File FileAlloc()]
\item[FileFree(File *)]
\end{description}


\subsection{Job Startup/Rundown API}

There are two separate steps involved in starting a job (or new processes as
part of \code{MPI\_Comm\_spawn}): the actual job creation and management, and
the steps necessary to connect the newly-created processes to each other (and
possibly to the spawner).  

The first step, job creation and management, may be under the control of an
outside agent, such as a batch queuing system or load-management system.  Such
systems often have only the most rudimentary understanding of parallel
programs.

Some job management systems are very simple and offer very limited
services; others are very sophisticated and offer a wide range of services.
In some cases, we may want to use different code depending on whether services
such as stdin/out/err forwarding are provided.  A query interface is provided
to discover these capabilities.

\begin{description}
\item[{ProcessCreate(const char *argv[], int nproc, ??, ProcessId\_t *id)}]
\item[{ProcessSignal(const ProcessId\_t id[], int nproc, int sig)}]
\item[{ProcessKill(const ProcessId\_t id[], int nproc)}]
\item[ProcessControlQuery(const char *name, char **value)]Queries include
      \begin{description}
      \item[signals]Can signals be delivered?
      \item[stdio]Are stdin/out/err handled?
      \item[env]Is the environment propagated?
      \item[cmdlineargs]Are the command-line arguments propagated?
      \end{description}
\end{description}

The second step, connecting the created processes, requires passing
information between the created processes and the creating processes.  We
provide a simple query interface for this

\begin{description}
\item[QuerySet(const char *name, const char *value)]
\item[QueryGet(const char *name, char **result)]
\end{description}

Implementations of this can range from looking at environment variables to
contacting a distributed data server.

\questions
How are services such as connection signaling, stdin/out/err/command routing
handled?   My current preference is to make minimal requirements on the job
startup/rundown system, and allow all other services to be performed via a
separate user process or processes.  However, the API should allow a capable
job manager (e.g., one capable of delivering signals) to do so.

\subsection{Queue-based ADI}
The building blocks for this approach are queue operations combined with
remote memory operations.  Any method (TCP, VIA, shared-memory) is implemented
as insert data into queue and remote data copies.

A sample block diagram is shown in Figure~\ref{fig:block-diag}.

\begin{figure}
\centerline{\epsfbox{blockdiag.eps}}
\caption{Block diagram for a queue-based ADI}\label{fig:block-diag}
\end{figure}

One question is ``how many queues should there be''?  A better question is,
what should the queue interface look like (leaving the choice of the number of
queues up to the implementor)?

\begin{enumerate}
\item One for everything (most ADI2 implementations)
\item One per communicator (allows single-threaded communicator optimizations
  and a single test for contextid when the item is enqueued).
\item One per remote process (Tony/Boris approach)
\end{enumerate}

If there isn't a single queue for everything, we must keep the contextid in
the queue to allow for cancel.

Another question is whether the ``packets'' should be specified.  In other
words, is there a preferred header format that matches, for example, the queue
contents (a method can always translate between internal and external forms,
but if the external format is defined, it may be easier to define the
interfaces for the queue operations, and to handle things like collective
shortcuts).

\subsection{Request Structure}
From a performance standpoint, the structure of an \texttt{MPI\_Request} is
critical, particularly to avoid coping data between different internal
structures.  For this reason, the basic queue element \emph{is} an
\texttt{MPI\_Request}, and the basic fields of this struct are predefined.

Question: what about devices (methods in the new terminology) that want to
define their own request structure? 
Should we provide an ``extension'' area (of limited size)?  Make requests
pointers, and follow an inheritance approach?  

Packets should be part of the request structure.  That is, there is only one
data structure used in the ADI for messages, and that is a request.  Requests
are queued, and the data sent describing a message consists of a contiguous
subset of a elements in a request.\footnote{Another way to look at this is
  that there are only queue elements, and all queue elements contain a
  request.} 

Care must be exercised if the number of requests is limited.
While you can limit
the number of requests that are issued by, for example, MPI\_Isend, you must
not limit the ones used by MPI\_Bsend (they are supposed to be allocated out
of the user's buffer space) (If you really need to limit the number of
requests in a Bsend, you must not depend on starting an Isend with the buffer;
if the Isend fails to allocate a request, you need to try again later).  The
SGI implementation of MPI fails to execute a correct MPI program that uses
Bsend because SGI has a 16K request limitation.

\subsection{Communicator Structure}

Topology information is part of the communicator structure.
Information on the physical topology is made available with the functions
\begin{description}
\item[TopoDepth]
\item[TopoId(int depth)]
\end{description}

Question: Should all extensions to the communicator be through the attribute
mechanism, and how should the attribute mechanism be represented (it should
\emph{not} be a particular data structure)?

A related issue is whether there should be a standard interface for specifying
network topology.  Should the ``machines'' file contain this?  Should the
machines file be replaced with an API for querying a database about the
environment?

\subsection{Thread Safety Issues}
The biggest problem is the implementation of \texttt{MPI\_Comm\_dup}.  The
usual implementation uses an \texttt{MPI\_Allreduce} to find a free integer
context id.  The problem is that two overlapping communicators could perform an
\texttt{MPI\_Comm\_dup} (still properly ordered); in this case, the two
different dup operations might share the same context id, thus introducing a
bug.  Fixing this is hard.  One approach is to use a two-phase algorithm (like
a load-link/store-conditional):
\begin{enumerate}
\item Read available context ids and set a bit to indicate that a dup is in
  progress\label{dup:step1}
\item Perform an allreduce of the bitvector of available values or a range of
  values, along with the index of any other process that is setting contextid
\item Attempt to set value (if read-bit not set, success, else failure)
\item Allreduce of success flag.  If failure, return to \ref{dup:step1}.
In any case, clear read-bit.  Use index of processes that are setting context
id to provide for tiebreaking logic (to reduce live-lock).
\end{enumerate}
This won't dead lock, but it can live-lock.  

\subsection{Flow Control}
(need to outline envelope and data flow control, including adaptive rules)

\subsection{Progress Engines}
What thread or threads are responsible for advancing the communication of
data?  One simple design uses a separate thread for all communication; this
has the advantage of supporting overlapped communication and compuation.
However, it also can add to latency, particular when there are fewer CPUs than
threads, because of the need to context switch to the communication thread.
An alternative design allows any thread to advance the communication; the cost
here is that the threads must guarnatee exclusive access to the communication
routines (e.g., for write on a socket).  Generalizations of these allow both:
a backstop thread ensures that progress happens, but communication is usually
completed by the thread that requested it (sometimes called thread
impersonation).  

A related issue is the handling of multimethod systems.  Should all but the
fastest method be managed by one or more threads?  For example, on an SMP,
should there be one thread/process managing data exchanges with the outside
world?  One per physical device?  One per virtual device?

\subsection{Interfacing with Other devices}
We will take the position that we define the request, queue, and packet
structure.  However, we provide clear boundaries where the implementation of a
method (in Globus terms) can translate between our data-structures and ones
that the device prefers.  In most cases, these \emph{transfer functions}
should be no-ops.

\section{The ??? device (replaces channel)}

\begin{figure}
\centerline{\epsfbox{mpiddir.eps}}
\caption{A possible organization of the mpid directory}
\end{figure}
\subsubsection{Packet Types}
This enumerates all of the packet types supported in the queue oriented
device.  For each type, there is a rationale for that type.  Some are
optional, provided for performance optimizations.

\begin{description}
\item[ROUTE]? Forward packet?
\item[SHORT]
\item[LONG]
\item[SEND-REQ]
\item[SEND-ACK]
\item[RENDEZVOUS-CONTINUE]
\item[RENDEZVOUS-DATA]
\item[FLOW-ACK]
\item[CANCEL-SEND-REQ]
\item[CANCEL-SEND-REQ-ACK]
\item[BYPASS-ACK]This is used for data transfered ``out of band,'' for example
  by a remote memory operation or a shared-memory store.
\item[INFO]For remote errors (e.g., error in Rsend or Bcast) and other
  information.  May also be used for deadlock/livelock detection.

\item[JOIN](catenate with bit)
\item[CAT](catenate with local value and forward)
\item[COPY](copy value to local location and forward)
\item[REGISTER-GROUP]

\item[RECV-REQ]Support renezvous receive (for long-haul connections)
\item[RECV-ACK]
\item[CANCEL-RECV-REQ]
\item[CANCEL-RECV-REQ-ACK]

\item[RMA operations?]
\end{description}

For collective operations, it would help to have an error flag indicator as
part of the message header.

In heterogeneous systems, the packet structure must specify a data
representation.  We may want to use receiver-makes-right, at least where
possible, because this makes it easy to use native representation in many
cases.  

\section{Evaluation}

Here are some scenarios.

%The general themes are:  
%Translators are used between steps (i.e., between a
%request and a packet) with the identity map as the usual translator.  
A major theme is the use of queue operations that can be made thread safe
either 
through lock-free operations or atomic lock operations provided by the
hardware.  

\subsection{MPI\_Send}
The steps are
\begin{enumerate}
\item Validate arguments if error checking is enabled.
\item Lookup the ``method'' to use, based on the destination.  This requires
  translating the ``local rank of destination'' to an absolute rank of a
  ``connection''. (Should we call this a link?)  (no longer a rank in \code{MPI\_Comm\_world}, because of
  dynamic processes) This is a no-op
  if only one method is supported.
\item Invoke the send operation for the method
\end{enumerate}
Questions: Is the decision about contig/non-contig made at this level?
Is there a separate branch if heterogenity is supported?  

In Isend, a request is allocated and used.  Should the ADI support Send
separately, or only Isend?  Or even just persistent send?

We should try to use writev-like operations where possible.

Do we simply enqueue the send request rather than explicitly invoke a send
routine?  Note that the enqueue routine itself may want to try to dequeue
requests by completing them (think of it as ``add to task manager'' rather
than simply ``enqueue'').

\subsection{MPI\_Recv}
\begin{enumerate}
\item Validate arguments if error checking is enabled
\item Look for matching request in per-communicator message queue.  Extract
  using a thread-safe operation.  Q: should this know about/exploit the level
  of thread safety?
\item If no element in queue, wait for one.  Note that this is a \emph{queue}
  operation, since updates to the queue could happen asynchronously
  (particularly if we allow a separate thread to deliver messages).  Q: should
  the search in queue routine be a thread-safe operation that does not return
  until the item has been found?  This would allow us to use techniques for
  descheuling or sleeping waiting threads/processes.
  This is actually a paired queue operation.  In a thread-safe fashion, if you
  don't find an entry in queue A, add the request to match to queue B.  Both
  of these are per-communicator queues.  Q: use a single queue for both posted
  and unexpected?
\end{enumerate}

Because of the need to quickly locate the communicator containing the queue of
posted messages, the context id should be an array index.

\subsection{MPI\_Put}
We need to send data.  Q: does the window object contain the base addresses of
all of the local windows, or do we just send the displacement?  For VIA-like
interfaces, I believe that we need only the displacement relative to the
(virtual) connection.

Need to do:

search for ``protocol translators'' in the literature

work out byte vs datatype issues

We must also send the datatype.  We need to optimize for the contiquous case.
The Portugal group caches the datatype at the destination (data item for
datatype: where cached).  We will also need a way to invalidate the cache and
to control the amount of storage that is used to cache datatypes.

We clearly need endpoint mapping, both for send\_init and for windows.

Packet types for RMA?

\subsection{MPI\_Accumulate}
Perhaps the hardest case to implement efficiently is the third-party
\code{MPI\_Accumulate} case (using Lock/Unlock synchronization).

\subsection{Multimethod}

The possibilities for handling multimethod communication include:
\begin{enumerate}
\item Each method has a thread that handles communication.  This thread may
  use SIGIO on some systems to cause the thread (signal handler?) to be
  scheduled. 
\item For polling mode, each method should have a relative rate for the poll,
  with yield to OS and yield to other method.  One way to choose these
  parameters is to poll for roundtrip time + epsilon, starting from the
  fastest system.  The roundtrip times could be specified, precomputed (during
  configuration), or computed for each run.
\item A mixture of these, using polling for the very fast devices and a
  service thread for slow ones.
\end{enumerate}

\section{Coding Standards and Modularity}
The ADI-3 design is more modular than the previous ADI designs.  Adding a new
device or method does not require changing any of the files or code outside
of the device.  This section details how the interfaces between
\code{configure}, \code{mpirun}, \code{MPI\_Init}, and \code{mpiinstall} are
specified.  In addition, a set of standard instrumentation operations should
be implemented by each device.

\subsection{Code Modularity}
The original ADI1 and ADI2 designs required that for each device added, 
\file{configure.in} and \file{mpirun.in} be modified.  In addition, special
features require changes to \file{mpirun.args.in}.  

As an interim test, I've made the following changes to the ADI-2
implementation.  ADI-3 should provide something at least as capable:
\begin{itemize}
\item The main \file{configure.in} now looks for the file
  \file{setup\_<device-name>} in the \file{mpid/<device-name>} directory.
  This script is executed (sourced, actually) and processes the arguments
  \texttt{--usage} for   help information and the variable
  \texttt{device\_args} for device-specific options.  It can access and modify
  the variables used by \file{configure.in} (we need to enumerate the list of
  variables and limit what can be changed).  For example, there are now
  \file{mpid/ch\_p4/setup\_ch\_p4} and \file{mpid/globus/setup\_globus} files.
  When there are specific configure tests to be run for the device, these are
  now run with a \file{configure} file in the device's directory.

\item Device-specific \file{mpirun} scripts are now part of the device
  directory.  To allow both separate scripts, and the use of some of the
  common scripts (such as \file{mpirun.pg.in}), there is a file,
  \file{mpirun.lst}, that lists all of the files that must be moved to the
  device-specific \file{bin} directory for that device.  Files with a
  \file{.in} suffix are still processed.

  In addition, the file \file{mpirun.<default-device>.args} (marked as 
  executable) can also be provided.  This file is sourced by
  \file{mpirun.args} to check for valid, device-specific options.  It also
  must accept the command-line argument \texttt{--usage} and provide 
  information about any special options for that particular device.

  This needs to be generalized for \code{mpirun} programs that are not scripts
  and do not use the generic \code{mpirun} system.
\end{itemize}

Related to these device-specific issues are architecture-specific issues in
configuring specific architectures.  There are three ways to approach this:
\begin{enumerate}
\item Write architecture-specific code and tests directly into the main
  configure script.  There are several problems with this.  First, we have to
  maintain scripts for architectures that we have never seen.  Second, it is
  hard for users to modify the architectural-specific tests (configure is 15K
  lines of shell script; even the configure.in is over 3000 lines).  Third, 
  related architectures can't share the same code (this is made even worse by
  the use of the \texttt{ARCH} variable to set the default library/bin
  directory paths).
\item Write an autoconf macro for the architecture and add it to aclocal.m4
  (or to a file included by aclocal.m4).  The advantage to this is that 
  it is somewhat easier to isolate the architecture-specific code; in
  addition, it can use all of the autoconf macros to double-check any
  features, and can be used by other configure files.  
\item Write a script file to be called when a particular architecture is
  chosen.  This is the most modular (users can test and send us scripts to
  run), but makes it difficult to take advantage of autoconf.  
\end{enumerate}
This is one of those cases where there isn't a simple solution.  The autoconf
capabilities are too useful to give up, so there will be \code{AC\_ARCH\_xxx}
for various architecture families; these are like the existing \code{AC\_AIX}
macros for Unix variants.  These are placed in \file{acarchfamily.m4}. 
In addition, configure will look for
\file{util/archfamily/xxxx} for \code{xxxx} either the value of \code{ARCH} or
the value of \code{--with-archfamily=xxxx}.  If it finds that file, it will
source it.  This will allow users to add shell commands for specific
architectures. 

Other miscellaneous issues:
\begin{itemize}
\item Signals.  The MPI standard requires that any signals used be
  documented, whether handlers are established for them or they are used with
  \code{kill}.  Since this information is device specific, yet needs to be in
  the documentation, each device must present this information in a common
  location so that the man pages can be correctly generated.
\end{itemize}

\subsection{Instrumentation}
To include: time waiting, time spinning, time waiting for flow ack, time in
each ADI routine; events for each data packet received; each thread context
switch. 

\subsection{Pointers and Indexes}
How should opaque objects be represented?  There are two obvious choices:
pointers to a structure and an index into an array of structures.  

\begin{center}
\begin{tabular}{p{2.5in}p{2.5in}}
Pointers&Index\\\hline
Direct access (removes the step \&base[idx])&Easy to check for valid values\\
Allows use of entire memory&Bounds memory use\\
Compile-time type checking&C2F and F2C are trivial\\
&Length is the same on all systems (can use 32 or even 16 bit integer)\\
&Small (could use only 16 bits in send-rendezvous rather than the 64 bits
required on systems with 64-bit pointers)\\
Easy to extend (just malloc)&Easy to preallocate\\
\end{tabular}
\end{center}

\bibliography{/home/MPI/allbib,/home/gropp/Update/new/gropp}
\bibliographystyle{plain}

\end{document}
