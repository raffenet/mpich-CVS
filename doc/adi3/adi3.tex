\documentclass{article}
\let\file=\texttt
\let\code=\texttt
%\usepackage{times} % Necessary because acrobat can't handle fonts properly
\usepackage{epsf}

\def\questions{\ifvmode\else\par\fi\paragraph*{Questions:}}
\def\URL#1{\texttt{#1}}
\let\note=\marginpar

%
% Include an entire FILE in verbatim format.  This is cribbed from old TeX
% code; I may have to make some changes to make it work under LaTeX
% We'd like blank lines at the tops of pages to disappear.  To do this,
% the line must be entirely blank (so that the \par is in vmode)
\makeatletter
\def\@eatpar{\futurelet\next\@checkpar}
\def\@checkpar#1{\ifx\next\par\else#1\fi}
\def\uncatcode#1{\catcode`#1=12}
\def\@esp{\ \ \ \ \ \ \ \ }%
{\obeyspaces\global\let =\ }%
{\catcode`\^^I=\active \gdef\obeytabs{\catcode`\^^I=\active \let^^I=\@esp}}%
\def\fileinclude#1{\begingroup\@fileverbatim\input#1\endgroup}
\def\@fileverbatim{%
  \hsize=6in
  \tt
  \parskip=\z@
  \parindent=\z@
    % include blank lines
  %\def\par{\leavevmode\endgraf}%
    % Make blank lines into vskip
  \def\par{\ifvmode\vskip\baselineskip\else\endgraf\fi}
    % defeat \tt ligitures for Spanish (!)
%  \catcode`\`=\active
    % make ^^M into a \par
  \obeylines
    % make | active
%  \catcode`|=\z@
    % remove the active chars
  \let\do=\uncatcode
  \dospecials
  \obeytabs
   % make spaces active
  \obeyspaces\@eatpar}
\makeatother

\begin{document}

\title{A Third Generation Abstract Device Interface for MPICH}
\author{David Ashton, William Gropp, Ewing Lusk, and Debbie Swider}
\maketitle

\pagenumbering{roman}
%\setcounter{page}{3}
\pagestyle{plain}
{\parskip=0pt
\tableofcontents

\bigskip
}
\bigskip

\pagestyle{plain}
\clearpage

\pagenumbering{arabic}
\setcounter{page}{1}


\section{Introduction}
The MPICH implementation of MPI has used an abstract device interface (ADI) to
provide an easy way to port MPI to new platforms.  The first version of the
ADI was based on a message-passing model, and was designed to allow MPICH to
make use of underlying message-passing systems such as Intel's NX or IBM's
MPL.  The second version of the ADI was a relatively minor change intended to
allow for more experimentation and to allow the device more control over the
formats of datatypes and communicators.  The Globus device \cite{mpich-globus}
made use of these features.

With the success of MPI, most other message-passing systems (for scientific
computing) have disappeared.  Many massively parallel computer vendors have
also disappeared, and the remaining ones all have their own implementations of
MPI.  At the same time, high-performance interconnects that provide some form
of OS-bypass have emerged (Myrinet), along with proposed commodity
standards (VIA, Infinaband).  There is increasing interest in multithreaded
MPI applications (in fact, several vendor MPI's already support multithreaded
applications) and in large clusters of SMPs.  Support for MPI-2, including
process creation, one-sided communication, and high-performance I/O (though
partially addressed with ADIO \cite{ADIO}) is also needed.
In order to support research
into MPI implementations, MPICH needs a new ADI that addresses the new
features of MPI-2, high-performance user-mode interconnects, multithreaded
environments, and experiments in performance enhancements for both
point-to-point and collective operations.  

\subsection{Related Work}

This section will cover other MPI implementation work, including:

Tony and Boris argue for a single protocol that moves small blocks fast is
better than a multi-level protocol \cite{techreport}.

Several groups have looked at receiver rendezvous; this may be particularly
helpful for wide-area networks.

NEC (as well as us) have looked at datatype optimizations.

The LAPI-based implementation uses remote-memory-access.

Also relevant are other low-level communication approaches, including active
messages \cite{isca92*256} and Nexus \cite{JPDCNexus,Nexus}.

\section{Requirements}
The focus of the first two ADI designs was on the point-to-point message
passing.  
The third version of the abstract device interface is intended to provide
support for more of MPI.  
This section lists requirements that we address in the ADI-3 design.

\subsection{General Capabilities}
\begin{enumerate}
\item Thread-safe
\item Full MPI-2 (and MPI-1, including cancel for sends)
\item Support multimethod, including IMPI
\item Uniform configuration support (e.g., environment variables for tuning)
\item Complete and deep instrumentation (e.g., ADI instrumented)
\item SMP friendly (e.g., no busy waits)
\item Support external scheduling systems
\item Support overlap of communication and computation (using threads or
  asynchronous operations)
\item Support fault-tolerance on a group basis (see xxx)
\end{enumerate}

\subsection{Performance Issues}
\begin{enumerate}
\item Collective implementations should allow pipelining optimizations (store
  and forward).  This optimizes the long-data case
\item Collective short-cuts for join/copy/concatenate; this optimizes the
  short-data case.
\item Collective operations can exploit support for low-level multi-party
  operations such as IP (unreliable) multicast
\item Access to network topology
\item Make use of system network topology
\item Scalable and low latency (in shared-memory case, this means no locks)
\item Direct (and short) path for common operations
\item Asymptotically fast bandwidth
\item Fast and efficient non-contiguous datatypes
\item Fast one-sided support
\item No performance penalty for thread-safety in single-threaded case
\item Are there opportunities for I/O communication?
\end{enumerate}

\subsection{Job Startup and Rundown and Abort}
\begin{enumerate}
\item Abort must be rock solid.
\item Process creation through an API that allows use of third-party job
  managers and schedulers.
\item Scalable delivery of executable to hosts when (a) executable is not on a
  shared system and/or (b) it is preferable to run from a local disk.  Should
  there be version   control when running the same, unchanged, executable?
\item What info values should be standardized?  For queuing systems, we could
  consider host, user, queueclass.  For interconnection, proxy would be
  useful.  (Some of these have been used by STAMPI \cite{stampi}).
\item All resources must be released at the end of a job, even if it is killed
  with an uncatchable signal.  This is so difficult as to not even be strictly
  possible (deficiencies in the OS design), but we can reduce the windows of
  vunerability by logging all permanent resources (e.g., process ids, SYSV
  memory segments and semaphores, files, IP ports) into a file or database.
\end{enumerate}

\subsection{Interoperability (IMPI and Java)}
\begin{enumerate}
\item We would like to support IMPI which is a TCP protocol with specific
  requirements for job start, connection, and data transfer, without impacting
  performance when IMPI is not used.
\end{enumerate}

\subsection{Communication and Flow Control}
\begin{enumerate}
\item Communication buffers must be managed to avoid out-of-memory errors
  because of buffer limits.
\item Support pipelined collective algorithms (this requires a byte-stream
  data model for implementing the collective algorithms, not MPI
  point-to-point) 
\item Support for topology information (e.g., the MPID\_Depth and
  MPID\_Clusterid in the current experimental WAN version)
\item Fast datatype support
\item There has been some work on different approaches, such as having
  receives with a particular source send a ready-to-receive.
\item Support for error detection and reporting on non-local events.  For
  example, a mismatch in collective communication parameters, or a failed
  ready-send. 
\end{enumerate}

\subsection{Device-Specific Issues}
How are device-specific parameters set and communicated to the device?  How
are parameters made available to an interrogator?

\subsubsection{TCP}
\begin{enumerate}
\item Lost connections must be restartable.  This will require keeping track
  of message progress in case a connection is dropped during a message
  transfer. 
\end{enumerate}

\subsubsection{Shared Memory}
\begin{enumerate}
\item Support MIMD as well as SPMD
\item Spin wait control
\item Sends must pipeline transfers when appropriate (multibuffer)
\item Memory synchronization and ordering.  We want to make this as
  light-weight as possible, issuing assembler instructions when possible
\item Lock-free code: there quite a bit of literature on lock-free systems.
  We want to avoid locks as much as possible.  We probably want to use
  abstractions like queue insert and remove for various kinds of queues
  (multi-reader/multi-writer, single-reader/single-writer).  We may need to
  implement these with operations like load-link/store-conditional (note that
  compare-and-swap is what much of the literature uses).  An issue in using
  LL/SC is fairness (these algorithms aren't wait-free, just lock-free).
\end{enumerate}

\subsubsection{VIA}
\begin{enumerate}
\item Support pinning of memory locations, both on request (e.g., for
  MPI\_Send\_init), and adaptively (i.e., recognize a loop of requests).
\end{enumerate}


\section{Design}
\label{sec:design}

%This section contains ideas.  As we evaluate them, we will keep the reasons
%why design choices were made.

There are two major goals of the ADI that are in tension.  One is to provide
an easy-to-port interface; the other is to provide a general, highly capable
interface that provides very high performance.  The first requirement is most
easily met by defining a simple remote-memory access interface (similar to the
``channel'' interface in ADI-1 and ADI-3).  The second requirement is best met
by providing a full message-passing interface, complete with collective
communications routines.  

To provide for both of these goals, we specify three interfaces:
\begin{enumerate}
\item A remote-memory interface that replaces the channel interface. This is
  called the RMQ interface (remote memory and queuing).
\item A high-level, message-passing style interface, similar to the ADI-2
  interface.  This is the ADI (ADI-3) interface.  The major difference between
  ADI-3 and MPI is that the data structures are exposed (not opaque), and that
  the operations are cast as queue operations (inserts and deletes).
\item Definition of the data structures that transfer data between the MPI
  calls and the lowest level of the device.  This provides a vertical
  integration of the data structures, and is a new feature in ADI-3.  This is
  called the VDSI (vertical data structure interface)
\end{enumerate}
Only the last two interfaces are required.  The RMQ interface also defines a
companion VDSI; the data structures in the VDSI are designed to fit closely
with the RMQ.

This design allows us to eliminate many (all?) of the data structure
translations 
that exist between the levels of the device.  In addition, these help define
the ADI-3 interface.  A diagram of the layers (and cut throughs) is shown in
Figure~\ref{fig:layers}.

\begin{figure}
\centerline{\epsfbox{layers.eps}}
\caption{Layers in ADI-3 design}
\label{fig:layers}
\end{figure}

The routines at the ADI-3 level look much like the MPI communications
routines, with the change that the arguments to the ADI-3 routines are usually
a (pointer to a) single object, such as a request.  The MPI routines are
responsible for basic error checking (e.g., rank in range) and for filling in
the fields of the data structure, such as a request, that is used to pass data
into the ADI.  

\subsection{Vertical Data Structure Interface}
There are five basic objects: Requests, Datatypes, Communicators, Windows, and
Files.  An ADI implementation may choose to define all of these datatypes, or
it may choose to use the ones that are defined.  

\subsubsection{Requests}
Requests are queuable objects \note{should everything be queueable?}.  In
addition, the VDSI should define requests so that data can be transfered
directly to/from the request to the communications media.  For example, in a
shared-memory system, the Request should have the message-matching information
stored on a cache-line, and the communication operation should involve just
copying that cacheline.  In a TCP environment, the request should contain all
of the data that is needed for the envelope, in a convenient form.  In a
mixed-method environment, the Request might be defined in the form best for
the fastest (lowest latency) method, and translated as necessary for the other
methods.  

The operations on a Request are
\begin{description}
\item[Request RequestAlloc()]
\item[RequestFree( Request * )]
\item[RequestQueue]( Queue, Request )
\item[RequestDequeue](Queue, Request, int tag, int dest, int contextid )
\end{description}
? Do we need an AllocAndEnqueue?

\subsubsection{Communicators}
Communicators are vital for the ADI but are also manipulated by a large range
of MPIR functions.  For this reason, the communicator structure is defined in
large part by the MPIR routines.  An ADI implementation may extend this
definition (adding fields to the structure) if necessary.

The operations on a Communicator are
\begin{description}
\item[Comm CommAlloc()]
\item[CommFree( Comm * )]
\end{description}

Question:  Communication will be on \emph{links}; finding a link from a
relative rank 
in a communicator must be fast.  How opaque are links as objects?  For
example, the RMQ device will want to attach methods to links, but methods are
not an ADI-3 concept.

\subsubsection{Datatypes}
Because datatypes need to be manipulated by the MPIR routines, their
definition, like that of communicators, is more constrained.

Since many (most) uses of datatypes are for the predefined datatypes, we may
want to represent a datatype as 
\begin{verbatim}
typedef struct {
    int index:24;
    int basesize:8;
} Datatype;
\end{verbatim}
Then a non-zero basesize can be used to indicate that the datatype is
contiguous and of the number of bytes specified by that size.  If additional
information is required (e.g., for heterogeneous communication or for
non-contiguous types, the index is used to access an array of more complex
structures containing details about the datatype).

The operations on a Datatype are
\begin{description}
\item[Datatype DatatypeAlloc()]
\item[DatatypeFree( Datatype * )]
\item[DatatypeCommit( Datatype )]
\end{description}

Note that the one-sided operations may need to send a datatype to a remote
process.  To make this more efficient, we will cache datatypes at the
destination.  We will record this fact within the datatype.

\subsubsection{Windows}

The operations on a Window are
\begin{description}
\item[Window WindowAlloc()]
\item[WindowFree(Window *)]
\end{description}


\subsubsection{Files}

The operations on a File are
\begin{description}
\item[File FileAlloc()]
\item[FileFree(File *)]
\end{description}

\subsubsection{Info}
The \code{MPI\_Info} object is used by several classes of MPI-2 routines to
provide for customization.  Thus, the \code{Info} object must be understood by
the device, and it makes sense to put it on the same footing as the other MPI
opaque objects.  Most if not all devices will choose to use the generic
\code{Info} support included with ADI-3.

The operations on an Info are
\begin{description}
\item[Info InfoAlloc()]
\item[InfoFree(Info *)]
\item[InfoSet( const char *key, const char *value )]
\item[InfoGet( const char *key, char **value )]
\end{description}

\subsection{Control and Customization}
MPI attributes on communicators (both MPI\_COMM\_WORLD and specific
communicators) provide a clean way to pass information to the ADI without
introducing a new API.  Such attributes could control debugging, packet sizes
(payloads), flow-control thresholds, and the like.

Question: where are the keyvals defined?  Since these might be
device-specific, how are they added to \file{mpi.h} (see signals below)?  

For MPI-3: an \code{info} on communicator creation would be better than an
attribute. 

When an attribute is change, the ADI needs to be notified.  The ADI also needs
to make its own changes (e.g., to use the attribute to report data back to the
user).

\begin{description}
\item[AttrSet()]
\item[AttrGet()]
\item[AttrChanged()]
\end{description}

\subsection{Job Startup/Rundown API}

There are two separate steps involved in starting a job (or new processes as
part of \code{MPI\_Comm\_spawn}): the actual job creation and management, and
the steps necessary to connect the newly-created processes to each other (and
possibly to the spawner).  

The first step, job creation and management, may be under the control of an
outside agent, such as a batch queuing system or load-management system.  Such
systems often have only the most rudimentary understanding of parallel
programs.

Some job management systems are very simple and offer very limited
services; others are very sophisticated and offer a wide range of services.
In some cases, we may want to use different code depending on whether services
such as stdin/out/err forwarding are provided.  A query interface is provided
to discover these capabilities.

One effort to define a job management interface is PSCHED.
One part of the PSCHED interface, described in \cite{psched01}, addresses the
needs of third-party scheduling 
policy services, and does not include any services for submitting jobs into a
job management system.  
The other part of the PSCHED interface, described in \cite{psched01other},
provides task management routines that can be used to create and manage
individual processes.  An extension supports the creation of multiple
processes with a single call (\code{tm\_spawnTaskPool}).  

\begin{description}
\item[ProcessCreate](const char *argv[], const char *env[], 
                     int nproc, Info info,
    ProcessGroupId\_t *id)\\
\code{argv} includes the command name as \code{argv[0]}.  The returned value
    is a \code{ProcessGroupId\_t} which contains everything about the created
    processes. 
    \code{env} is of the form \code{name=value} for each environment variable.
    Both \code{argv} and \code{env} are terminated by a null string.
\item[ProcessSignal](const ProcessGroupId\_t id[], int sig)\\
Should this be ProcessSendEvent instead?  Events can include Stop (e.g., like
SIGSTOP), Exit (SIGINT), or a small message (e.g., containing port to listen
on for a connection)
\item[ProcessKill](const ProcessGroupId\_t id[], int nproc)\\
Should this have a request exit and force exit option?
\item[ProcessControlQuery](const char *name, char **value)\\
      Queries include
      \begin{description}
      \item[signals]Can signals be delivered?
      \item[stdio]Are stdin/out/err handled?
      \item[env]Is the environment propagated?
      \item[cmdlineargs]Are the command-line arguments propagated?
      \end{description}
\item[ProcessFree](ProcessGroupId\_t *id)\\
\item[ProcessGroupCreate](const ProcessGroupId\_t in, const int ranks[], int
  nranks, ProcessGroupId\_t *out )\\
  Create a process group from an existing group by selecting particular
  members.
\item[ProcessGetMyGroupId]()\\
Returns the process group id of the calling process.  This is needed by the
processes that have been created.
\item[ProcessGetMyRank]()\\
Returns the process rank in its created group.
\end{description}
Question: signal is Unix-specific.  Do we want that?  What about enviroment
variables? 

Question: What info keys do we predefine?

Question: Should \code{ProcessCreate} be non-blocking?  

Question: What happens to stdio when performing multiple spawns?

A sample \code{ProcessGroupId\_t} might look something like
\begin{verbatim}
    typedef struct {
        HostName name;   /* Not just an IP address */
        int      pid;
        int      rcode;  /* Return code if it has exited */
    } ProcessInfo;
    typedef struct {
        int n_processes;
        ProcessInfo *pinfo;
    } *ProcessGroupId_t;
\end{verbatim}

The second step, connecting the created processes, requires passing
information between the created processes and the creating processes.  We
provide a simple query interface for this:
\begin{description}
\item[QuerySet](ProcessGroupId\_t id, const char *name, const char *value)\\
\item[QueryGet](ProcessGroupId\_t id, const char *name, const char **result)\\
\item[QueryFree](ProcessGroupId\_t id, const char **result)\\
\end{description}
These are all scoped to a particular process group id.

Implementations of this can range from looking at environment variables to
contacting a distributed data server.

Note that in a single-threaded environment, \code{QueryGet} can simply return
a pointer to the internal storage; \code{QueryFree} is then a no-op.  In a
multi-threaded environment, it may be necessary to allocate and free
thread-specific storage.

\questions
How are services such as connection signaling, stdin/out/err/command routing
handled?   My current preference is to make minimal requirements on the job
startup/rundown system, and allow all other services to be performed via a
separate user process or processes.  However, the API should allow a capable
job manager (e.g., one capable of delivering signals) to do so.

Should \code{mpiexec} be built using these operations? (Yes?)

\subsection{Queue-based ADI}
The building blocks for this approach are queue operations combined with
remote memory operations.  Any method (TCP, VIA, shared-memory) is implemented
as insert data into queue and remote data copies.

A sample block diagram is shown in Figure~\ref{fig:block-diag}.

\begin{figure}
\centerline{\epsfbox{blockdiag.eps}}
\caption{Block diagram for a queue-based ADI}\label{fig:block-diag}
\end{figure}

One question is ``how many queues should there be''?  A better question is,
what should the queue interface look like (leaving the choice of the number of
queues up to the implementor)?

\begin{enumerate}
\item One for everything (most ADI2 implementations)
\item One per communicator (allows single-threaded communicator optimizations
  and a single test for contextid when the item is enqueued).
\item One per remote process (Tony/Boris approach)
\end{enumerate}

If there isn't a single queue for everything, we must keep the contextid in
the queue to allow for cancel.

Another question is whether the ``packets'' should be specified.  In other
words, is there a preferred header format that matches, for example, the queue
contents (a method can always translate between internal and external forms,
but if the external format is defined, it may be easier to define the
interfaces for the queue operations, and to handle things like collective
shortcuts).

\subsection{Request Structure}
From a performance standpoint, the structure of an \texttt{MPI\_Request} is
critical, particularly to avoid coping data between different internal
structures.  For this reason, the basic queue element \emph{is} an
\texttt{MPI\_Request}, and the basic fields of this struct are predefined.

Question: what about devices (methods in the new terminology) that want to
define their own request structure? 
Should we provide an ``extension'' area (of limited size)?  Make requests
pointers, and follow an inheritance approach?  

Packets should be part of the request structure.  That is, there is only one
data structure used in the ADI for messages, and that is a request.  Requests
are queued, and the data sent describing a message consists of a contiguous
subset of a elements in a request.\footnote{Another way to look at this is
  that there are only queue elements, and all queue elements contain a
  request.} 

Care must be exercised if the number of requests is limited.
While you can limit
the number of requests that are issued by, for example, MPI\_Isend, you must
not limit the ones used by MPI\_Bsend (they are supposed to be allocated out
of the user's buffer space) (If you really need to limit the number of
requests in a Bsend, you must not depend on starting an Isend with the buffer;
if the Isend fails to allocate a request, you need to try again later).  The
SGI implementation of MPI fails to execute a correct MPI program that uses
Bsend because SGI has a 16K request limitation.

\subsection{Communicator Structure}

Topology information is part of the communicator structure.
Information on the physical topology is made available with the functions
\begin{description}
\item[int TopoDepth()]
\item[int TopoId(int depth)]
\end{description}

Question: Should all extensions to the communicator be through the attribute
mechanism, and how should the attribute mechanism be represented (it should
\emph{not} be a particular data structure)?

A related issue is whether there should be a standard interface for specifying
network topology.  Should the ``machines'' file contain this?  Should the
machines file be replaced with an API for querying a database about the
environment?

\subsection{Thread Safety Issues}
The biggest problem is the implementation of \texttt{MPI\_Comm\_dup}.  The
usual implementation uses an \texttt{MPI\_Allreduce} to find a free integer
context id.  The problem is that two overlapping communicators could perform an
\texttt{MPI\_Comm\_dup} (still properly ordered); in this case, the two
different dup operations might share the same context id, thus introducing a
bug.  Fixing this is hard.  One approach is to use a two-phase algorithm (like
a load-link/store-conditional):
\begin{enumerate}
\item Read available context ids and set a bit to indicate that a dup is in
  progress\label{dup:step1}
\item Perform an allreduce of the bitvector of available values or a range of
  values, along with the index of any other process that is setting contextid
\item Attempt to set value (if read-bit not set, success, else failure)
\item Allreduce of success flag.  If failure, return to \ref{dup:step1}.
In any case, clear read-bit.  Use index of processes that are setting context
id to provide for tiebreaking logic (to reduce live-lock).
\end{enumerate}
This won't dead lock, but it can live-lock.  

\subsection{Flow Control}
(need to outline envelope and data flow control, including adaptive rules)

\subsection{Progress Engines}
What thread or threads are responsible for advancing the communication of
data?  One simple design uses a separate thread for all communication; this
has the advantage of supporting overlapped communication and compuation.
However, it also can add to latency, particular when there are fewer CPUs than
threads, because of the need to context switch to the communication thread.
An alternative design allows any thread to advance the communication; the cost
here is that the threads must guarnatee exclusive access to the communication
routines (e.g., for write on a socket).  Generalizations of these allow both:
a backstop thread ensures that progress happens, but communication is usually
completed by the thread that requested it (sometimes called thread
impersonation).  

A related issue is the handling of multimethod systems.  Should all but the
fastest method be managed by one or more threads?  For example, on an SMP,
should there be one thread/process managing data exchanges with the outside
world?  One per physical device?  One per virtual device?

\subsection{Interfacing with Other devices}
We will take the position that we define the request, queue, and packet
structure.  However, we provide clear boundaries where the implementation of a
method (in Globus terms) can translate between our data-structures and ones
that the device prefers.  In most cases, these \emph{transfer functions}
should be no-ops.

\section{The RMQ device}
The Remote Memory Queue (RMQ) device replaces the channel device as the simple,
low-level device that is easy to implement.   The RMQ device provides a simple
device that can support multiple \emph{methods} or ways to move data between
processes.  
The implementation of the RMQ device
assumes only that the device can send data between processes.  To provide for
efficient implementations, there are two types of data communications: control
and streams.  Control information is generally a small amount of data, and is
sent as a single operation.  Stream data may be arbitrarily long, and may be
both sent and received in parts.  

Adding a method to the RMQ device requires providing only a few routines.  The
common RMQ device code calls these routines as required in order to implement
the ADI-3 interface.  

\subsection{A Quick Overview}
The RMQ device is, roughly, an event-driven device.  Each method provides a
routine, \code{POLL\_METHOD}, that checks for incoming data and, for each
control message, calls \code{RMQ\_Dispatch\_packet}.  In addition, the routine
\code{POLL\_METHOD\_COST} provides an estimate of the cost of calling
\code{POLL\_METHOD}, in microseconds.  

Outgoing data is sent with one of two routines.  The first, \code{PUT\_PACKET},
sends a small amount of data.  
This routine is blocking in the MPI sense; when
it returns, the buffer given as input may be re-used. Question: do we want
this, or should this be \code{ENQUEUE\_PACKET}?

The second, \code{PUT\_STREAM}, begins sending stream data.

All data is in contiguous locations.  The handling messages described by MPI
datatypes is discussed in Section~\ref{sec:rmq-datatypes}.

The operations that a method must define for the RMQ device include:
\begin{description}
\item[INIT-METHOD]
\item[INIT-LINK]? Returns buffering on link?
\item[CLOSE-LINK]
\item[PUT-PACKET]
\item[PUT-STREAM]
\item[POLL-METHOD]
\item[POLL-METHOD-COST]
\end{description}

In addition, the following may be defined; if undefined, the RMQ device will
use generic versions
\begin{description}
\item[INIT-STREAM-IN]Initialize the source of a stream
\item[INIT-STREAM-OUT]Initialize the destination of a stream
\item[FREE-STREAM]Free a stream
\item[GET-STREAM]Get a stream ID
\end{description}


The \code{POLL-METHOD} routine is called by all processes either whenever there
is I/O activity (e.g., in response to a \code{SIGIO}), during MPI calls (ADI-3
hook for poke-progress), or in response to a timer (\code{SIGALRM}).  In a
system with true one-sided operations, the \code{POLL-METHOD} may be undefined
or 
null.  \code{POLL-METHOD-COST} provides a hint of the cost of
\code{POLL-METHOD} and may be used to adjust the frequency at which
\code{POLL-METHOD} should be called relative to other methods (if any).

Question: Should we allow a separate \code{POLL-METHOD-LINK} for each link as
well as a combined operation?

All other ADI-3 operations are provided by generic implementations built from
these simple operations.  Each class of ADI-3 operations, such as datatype
handling or collective communication, is provided by code in a separate
directory, as shown in Figure~\ref{fig:mpid-dirs}.

\begin{figure}
\centerline{\epsfbox{mpiddir.eps}}
\caption{A possible organization of the mpid directory}
\label{fig:mpid-dirs}
\end{figure}

Questions:
Where is flow control?  Is it in RMQ or the individual method?  If in RMQ, how
are the parameters set (e.g., if the socket buffer size is 64k, we can take
advantage of that)?

\subsection{Example Implementation of the RMQ for TCP}
The following shows how the RMQ could be implemented for communication by
sockets.  The code shown is pseudocode and does not include error handling or
all required arguments.

\fileinclude{rmq-tcp.txt}

Question: Thread locks on I/O operations.  Are they needed per fd or per OS
call (i.e., can separate threads write to seperate fds at the same time)?

Question: Add a shared-memory method and show how the RMQ uses the two
POLL-RMQ operations.

Question: Still don't know how to handle data put in the POLL-RMQ routine.
We could define end-point handlers (which could be null for direct copy or
could handle simultaneous copy for shared memory).  In that case, we'd have 
a RECV-DATA( link, void *p )?

\subsubsection{Notes on the TCP implementation}
If this is part of a multimethod implementation, the TCP method may be
required to convert requests from the canonical form to the form required by
TCP.  For example, the TCP data may need to be send in network byte order, or
it may require some additional data (such as TCP-specific flow control).

\subsection{Example Implementation of the RMQ for Shared Memory}

\fileinclude{rmq-shmem.txt}

\subsection{Example Implementation of the RMQ for VIA}

\fileinclude{rmq-via.txt}

\subsection{Implementation of the ADI by RMQ}
In the RMQ implementation, the ADI operations are provided by selecting the
appropriate operation from a small set.  This set provides for such ADI
required services as flow control, cancel, and collective communication.

\subsection{Progress Engine Implementations}
The definition of \code{POLL\_METHOD} was defined to allow different
implementations of the RMQ ``progress engine.''  The progress engine is the
code that reads packets and data and ensures that progress towards completion
of communication occurs.  Many different implementations are possible; we
discuss a few here.

\subsubsection{Thread Agents}
One approach that guarantees nearly continuous progress and good
responsiveness uses one thread for each method, plus a thread for the RMQ
device.  For each method, there is a thread running
\begin{verbatim}
    while (not done) {
        POLL_METHOD( blocking = true );
    }
\end{verbatim}
The routine \code{RMQ\_Dispatch\_packet} simply adds a packet to a shared queue
of incoming packets (in a thread-safe way, using lock-free queue update
operations).  Another thread runs 
\begin{verbatim}
    while (not done) {
        RMQ_Dequeue_packet( &packet );
        RMQ_Process_packet( &packet );
    }
\end{verbatim}
The \code{RMQ\_Dispatch\_packet} may cause this RMQ service thread to be
scheduled if the thread system supports that operation.

Another implementation is single-threaded.  In this case, the RMQ code
calls \code{RMQ\_Check} frequently; this routine looks roughly like
\begin{verbatim}
    do { 
        int found_incoming=0;
        for each method {
            if (method_test_count ++ > method_cost) {
                found_incoming += POLL_METHOD( blocking = false );
            }
        }
    } while (blocking && found_incoming > 0);
\end{verbatim}
The value of \code{method\_cost} is computed from \code{POLL\_METHOD\_COST}.
In addition, the routine \code{RMQ\_Dispatch\_packet} in this case simply calls
\code{RMQ\_Process\_packet}.  More sophisticated versions are possible
\cite{isca92*256}. 

It is also possible to switch between different progress engine approaches or
even to combine them.  For example, it might be more efficient to have the
same thread that is sending or receiving data process the operation by calling
\code{POLL\_METHOD} directly; the seperate service threads would handle
messages that arrived when the other threads were in long computation loops.
Of course, there would be some overhead, as all operations would need to be
thread-safe and reentrant.

\subsection{Handling Datatypes}
\label{sec:rmq-datatypes}
MPI provides a conveninent way to describe non-contiguous data items, as well
as including information on the basic datatypes that allows for data to be
correctly transfered between systems with different data representations.  The
RMQ implementation supports this, but through a simplified interface.

Each communication method transfers only contiguous data.  The RMQ itself will
arrange for non-contiguous data to be transferred to and from contiguous
locations.  In addition, when heterogeneous data representations are
supported, the RMQ will arrange for the conversion of data.  

The major design issue here is: how can copies be avoided in important special
cases, such as homogenous systems transferring contiguous data, while
maintaining a simple interface?

There are several approaches:
\begin{enumerate}
\item Define Put routines that are called as a datatype is processed to put
  data into the method, and define corresponding get routines that are called
  on the receive end.  This is similar to the style used in PVM
  \cite{pvm-book} and Nexus \cite{nexus-manual}.  A drawback of this approach
  is that it is very hard to avoid performing an extra copy if the routines
  are used in the contiguous, homogenous case.  An advantage is that they
  cleanly separate the access to the user data and the communication buffers.

\item Define Put and Get routines, but have them work with buffer pointers.
  For example, Put can return a buffer pointer; add
  the requirement that the buffer remain unmodified until released.
  This allows the routines to simply return a pointer to the input buffer in
  the contiguous case, and to choose one of several strategies if the data
  must be copied or converted.  These may also require a GetCommBuffer routine
  that returns an address that can be used as the buffer returned; this allows
  the communication method to provide the buffer if the user's buffer cannot
  be used.  Get is still a problem, because in the homogeneous contiguous
  case,  the message must be received directly into the destination buffer.

\item Have separate code for the two cases (this was the ADI-2 choice).  The
  problem with this is that it complicates the code; in particular, there is a
  lot of \code{\#ifdef HAS\_HETERO} stuff in the routines.
\end{enumerate}

Taking a stream view of data transfer, the operations for send are really the
following:
\begin{enumerate}
\item Prepare to send (identify data characteristics and size).
\item Provide some number of bytes, properly formatted, to the communication
  system.
\item Continue until all data has been sent.
\end{enumerate}
On the receive side, the operations are
\begin{enumerate}
\item Prepare to receive (identify data characteristics and size of buffer;
  may be different in location and size from final destination).
\item Receive some number of bytes and transfer to the final destination.
  This may be ``receive some number of bytes directly into the final
  destination.'' 
\item Continue until all data has been received.
\end{enumerate}

\subsection{The RMQ implementation}
The bulk of the RMQ implementation is in the routine
\code{RMQ\_Process\_packet}.  This routine takes a packet and, based on its
type and content, performs various actions.  This section describes the packet
types and sketches the operation of \code{RMQ\_Process\_packet}.

\subsubsection{Packet Types}
This enumerates all of the packet types supported in the queue oriented
device.  For each type, there is a rationale for that type.  Some are
optional, provided for performance optimizations.

Question:  Should some packet types be subtypes?  E.g., should there be a
single RMA type with subtypes for the RMA operations (allowing different RMA
implementations with a simple change to the routine called by 
\code{RMQ\_Process\_packet})? Similarly for the collective packet types?

Question: Should RENDEZVOUS-xxx be STREAM-xxx instead?

\begin{description}
\item[ROUTE]This contains a packet of arbitrary type, and simply indicates
  that the packet should be sent (forwarded) on the route specified.
\item[SHORT]
\item[LONG]
\item[SEND-REQ]
\item[SEND-ACK]
\item[RENDEZVOUS-CONTINUE]
\item[RENDEZVOUS-DATA]
\item[FLOW-ACK]
\item[CANCEL-SEND-REQ]
\item[CANCEL-SEND-REQ-ACK]
\item[BYPASS-ACK]This is used for data transfered ``out of band,'' for example
  by a remote memory operation or a shared-memory store.
\item[INFO]For remote errors (e.g., error in Rsend or Bcast) and other
  information.  May also be used for deadlock/livelock detection.

\item[JOIN](catenate with bit)
\item[CAT](catenate with local value and forward)
\item[COPY](copy value to local location and forward)
\item[REGISTER-GROUP]

\item[RECV-REQ]Support rendezvous receive (for long-haul connections)
\item[RECV-ACK]
\item[CANCEL-RECV-REQ]
\item[CANCEL-RECV-REQ-ACK]

\item[RMA-PUT]
\item[RMA-GET]
\item[RMA-ACCUMULATE]
\item[RMA-DATATYPE]
\item[RMA-DATATYPE-ACK]
\end{description}

For collective operations, it would help to have an error flag indicator as
part of the message header.

In heterogeneous systems, the packet structure must specify a data
representation.  We may want to use receiver-makes-right, at least where
possible, because this makes it easy to use native representation in many
cases.  

The RMA operations need to cache datatypes; because cached datatypes will use
space, they need to be recovered, either because the process that created that
datatype no longer needs it, or because the process with the cached datatype
has decided to remove that cached element.  In these cases, we need to
implement a cache-coherence policy; the RMA packet types must include these
cache coherence operations.

\section{Evaluation}

Here are some scenarios.

%The general themes are:  
%Translators are used between steps (i.e., between a
%request and a packet) with the identity map as the usual translator.  
A major theme is the use of queue operations that can be made thread safe
either 
through lock-free operations or atomic lock operations provided by the
hardware.  

\subsection{MPI\_Init}

\subsection{MPI\_Send}
The steps are
\begin{enumerate}
\item Validate arguments if error checking is enabled.
\item Lookup the ``method'' to use, based on the destination.  This requires
  translating the ``local rank of destination'' to an absolute rank of a
  ``connection''. (Should we call this a link?)  (no longer a rank in
  \code{MPI\_Comm\_world}, because of 
  dynamic processes) This is a no-op
  if only one method is supported.
\item Invoke the send operation for the method
\end{enumerate}
Questions: Is the decision about contig/non-contig made at this level?
Is there a separate branch if heterogenity is supported?  

In Isend, a request is allocated and used.  Should the ADI support Send
separately, or only Isend?  Or even just persistent send?

We should try to use writev-like operations where possible.

Do we simply enqueue the send request rather than explicitly invoke a send
routine?  Note that the enqueue routine itself may want to try to dequeue
requests by completing them (think of it as ``add to task manager'' rather
than simply ``enqueue'').

\subsection{MPI\_Recv}
\begin{enumerate}
\item Validate arguments if error checking is enabled
\item Look for matching request in per-communicator message queue.  Extract
  using a thread-safe operation.  Q: should this know about/exploit the level
  of thread safety?
\item If no element in queue, wait for one.  Note that this is a \emph{queue}
  operation, since updates to the queue could happen asynchronously
  (particularly if we allow a separate thread to deliver messages).  Q: should
  the search in queue routine be a thread-safe operation that does not return
  until the item has been found?  This would allow us to use techniques for
  descheuling or sleeping waiting threads/processes.
  This is actually a paired queue operation.  In a thread-safe fashion, if you
  don't find an entry in queue A, add the request to match to queue B.  Both
  of these are per-communicator queues.  Q: use a single queue for both posted
  and unexpected?
\end{enumerate}

Because of the need to quickly locate the communicator containing the queue of
posted messages, the context id should be an array index.

\subsection{MPI\_Put}
We need to send data.  Q: does the window object contain the base addresses of
all of the local windows, or do we just send the displacement?  For VIA-like
interfaces, I believe that we need only the displacement relative to the
(virtual) connection.

We must also send the datatype.  We need to optimize for the contiquous case.
The Portugal group caches the datatype at the destination (data item for
datatype: where cached).  We will also need a way to invalidate the cache and
to control the amount of storage that is used to cache datatypes.

We clearly need endpoint mapping, both for send\_init and for windows.

Packet types for RMA?

\subsection{MPI\_Accumulate}
Perhaps the hardest case to implement efficiently is the third-party
\code{MPI\_Accumulate} case (using Lock/Unlock synchronization).

\subsection{Multimethod}

The possibilities for handling multimethod communication include:
\begin{enumerate}
\item Each method has a thread that handles communication.  This thread may
  use SIGIO on some systems to cause the thread (signal handler?) to be
  scheduled. 
\item For polling mode, each method should have a relative rate for the poll,
  with yield to OS and yield to other method.  One way to choose these
  parameters is to poll for roundtrip time + epsilon, starting from the
  fastest system.  The roundtrip times could be specified, precomputed (during
  configuration), or computed for each run.
\item A mixture of these, using polling for the very fast devices and a
  service thread for slow ones.
\end{enumerate}

\section{Need to do}

search for ``protocol translators'' in the literature

work out byte vs datatype issues


\section{Coding Standards and Modularity}
The ADI-3 design is more modular than the previous ADI designs.  Adding a new
device or method does not require changing any of the files or code outside
of the device.  This section details how the interfaces between
\code{configure}, \code{mpirun}, \code{MPI\_Init}, and \code{mpiinstall} are
specified.  In addition, a set of standard instrumentation operations should
be implemented by each device.

\subsection{Code Modularity}
The original ADI1 and ADI2 designs required that for each device added, 
\file{configure.in} and \file{mpirun.in} be modified.  In addition, special
features require changes to \file{mpirun.args.in}.  

As an interim test, I've made the following changes to the ADI-2
implementation.  ADI-3 should provide something at least as capable:
\begin{itemize}
\item The main \file{configure.in} now looks for the file
  \file{setup\_<device-name>} in the \file{mpid/<device-name>} directory.
  This script is executed (sourced, actually) and processes the arguments
  \texttt{--usage} for   help information and the variable
  \texttt{device\_args} for device-specific options.  It can access and modify
  the variables used by \file{configure.in} (we need to enumerate the list of
  variables and limit what can be changed).  For example, there are now
  \file{mpid/ch\_p4/setup\_ch\_p4} and \file{mpid/globus/setup\_globus} files.
  When there are specific configure tests to be run for the device, these are
  now run with a \file{configure} file in the device's directory.

\item Device-specific \file{mpirun} scripts are now part of the device
  directory.  To allow both separate scripts, and the use of some of the
  common scripts (such as \file{mpirun.pg.in}), there is a file,
  \file{mpirun.lst}, that lists all of the files that must be moved to the
  device-specific \file{bin} directory for that device.  Files with a
  \file{.in} suffix are still processed.

  In addition, the file \file{mpirun.<default-device>.args} (marked as 
  executable) can also be provided.  This file is sourced by
  \file{mpirun.args} to check for valid, device-specific options.  It also
  must accept the command-line argument \texttt{--usage} and provide 
  information about any special options for that particular device.

  This needs to be generalized for \code{mpirun} programs that are not scripts
  and do not use the generic \code{mpirun} system.
\end{itemize}

Related to these device-specific issues are architecture-specific issues in
configuring specific architectures.  There are three ways to approach this:
\begin{enumerate}
\item Write architecture-specific code and tests directly into the main
  configure script.  There are several problems with this.  First, we have to
  maintain scripts for architectures that we have never seen.  Second, it is
  hard for users to modify the architectural-specific tests (configure is 15K
  lines of shell script; even the configure.in is over 3000 lines).  Third, 
  related architectures can't share the same code (this is made even worse by
  the use of the \texttt{ARCH} variable to set the default library/bin
  directory paths).
\item Write an autoconf macro for the architecture and add it to aclocal.m4
  (or to a file included by aclocal.m4).  The advantage to this is that 
  it is somewhat easier to isolate the architecture-specific code; in
  addition, it can use all of the autoconf macros to double-check any
  features, and can be used by other configure files.  
\item Write a script file to be called when a particular architecture is
  chosen.  This is the most modular (users can test and send us scripts to
  run), but makes it difficult to take advantage of autoconf.  
\end{enumerate}
This is one of those cases where there isn't a simple solution.  The autoconf
capabilities are too useful to give up, so there will be \code{AC\_ARCH\_xxx}
for various architecture families; these are like the existing \code{AC\_AIX}
macros for Unix variants.  These are placed in \file{acarchfamily.m4}. 
In addition, configure will look for
\file{util/archfamily/xxxx} for \code{xxxx} either the value of \code{ARCH} or
the value of \code{--with-archfamily=xxxx}.  If it finds that file, it will
source it.  This will allow users to add shell commands for specific
architectures. 

Other Miscellaneous Issues:
\begin{itemize}
\item Signals.  The MPI standard requires that any signals used be
  documented, whether handlers are established for them or they are used with
  \code{kill}.  Since this information is device specific, yet needs to be in
  the documentation, each device must present this information in a common
  location so that the man pages can be correctly generated.
\end{itemize}

\subsection{Naming Conventions}
All macros are in all upper case.  Functions have an \code{ADI3} prefix
followed by a mixed-case name for ADI-3; a \code{RMQ1} prefix is used for RMQ.
Methods should use \code{RMQ1_xxx}, where \code{xxx} is short for the method.
For example, \code{RMQ1_TCP} or \code{RMQ1_SHM}.

\subsection{Instrumentation}
To include: time waiting, time spinning, time waiting for flow ack, time in
each ADI routine; events for each data packet received; each thread context
switch. 

\subsection{Pointers and Indexes}
How should opaque objects be represented?  There are two obvious choices:
pointers to a structure and an index into an array of structures.  

\begin{center}
\begin{tabular}{p{2.5in}p{2.5in}}
Pointers&Index\\\hline
Direct access (removes the step \&base[idx])&Easy to check for valid values\\
Allows use of entire memory&Bounds memory use\\
Compile-time type checking&C2F and F2C are trivial\\
&Length is the same on all systems (can use 32 or even 16 bit integer)\\
&Small (could use only 16 bits in send-rendezvous rather than the 64 bits
required on systems with 64-bit pointers)\\
Easy to extend (just malloc)&Easy to preallocate\\
\end{tabular}
\end{center}
Note that in both cases, thread-safe allocation requires a fairly simple
operation such as compare and swap; fetch and increment is not sufficient
because the objects are not allocated in a stack-like fashion.

A third choice is to used both representations:  all objects are allocated
from an array, providing the advantages of an index-based representation, but
the objects themselves are represented by a pointer to the object, making
direct access faster.

\bibliography{/home/MPI/allbib,/home/gropp/Update/new/gropp,adi3}
\bibliographystyle{plain}

\end{document}
