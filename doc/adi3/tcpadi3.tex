%
% This file contains a discussion of a possible TCP device
% implementation of ADI3  

\documentclass{article}
\usepackage{/home/gropp/data/share/refman}
\usepackage{/home/gropp/sowing-proj/sowing/docs/doctext/tpage}
\usepackage{url} 
\usepackage{epsf}
\usepackage{psfig}
\textheight=9in
\textwidth=6.1in
\oddsidemargin=.2in
\topmargin=-.50in

\def\nopound{\catcode`\#=13}
{\nopound\gdef#{{\tt \char`\#}}}
\catcode`\_=13
\def_{{\tt \char`\_}}
\catcode`\_=11
\def\code#1{\texttt{#1}}
\let\file=\code

\begin{document}

\title{A TCP Implementation of the ADI-3}
\author{}
\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
This document outlines an implementation of the ADI on TCP.  It defines a
specific interface to the low level OS TCP operations, and outlines a way
for at least the basic \code{MPID_} routines to be implemented in terms of
these 
abstract operations.  This document is preliminary.

Some goals:

``straight through'', common cases involve as little overhead as possible.
For example, a send/receive of a single word should generate as few ``extra''
allocations of internal objects as possible.  For example, on a send, this
design allows the data to be sent directly without creating a
\code{MPID_Request}. We haven't quite managed that on the receive side.

Provide a relatively small interface that can be used to port MPICH to new
platforms.  This replaces the ADI-2 ``channel'' interface.

Provide an example that can use remote write (put) operations for data
transfers.  

It is \emph{not} a goal to provide an optimally fast implementation. This is
intended to be a relatively simple but reasonably efficient implementation.

\section{Outline of the Implementation Structure}

Layers: 

The MPID routines are implemented in terms of a smaller set of routines that
perform relatively simple data communication operations.  These are designed
so that they can easily be implemented with, for example, TCP, but are not
restricted to TCP.  For those familar with the ``channel device'' in ADI-2,
these routines represent the ADI-3 version of the channel device interface.
These routines are prefixed with \code{CH3_} (really \code{MPID_CH3_}) to
indicate that they belong to 
this interface; a complete description of the CH3 routines is presented in
Appendix~\ref{app:ch3}. 

Communication:

The MPID layer communicates by sending messages consisting of a message header
(called a packet header to distinguish it from an MPI message), possibly
followed by data.  The MPID layer defines:
\begin{description}
\item[Packet Types]An \code{enum} of types, this is roughly a dozen kinds of
  message that are needed to implement MPI message-passing semantics.  The
  packet types and what they contain are:
  \begin{description}
    \item[eager_send] MPI envelope; data immediately follows the packet (with
      no separate header)
    \item[rndv_req_to_send]MPI envelope and send request id.
    \item[rndv_OK_to_send]Send and receive request id
    \item[put]Address and length, followed immediately by data
    \item[rndv_send]send and receive request id, followed immediately by data
    \item[cancel_send]Send request id to cancel
    \item[cancel_send_ack]Send request id and true/false for was cancelled
    \item[flow_cntl_update]Flow control for eager messages and rendezvous
      requests.  This is separate from any low-level flow control, though it
      may be coordinated with it.  For example, we may include knowledge about
      the size of the socket buffer in this level of flow control.
    \end{description}
\item[Packet Format]The actual layout of a packet; for each packet type, there
  is a corresponding packet format defined by a structure.  For version zero,
  all packet layouts will have the same size in bytes; this simplifies
  the implementation.
\item[Packet Handlers]The code to be invoked when a packet arives at its
  destination. 
\end{description}

Queues and Connections:

(Need some discussion of these, since the requests move between queues and the 
read/write operations are ordered on a connection.)

CH3 routine brief summary:

(I don't think that these are right yet, but we need to start somewhere.)
Details in Appendix~\ref{app:ch3}.

\begin{verbatim}
CH3_iWrite( MPID_VC *, MPID_Request * )
CH3_iStartMsg( MPID_VC *, void *header, int header_count )
CH3_iStartMsgv( MPID_VC *, struct iovec *iov, int count )
CH3_iRead( MPID_VC *, MPID_Request * )
CH3_Progress( int is_blocking )
CH3_Init( )
CH3_Finalize( )
CH3_iPut( MPID_VC *, void *buf, int count, MPID_RemoteAddress offset )
\end{verbatim}

This use of the letter \code{i} in the names is meant to emphasize that these
are non-blocking in the MPI sense: data is not necessarily transfered before
the routine returns and the transfer may continue afterwards (this is what is
different between, for example, \code{CH3_iWrite} and a \code{write} to a
nonblocking socket).

CH3 routines do not handle datatypes; only handle (buf,count) or iovec (viewed
as bytes).  Communication routines are nonblocking, so they must have (a) a
connection to which they are attached (so that data is correctly ordered and
sent/received as possible) and (b) a request that allows incremental
pack/unpack.  Each CH3 communication routine, in effect, makes a callback when
the communication completes.  The action may be as simple as ``decrement busy
flag and dequeue communication'' or as complex as ``pack next buffer for
sending and send it''.  However, we do not require the full generality of
arbitrary callback routines, so the action to take on completion will be
specified by an integer.  The value will either indicate ``decrement busy
counter and dequeue'' (the common case; it applies to all contiguous data
transfers) or ``call MPIDi_Advance_communication'' with the connection and
request as arguments (an alternative is to store the function in the request,
and use the null function as the decrement-busy-and-dequeue case).

These routines must be prepared to create actual connections (e.g., establish
a socket) if there is connection already present.  It is up to the
implementation of the CH3 routines to decide how this is accomplished.

\code{CH3_Put}:

By having a CH3 routine that can perform a contiguous put to remote memory, we
make it easy to experiment with RMA-capable networking (such as VIA) without
requiring a completely new device implementation.  However, this is an
optional routine (at least while we target only MPI-1) and need not be
implemented.  However, the packet handler descriptions given show how a put
operation can be implemented, even in a TCP/sockets environment.

Unresolved questions on CH3:

Who provides the queue manipulation routines (e.g., FOA)?  Is this part of the
base set of CH3 routines, or is it an MPID routine?  Or is it
something in the middle?  

Thread-safety of FOA (currently, inserted request must be marked as unready
unless the other receive parameters are passed to FOA so that the request can
be atomically created).  One possibility is to combine this with the request
state and have a state update function that ensures that any modifications are
written to memory (e.g., using a write barrier) before another thread might
access the request.


\begin{verbatim}
Notes from the board:

  The MPI application layer

                                        owns/allocates MPI_Status

___________ MPI_Isend, MPI_IRecv, MPI_Wait _______________________

  The MPI Layer                         owns/allocates
  (aka MPIR layer)                          communicators
  (mpich2.tex)                              datatypes
                                            attributes
                                            groups

____goals.tex____ MPID_Isend, MPID_Irecv, Progress routines ______

                                        owns/allocates

                                            packet-type handlers
                                            data structures connecting
                                            fds to handlers, which call
                                            TCP layer, below 
  packets


____tcpadi3.tex____CH3_Writev, CH3_Startmsgv ____________________


     select                              owns/allocates
     readv                                     MPID_VC connections
     writev                                    fd readahead buffer
     non-blocking connect                      connection state
                                            requests
                                            segments (in requests)

\end{verbatim}

\section{Pseudo-code for some of the \code{MPID_} routines}
This section outlines the code for some of the major MPID routines.

\subsection{Sending}
The code for \code{MPID_Isend} is shown below.  The code for the other
MPID send routines is similar, with the appropriate choice of eager
(for rsend) or rendezvous (for ssend) operations.  Blocking and
nonblocking versions differ only in whether a request is returned if
the message is complete.

\begin{verbatim}
MPID_Isend( )
{
    if (eager) {
        create packet on stack
        fill in as eager send packet
        request = CH3_iStartmsg #(might copy packet, remember user buf ptr)
        if request
            return request
        else 
            return completed-request # MPID_Send returns null instead
         }
    else (rendezvous) {
        create packet on stack
        fill as rndv_req to send
        request = CH3_iStartmsg #(copies packet if can't send)
        fill in rest of request (datatype, buffer, count)
        CH3U_Request_change_state( waiting to rndv )
    } 

}
\end{verbatim}

\subsection{Receiving}
Both \code{MPID_Irecv} and \code{MPID_Recv} use similar code.

\begin{verbatim}
MPID_Irecv( )
{
    CH3_Progress_poke
    request = MPID_CH3U_FOA( tag, context_id, source, &found )
    if (found)  /*unexpected */ {
        if (eager)
            copy data
            mark request completed
        else
            # rendezvous
            create packet on stack
            fill in as rndv_ok_to_send
            CH3_iAck( request, packet )
    {
    else {
        fill in request
        CH3U_Request_change_state( waiting for match )
    }
}
\end{verbatim}

Note that this code cannot avoid the allocation of a request, even in
the case of \code{MPID_Recv} and the data is already available.  To
optimize for low latency in the case of a small, contiguous transfer,
we may want to have a version of \code{MPID_Recv} that looks something
like
\begin{verbatim}
    if (datatype is contiguous and small &&
        receive queue for this tag/context/source is empty &&
        no active receive request) {
        Try to read next packet
        if (packet read) {
            if (packet type is eager &&
                MPI envelope matches this receive) {
                transfer data to destination
                if (transfer complete) return # null request since done.
                else {
                    create request, make active
                    return request
                }
            }
            else {
                dispatch packet (e.g., same code as in progress engine)
            }
	} 
    }
    /* fall through in case we didn't receive the message */
    MPID_Irecv( ... )
\end{verbatim}
An advantage of this is that it avoids both the need for allocating a
request and it avoids calling the Progress routine (note that we must
still ensure that the progress routine is called sufficiently often).
The complicated tests are necessary to ensure that correct message
ordering is preserved. Note that the test ``receive queue for this tag
etc.'' need not be perfect in that false negatives (queue may be
nonempty) are allowed, since this only drops the code into the
\code{MPID_Irecv} case.  For example, a simple test to see if the
queue is empty is sufficient.

This example also serves to illustrate why the MPID interface includes
blocking receive.

\subsection{Completion}
The ADI does not provide completion routines that correspond directly
to the MPI completion routines (e.g., \code{MPI_Test}).  Instead,
there are routines to make progress on communication.  To test whether
a request is complete, the \mpids{MPID_Request}{busy} flag is checked.

The progress routine may be implemented as
\begin{verbatim}
CH3_Progress_wait/test()
{
    select/poll on all fd's (wait is blocking, test is not)
    For each fd, find associated MPID_VC.
    if (write)
        # fd was set because data is waiting
        send data described by active request in MPID_VC.  
        If (complete)
            invoke request_state_method.
            if no active send requests, clear need-to-write
    if (read)
        switch on state
            case reading pkt hdr: 
                switch on packet headr
                    call fcn associated with pkt type
                      (These are MPID functions, not TCP functions, 
                      since pkts are in MPID layer.  Call them MPID_CH3_) 
                end switch
            case reading data to known address:
                 read more data.
                 if (complete)
                     invoke request_state_method
        end switch
}
\end{verbatim}

\begin{verbatim}

Unresolved issue:  request queue changes/transitions in detail
  create
  move
  update
\end{verbatim}


\section{Pseudo-code for Message Handlers}
These are the routines that are called by the progress engine on receiving a
message packet.  All of these assume that the entire packet header has been 
read but that any following data may not yet have been read.

\subsection{EagerSend}
Action invoked by the receiver of an eagerly sent message.
\begin{verbatim}
    request = FOA( tag, source, contextid, &found )
    if (found) {
        CH3_iRead( request )
    }
    else {
        request->active_buf = AllocateStorageFromEagerBuffer( len )
        CH3_iRead( request )
    }
\end{verbatim}

\subsection{RndvReqToSend}
Action invoked by the reciever of a rendezvous message
\begin{verbatim}
    request = FOA( tag, source, contextid, &found )
    if (found) {
        if (dest buffer is contiguous)
            create rndv-ok-to-put packet on stack
            fill in packet
            CH3_iAck( request, packet )
        else
            create rndv-ok-to-send packet on stack
            CH3_iAck( request, packet )
    }
\end{verbatim}

\subsection{RndvOkToSend}
Action invoked by the sender of a rendezvous message on receipt of an
acknowledgement from the receiver.
\begin{verbatim}
    Find matching request (from id)
    create RndvData packet
    CH3_iAckWrite( request, packet )
\end{verbatim}

\subsection{Put}
Action invoked by reciever of a put packet
\begin{verbatim}
    Read Address from packet
    CH3_iRead( ? no request )
\end{verbatim}

\subsection{RndvData}
Action invoked by the receiver of data sent in response to an ok to send after
a rendezvous message
\begin{verbatim}
    Find matching request (from id)
    Find memory location
    CH3_iRead( request )
\end{verbatim}

\subsection{CancelSend}
\begin{verbatim}
    Find matching request (from id)
    If (found and alread matched) {
        create CancelSendAck(failed) on stack
        CH3_iAck( request, packet )
    }
    else {
        Remove and discard request
        create CancelSendAck(succeeded) on stack
        CH3_iAck( request, packet )
    }
 \end{verbatim}

\subsection{CancelSendAck}
\begin{verbatim}
   Set request to indicate whether cancel succeeded
\end{verbatim}

\subsection{FlowControlUpdate}
\begin{verbatim}
   (Not yet defined)
\end{verbatim}


\section{Implementing \code{mpiexec}}
\label{sec:mpiexec}

(This is a temporary spot for these remarks, since they may apply to more than
just the TCP device.)

There are multiple possible implementations for \code{mpiexec}, and each has
its own advantages and disadvantages.  We might implement all of them, but we
should implement the quickest-to-implement first.

\begin{description}
\item[As MPD console program] This is ready-to-go as a minor change to
  \file{mpdcon.c}, except that the BNR interface might need to be updated to
  match 
  the current specification.  A version of \code{MPI_Info} is needed for the
  new 
  \code{BNR_Spawn}, but not for anything else, so MPI-1 routines should be OK.
  I.e, 
  the database part of BNR is already running.  All handling of \code{stdio}
  is done.
\item[As a BNR program] This requires the above plus implementation of
  \code{BNR_Spawn}, at least for use by console.  It could also be built to
  interact 
  with a scheduler.  This (using \code{BNR_Spawn} to start the initial
  processes as 
  well as for the implementation of \code{MPI_Spawn}) was the ``original''
  plan.
\item[As an MPI program] This is the idea in the current MPICH2 document.  It
  relies on \code{MPI_Connect}, etc.  It requires the above plus the MM
  component 
  of the BNR interface, to set up the connections.  This approach as the
  advantage of providinga ``universal'' \code{mpiexec}. 
\item[As an ``immediate scheduler''] This makes \code{mpiexec} into a stand-in
  for the scheduler component of the Scalable System Software Project.  It is
  much like the ``MPD console'' option, but instead of using the existing
  console code to contact a local MPD, it sends the standard XML defined by
  the SSS project to the MPD, which is standing in for an arbitrary process
  startup component.  It requires hooking in an XML parser like \code{xpat}
  into the MPD and having \code{mpiexec} emit XML code.
\item[As a ``one-host-only'' process starter] The \code{mpiexec} process could
  simply fork the application processes.  This requires a new but simple
  implementation of the put/get/fence part of the BNR interface.  The original
  \code{mpiexec} process could become the database server part after forking.
\end{description}

The first and last options seem to present the shortest paths to getting
something running that we can use to debug the coming avalanche of code with.

Any of these need to contain the argument-processing code for the defined
standard arguments to \code{mpiexec}.  These are defined in Volume 1 of \emph{ MPI---The Complete Reference}, starting on page 353.  There are multiple
approaches to dealing with arguments.
\begin{description}
\item[Plain] Use straightforward code as in \file{p4_args.c}.
\item[Fancy] Use an ``options database'' approach, as in PETSc.
\end{description}

We will want to do both, but the first option can be implemented
immediately, especially if we postpone some of the more elaborate argument
lists and require that those be used with a file.  We have to define the
format of the file for use with the \code{-file} option.  There are three
possibilities.
\begin{description}
\item[Keyword=value pairs] This is easy to read, and we can use the
  parsing routines from MPD, so we are practically already done.
\item[XML] We could match the process-startup file to the format of a
  process-startup request as being defined by the Scalable Systems Software
  Project.  This would be sort of cool.  Validating XML parsers in C exist.
\item[Custom Format] We could define our own formats, so that we could express
  anything whatsoever.  We could use multiple formats to match other software
  that we might find it useful to be compatible with, such as schedulers and
  other process managers.
\end{description}

Again, we might want to implement all three of these, since each has
advantages.  The quickest option is the first.


% \section{Summary}
% \label{sec:tcpadi-summary}
% (This section should summarize the \code{CH3_} routines, giving just their
% prototypes) 

\appendix
\section{CH3 Routines}
\label{app:ch3}

This section provides pseudocode for a TCP implementation of the CH3
routines.

\subsection{iWrite}
iWrite is called to continue a data transfer.  This is used by the
segment processing code to handle the incremental packing and sending
of noncontiguous datatypes.

\begin{verbatim}
CH3_iWrite( MPID_VC *, MPID_Request * )
    if request is not the active request on this VC, internal error
    try to write on fd associated with MPID_VC.  
    record amount of data written in request.
    If all data written
        switch(request->state)
            0: decrement request->busy and remove this request from
               the active list
            >0: invoke MPID_CH3_Request_update( request )
    return
\end{verbatim}
The request update routine handles incremental packing; typically, it

\begin{verbatim}
CH3_iStartMsg( MPID_VC *, void *header, int header_count )
\end{verbatim}

\begin{verbatim}
CH3_iStartMsgv( MPID_VC *, struct iovec *iov, int count )
\end{verbatim}

\begin{verbatim}
CH3_iRead( MPID_VC *, MPID_Request * )
\end{verbatim}

\begin{verbatim}
CH3_Progress( int is_blocking )
\end{verbatim}

\begin{verbatim}
CH3_Init( )
\end{verbatim}

\begin{verbatim}
CH3_Finalize( )
\end{verbatim}

\begin{verbatim}
CH3_iPut( MPID_VC *, void *buf, int count, MPID_RemoteAddress offset )
\end{verbatim}

\end{document}
