%
% This file contains a discussion of a possible TCP device
% implementation of ADI3  

\documentclass{article}
\usepackage{/home/gropp/data/share/refman}
\usepackage{/home/gropp/sowing-proj/sowing/docs/doctext/tpage}
\usepackage{url} 
\usepackage{epsf}
\usepackage{psfig}
\textheight=9in
\textwidth=6.1in
\oddsidemargin=.2in
\topmargin=-.50in
% \mpids{MPI_Comm}{rank}.  Prints the second argument.
\def\mpids#1#2{\code{#2}\index{#1!#2}}

\def\nopound{\catcode`\#=13}
{\nopound\gdef#{{\tt \char`\#}}}
%\catcode`\_=13
%\def_{\texttt{\char`\_}}
%\catcode`\_=11
%\def\code#1{\texttt{#1}}
\def\code{\begingroup\makeustext\eatcode}
\def\eatcode#1{\texttt{#1}\endgroup}
\let\file=\code

\begin{document}

\title{A TCP Implementation of the ADI-3}
\author{}
\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
This document outlines an implementation of the ADI on TCP.  It defines a
specific interface to the low level OS TCP operations, and outlines a way
for at least the basic \code{MPID_} routines to be implemented in terms of
these 
abstract operations.  This document is preliminary.

Some goals:

``straight through'', common cases involve as little overhead as possible.
For example, a send/receive of a single word should generate as few ``extra''
allocations of internal objects as possible.  For example, on a send, this
design allows the data to be sent directly without creating a
\code{MPID_Request}. We haven't quite managed that on the receive side.

Provide a relatively small interface that can be used to port MPICH to new
platforms.  This replaces the ADI-2 ``channel'' interface.

Provide an example that can use remote write (put) operations for data
transfers.  

It is \emph{not} a goal to provide an optimally fast implementation. This is
intended to be a relatively simple but reasonably efficient implementation.

\section{Outline of the Implementation Structure}

Layers: 

The MPID routines are implemented in terms of a smaller set of routines that
perform relatively simple data communication operations.  These are designed
so that they can easily be implemented with, for example, TCP, but are not
restricted to TCP.  For those familar with the ``channel device'' in ADI-2,
these routines represent the ADI-3 version of the channel device interface.
These routines are prefixed with \code{CH3_} (really \code{MPID_CH3_}) to
indicate that they belong to 
this interface; a complete description of the CH3 routines is presented in
Appendix~\ref{app:ch3}. 

Communication:

The MPID layer communicates by sending messages consisting of a message header
(called a packet header to distinguish it from an MPI message), possibly
followed by data.  The MPID layer defines (internal to itself, so this
discussion only applies to the CH3/TCP implementation of MPID):
\begin{description}
\item[Packet Types]An \code{enum} of types, this is roughly a dozen kinds of
  message that are needed to implement MPI message-passing semantics.  The
  packet types and what they contain include:
  \begin{description}
    \item[eager\_send] MPI envelope; data immediately follows the packet (with
      no separate header).  An MPI envelope contains the data used to match
      MPI messages: tag, sender's rank, and context id, along with any MPI 
      flow control (for eager messages) and error-checking features such as
      datatype signatures.
    \item[rndv\_req\_to\_send]MPI envelope and send request id.
    \item[rndv\_OK\_to\_send]Send and receive request id
    \item[put]Address and length, followed immediately by data
    \item[rndv\_send]send and receive request id, followed immediately by data
    \item[cancel\_send]Send request id to cancel
    \item[cancel\_send\_ack]Send request id and true/false for was cancelled
    \item[flow\_cntl\_update]Flow control for eager messages and rendezvous
      requests.  This is separate from any low-level flow control, though it
      may be coordinated with it.  For example, we may include knowledge about
      the size of the socket buffer in this level of flow control.
    \end{description}
    Additional packet types will be defined to support MPI-2 operations such
    as RMA.
\item[Packet Format]The actual layout of a packet; for each packet type, there
  is a corresponding packet format defined by a structure.  For version zero,
  all packet layouts will have the same size in bytes; this simplifies
  the implementation.
\item[Packet Handlers]The code to be invoked when a packet arives at its
  destination. 
\end{description}

Queues and Connections:

(Need some discussion of these, since the requests move between queues and the 
read/write operations are ordered on a connection.)

CH3 routine brief summary:

(I don't think that these are right yet, but we need to start somewhere.)
Details in Appendix~\ref{app:ch3}.

\begin{description}
\item[CH3\_new\_request]Create a new request. This is used with
  \code{CH3_iSend}, and for cases where a completed request is required (see
  the discussion of \code{MPID_Isend}).
\item[CH3\_iStartMsg]Begin a message.  Return a request if the message has not
  been completely sent.
\item[CH3\_iStartMsgv]Like \code{CH3_iStartmsg}, but with an \code{struct
    iovec}.
\item[CH3\_iSend]Send data using an existing request
\item[CH3\_iContWrite]Like \code{CH3_iSend}, but the data sent is within the
  request (the \mpids{MPID_Request}{active\_buf} member).
\item[CH3\_iRead]Read data to a location specified by a request
\item[CH3\_Progress\_xxx]Progress functions.  \code{CH3_Progress} is
  responsible for dispatching incoming messages.
\item[CH3\_Init]Initialize the device and setup the initial communicators
\item[CH3\_Finalize]Finalize the device
\item[CH3\_InitParent]Initialize the parent communicator (if one exists).
\item[CH3\_iPut]Nonblocking, contiguous put into remote memory (optional,
  provided as a hook for non-TCP methods and as a placeholder for the
  routines necessary for implementing MPI-2 RMA).
\end{description}

The bindings for these routines are:
\begin{verbatim}
MPID_Request *CH3_new_request( void );
MPID_Request *CH3_iStartMsg( MPID_VC *, void *header, int header_count )
MPID_Request *CH3_iStartMsgv( MPID_VC *, struct iovec *iov, int count )

void CH3_iSend( MPID_Request *, void *header, int header_count )
void CH3_iContWrite( MPID_VC *, MPID_Request * )
void CH3_iRead( MPID_VC *, MPID_Request * )

void CH3_Progress_start( void )
void CH3_Progress_end( void )
int CH3_Progress( int is_blocking )

int CH3_Init( int *has_args, int *has_env )
void CH3_Finalize( void )
void CH3_InitParent( MPID_Request *parent )

void CH3_iPut( MPID_VC *, void *buf, int count, MPID_RAint offset, 
               MPID_RAint cmpl_flag )
\end{verbatim}

The use of the letter \code{i} in the names is meant to emphasize that these
are non-blocking in the MPI sense: data is not necessarily transfered before
the routine returns and the transfer may continue afterwards (this is what is
different between, for example, \code{CH3_iWrite} and a \code{write} to a
nonblocking socket).

CH3 routines do not handle datatypes; only handle (buf,count) or struct iovec
(viewed 
as bytes).  Communication routines are nonblocking, so they must have (a) a
connection to which they are attached (so that data is correctly ordered and
sent/received) and (b) a request that allows incremental
pack/unpack.  Each CH3 communication routine, in effect, makes a callback when
the communication completes.  The action may be as simple as ``decrement busy
flag and dequeue communication'' or as complex as ``pack next buffer for
sending and send it''.  However, we do not require the full generality of
arbitrary callback routines, so the action to take on completion will be
specified by an integer.  The value will either indicate ``decrement busy
counter and dequeue'' (the common case; it applies to all contiguous data
transfers) or ``call MPIDi\_Advance\_communication'' with the connection and
request as arguments (an alternative is to store the function in the request,
and use the null function as the decrement-busy-and-dequeue case).

These routines must be prepared to create actual connections (e.g., establish
a socket) if there is connection already present.  It is up to the
implementation of the CH3 routines to decide how this is accomplished.

\paragraph{Consequences.}
We can summarize the requirements for the CH3 interface as:
\begin{enumerate}
\item Nonblocking.  Correct operation of the code is not dependent on any read
  or write operation completing when first issued.  
\item Contiguous (or simple iovec) data.  For simplicity, only contiguous byte
  ranges (or Unix-style iovec) data moves are handled at the lowest level
\item Correctness.  MPI requires that the message envelopes are ordered; data
  transfers must also arrive in expected order.
\item Handshakes.  Some communication, particularly rendezvous transfers,
  requires handshakes or cooperation between sender and receiver
\item Low latency for short messages.
\end{enumerate}
These requirements suggest that there be a single data structure that holds
the progress of all communication. 
In particular, to correctly support the first (nonblocking) requirement, there
must be a queue (a queue because of the third (ordering) requirement) of
pending data transfer operations.  
Because of the second (contiguous) requirement, combined with the need to
handle general (possibly noncontiguous) MPI datatypes, we must be able to
transfer data in parts (segments).  This requires having the option of
invoking a routine when a data transfer completes (the callback described
above); further, it requires that the queue element not be automatically
removed from the data transfer queue when the current communication completes
because more data for this communication may be on the way.

Because of the fourth (handshake) requirement, once communication has been
initiated, the same data structure should be used with any communication that
requires a handshake, rather than generating a new data structure for each
individual communication operation.  

The data structure that satisfies these requirements is the
\code{MPID_Request}.  Further, once communication is started (e.g., with
\code{CH3_Startmsg}, further data transfers are accomplished by either placing
the request into the queue of pending data transfers or by updating a request
that is already the active request (rather than creating a new request).

To handle the communication of data, the CH3 layer needs the following fields
in the \code{MPID_Request}:
\begin{description}
\item[comm\_state]State of the communication.  This indicates what operation
  should be performed when a data transfer is complete.
\item[active\_buf]Pointer to the data to transfer.  It uses \code{char} so
  that byte address offsets can be applied to the buffer address.
\item[count\_left]Number of bytes that remain to be transfered.
\item[connection]A pointer to the structure holding the communication
  information (e.g., things like the fd).
\item[top\_seg]Segment used to process noncontiguous data
\end{description}

Thus, a \code{MPID_Request} contains
\begin{verbatim}
typedef struct { 
    ...
    int  comm_state;
    char *active_buf;
    int  count_left;
    MPID_VC *connection;
    MPID_Segment top_seg;
    ... } MPID_Request;
\end{verbatim}


\code{CH3_Put}:

By having a CH3 routine that can perform a contiguous put to remote memory, we
make it easy to experiment with RMA-capable networking (such as VIA) without
requiring a completely new device implementation.  However, this is an
optional routine (at least while we target only MPI-1) and need not be
implemented.  However, the packet handler descriptions given show how a put
operation can be implemented, even in a TCP/sockets environment.

Unresolved questions on CH3:

Who provides the queue manipulation routines (e.g., FOA)?  Is this part of the
base set of CH3 routines, or is it an MPID routine?  Or is it
something in the middle?  

Thread-safety of FOA (currently, inserted request must be marked as unready
unless the other receive parameters are passed to FOA so that the request can
be atomically created).  One possibility is to combine this with the request
state and have a state update function that ensures that any modifications are
written to memory (e.g., using a write barrier) before another thread might
access the request.

\subsection{Data Structures}
\begin{description}
\item[MPI application layer](User programs) Owns and allocates
  \code{MPI_Status} 
\item[MPI implementation layer](e.g., implementation of \code{MPI_Isend} in
  terms of \code{MPID_Isend}) Owns and allocates communicators, datatypes,
  attributes, groups, files, window objects, keyvals, error handlers.
\item[MPID Channel implementation layer](e.g., implementation of
  \code{MPID_Isend} in terms of CH3) Owns and allocates packets.
  Defines packet type handlers.
\item[CH3 layer] Owns and allocates connections, requests, segments (within
  requests) 
\end{description}

% \begin{verbatim}
% Notes from the board:

%   The MPI application layer

%                                         owns/allocates MPI_Status

% ___________ MPI_Isend, MPI_IRecv, MPI_Wait _______________________

%   The MPI Layer                         owns/allocates
%   (aka MPIR layer)                          communicators
%   (mpich2.tex)                              datatypes
%                                             attributes
%                                             groups

% ____goals.tex____ MPID_Isend, MPID_Irecv, Progress routines ______

%                                         owns/allocates

%                                             packet-type handlers
%                                             data structures connecting
%                                             fds to handlers, which call
%                                             TCP layer, below 
%   packets


% ____tcpadi3.tex____CH3_Writev, CH3_Startmsgv ____________________


%      select                              owns/allocates
%      readv                                     MPID_VC connections
%      writev                                    fd readahead buffer
%      non-blocking connect                      connection state
%                                             requests
%                                             segments (in requests)

% \end{verbatim}

\section{Pseudo-code for some of the \code{MPID\_} routines}
This section outlines the code for some of the major MPID routines.

\subsection{Sending}
The code for \code{MPID_Isend} is shown below.  The code for the other
MPID send routines is similar, with the appropriate choice of eager
(for rsend) or rendezvous (for ssend) operations.  Blocking and
nonblocking versions differ only in whether a request is returned if
the message is complete.  The envelopes for ready-send messages should include
a flag that indicates that they are ready-send so that the user-error of an
unmatched ready-send can be detected.


\begin{verbatim}
MPID_Isend( )
{
    if (eager) {
        create packet on stack
        fill in as eager send packet
        may need to pack noncontig data into contig buffer
        request = CH3_iStartmsgv #(might copy packet, remember user buf ptr)
        if request
            if packed, remember pack buffer in request
            return request
        else 
            if packed, free pack buffer
            return completed-request # MPID_Send returns null instead
         }
    else (rendezvous) {
        create packet on stack
        fill as rndv_req to send
        request = CH3_iStartmsg #(copies packet if can't send)
        fill in rest of request (datatype, buffer, count)
        CH3U_Request_change_state( waiting to rndv )
    } 
}
\end{verbatim}

An alternative to creating packets on the stack is to allow the CH3 layer to
provide a way to create a new packet. The semantics would allow simple
allocation as above, but would also allow the CH3 layer to provide specially
allocated memory.  For the near term, however, we will not include this
enhancement. 

\subsection{Receiving}
Both \code{MPID_Irecv} and \code{MPID_Recv} use similar code.

\begin{verbatim}
MPID_Irecv( )
{
    CH3_Progress_poke
    request = MPID_CH3U_FOA( tag, context_id, source, &found )
    if (found)  {
        /* Message was found in unexpected list */
        if (eager) {
            copy data
            free eager buffer used for data
            mark request completed
            }
        else {
            # rendezvous
            create packet on stack
            fill in as rndv_ok_to_send
            CH3_iAck( request, packet )
            }
    }
    else {
        fill in request
        CH3U_Request_change_state( waiting for match )
    }
}
\end{verbatim}

Note that this code cannot avoid the allocation of a request, even in
the case of \code{MPID_Recv} and where the data is already available in the
socket.  To
optimize for low latency in the case of a small, contiguous transfer,
we may want to have a version of \code{MPID_Recv} that looks something
like
\begin{verbatim}
    if (datatype is contiguous and small &&
        receive queue for this tag/context/source is empty &&
        no active receive request) {
        Try to read next packet
        if (packet read) {
            if (packet type is eager &&
                MPI envelope matches this receive) {
                transfer data to destination
                if (transfer complete) return # null request since done.
                else {
                    create request, make active
                    return request
                }
            }
            else {
                dispatch packet (e.g., same code as in progress engine)
            }
        } 
    }
    /* fall through in case we didn't receive the message */
    MPID_Irecv( ... )
\end{verbatim}
An advantage of this is that it avoids both the need for allocating a
request and it avoids calling the Progress routine (note that we must
still ensure that the progress routine is called sufficiently often).
The complicated tests are necessary to ensure that correct message
ordering is preserved. Note that the test ``receive queue for this tag
etc.'' need not be perfect in that false negatives (queue may be
nonempty) are allowed, since this only drops the code into the
\code{MPID_Irecv} case.  For example, a simple test to see if the
queue is empty is sufficient.

This example also serves to illustrate why the MPID interface includes
blocking receive.

\subsection{Completion}
The ADI does not provide completion routines that correspond directly
to the MPI completion routines (e.g., \code{MPI_Test}).  Instead,
there are routines to make progress on communication.  To test whether
a request is complete, the \mpids{MPID_Request}{busy} flag is checked.

The progress routine may be implemented as
\begin{verbatim}
CH3_Progress_wait/test()
{
    select/poll on all fd's (wait is blocking, test is not)
    For each fd, find associated MPID_VC. (perhaps using fd_to_vc[] array)
    if (write) {
        # fd was set because data is waiting
        do {
            send data described by active request in MPID_VC.  
            If (complete)
                invoke request_state_method.
                if no active send requests, clear need-to-write
            } while (can write and pending writes)
    }
    if (read) {
        do {
            switch on state
                case reading pkt hdr: 
                    switch on packet headr
                        call fcn associated with pkt type
                        (These are MPID functions, not TCP functions, 
                        since pkts are in MPID layer.  Call them MPID_CH3_xxx) 
                    end switch
                case reading data to known address:
                     read more data.
                     if (complete)
                         invoke request_state_method
                case establishing new connection:
                     ...
            end switch
        } while (can read)
    }
}
\end{verbatim}
To make this more efficient, maintain the data structures needed with the
\code{poll} or \code{select} calls with the active fd's: the ones on which
there are either pending writes or on which connections for reading have been
established.  

The above version of \code{CH3_Progress_wait} is for single-threaded
implementations.  Multi-threaded versions must keep track of whether any
completions occurred after \code{CH3_Progress_start} was called.

% \begin{verbatim}

% Unresolved issue:  request queue changes/transitions in detail
%   create
%   move
%   update
% \end{verbatim}


\section{Pseudo-code for Message Handlers}
These are the routines that are called by the progress engine on receiving a
message packet.  All of these assume that the entire packet header has been 
read but that any following data may not yet have been read.

\subsection{EagerSend}
Action invoked by the receiver of an eagerly sent message.
\begin{verbatim}
    request = FOA( tag, source, contextid, &found )
    if (found) {
        CH3_iRead( request )
    }
    else {
        if (message is readysend) {
             signal error 
             arrange to read and discard data 
             (simply allow the read as below; set the state to discard
             when the transfer is complete)
        }
        request->active_buf = AllocateStorageFromEagerBuffer( len )
        CH3_iRead( request )
    }
\end{verbatim}

\subsection{RndvReqToSend}
Action invoked by the reciever of a rendezvous message
\begin{verbatim}
    request = FOA( tag, source, contextid, &found )
    if (found) {
        if (dest buffer is contiguous)
            create rndv-ok-to-put packet on stack
            fill in packet
            CH3_iAck( request, packet )
        else
            create rndv-ok-to-send packet on stack
            CH3_iAck( request, packet )
    }
    else if (ready-send) {
        return an error message to sender
        remove request from unexpected receive queue
    }
\end{verbatim}

\subsection{RndvOkToSend}
Action invoked by the sender of a rendezvous message on receipt of an
acknowledgement from the receiver.
\begin{verbatim}
    Find matching request (from id)
    create RndvData packet
    CH3_iAckWrite( request, packet )
\end{verbatim}

\subsection{Put}
Action invoked by reciever of a put packet.  
\begin{verbatim}
    Read Address from packet
    request = CH3_iRead( address, count )
    if (request) {
        Save flag address in request
        set request state so that on completion of data transfer, flag is
            decremented
    }
    else {
        decrement flag (address provided by packet)
    }
\end{verbatim}
Note: We do need a request to handle incomplete data transfers; by setting the
request's reference count, we can ensure that the request is recovered once
the data transfer completes.  However, in the case where a put is used to
provide the data for a rendezvous receive, there is already an available
request.  Question: do we want a form of put that takes advantage of having an
existing request?  In that case, instead of the remote flag address, the
remote request id can be used.

\subsection{RndvData}
Action invoked by the receiver of data sent in response to an ok to send after
a rendezvous message
\begin{verbatim}
    Find matching request (from id)
    Find memory location (from the request)
    CH3_iRead( request )
\end{verbatim}

\subsection{CancelSend}
\begin{verbatim}
    Find matching request (from id)
    If (found and alread matched) {
        create CancelSendAck(failed) on stack
        newrequest = CH3_iStartmsg( packet )
    }
    else {
        Remove and discard request
        create CancelSendAck(succeeded) on stack
        newrequest = CH3_iStartmsg( packet )
    }
    if (newrequest) decrement ref count of newrequest
\end{verbatim}

\subsection{CancelSendAck}
\begin{verbatim}
   Set request to indicate whether cancel succeeded
   If succeeded, remove from pending send list
\end{verbatim}

\subsection{FlowControlUpdate}
\begin{verbatim}
   (Not yet defined)
\end{verbatim}
Flow control is used at the MPI level to control the use of eager buffers and
requests for unexpected messages.  Possible choices for flow control include
IMPI-style control or an integrated count of the number of envelopes and
buffer space used.  This is \emph{not} optional, though the early
implementation can ignore this.

\section{Implementing \code{mpiexec}}
\label{sec:mpiexec}

(This is a temporary spot for these remarks, since they may apply to more than
just the TCP device.)

There are multiple possible implementations for \code{mpiexec}, and each has
its own advantages and disadvantages.  We might implement all of them, but we
should implement the quickest-to-implement first.

\begin{description}
\item[As MPD console program] This is ready-to-go as a minor change to
  \file{mpdcon.c}, except that the BNR interface might need to be updated to
  match 
  the current specification.  A version of \code{MPI_Info} is needed for the
  new 
  \code{BNR_Spawn}, but not for anything else, so MPI-1 routines should be OK.
  I.e, 
  the database part of BNR is already running.  All handling of \code{stdio}
  is done.
\item[As a BNR program] This requires the above plus implementation of
  \code{BNR_Spawn}, at least for use by console.  It could also be built to
  interact 
  with a scheduler.  This (using \code{BNR_Spawn} to start the initial
  processes as 
  well as for the implementation of \code{MPI_Spawn}) was the ``original''
  plan.
\item[As an MPI program] This is the idea in the current MPICH2 document.  It
  relies on \code{MPI_Connect}, etc.  It requires the above plus the MM
  component 
  of the BNR interface, to set up the connections.  This approach as the
  advantage of providinga ``universal'' \code{mpiexec}. 
\item[As an ``immediate scheduler''] This makes \code{mpiexec} into a stand-in
  for the scheduler component of the Scalable System Software Project.  It is
  much like the ``MPD console'' option, but instead of using the existing
  console code to contact a local MPD, it sends the standard XML defined by
  the SSS project to the MPD, which is standing in for an arbitrary process
  startup component.  It requires hooking in an XML parser like \code{xpat}
  into the MPD and having \code{mpiexec} emit XML code.
\item[As a ``one-host-only'' process starter] The \code{mpiexec} process could
  simply fork the application processes.  This requires a new but simple
  implementation of the put/get/fence part of the BNR interface.  The original
  \code{mpiexec} process could become the database server part after forking.
\end{description}

The first and last options seem to present the shortest paths to getting
something running that we can use to debug the coming avalanche of code with.

Any of these need to contain the argument-processing code for the defined
standard arguments to \code{mpiexec}.  These are defined in Volume 1 of \emph{ MPI---The Complete Reference}, starting on page 353.  There are multiple
approaches to dealing with arguments.
\begin{description}
\item[Plain] Use straightforward code as in \file{p4_args.c}.
\item[Fancy] Use an ``options database'' approach, as in PETSc.
\end{description}

We will want to do both, but the first option can be implemented
immediately, especially if we postpone some of the more elaborate argument
lists and require that those be used with a file.  We have to define the
format of the file for use with the \code{-file} option.  There are three
possibilities.
\begin{description}
\item[Keyword=value pairs] This is easy to read, and we can use the
  parsing routines from MPD, so we are practically already done.
\item[XML] We could match the process-startup file to the format of a
  process-startup request as being defined by the Scalable Systems Software
  Project.  This would be sort of cool.  Validating XML parsers in C exist.
\item[Custom Format] We could define our own formats, so that we could express
  anything whatsoever.  We could use multiple formats to match other software
  that we might find it useful to be compatible with, such as schedulers and
  other process managers.
\end{description}

Again, we might want to implement all three of these, since each has
advantages.  The quickest option is the first.  However, we could easily
implement an XML-style version of keyword/value pairs using a format such as
\begin{verbatim}
<MPICH keywork=value />
\end{verbatim}


% \section{Summary}
% \label{sec:tcpadi-summary}
% (This section should summarize the \code{CH3_} routines, giving just their
% prototypes) 

\appendix
\section{CH3 Routines and Data Structures}
\label{app:ch3}

This section provides pseudocode for a TCP implementation of the CH3
routines.

General note: in if-else code, the most likely case should be placed first.
This is both faster and moves the most common code branch to the top, where it
makes it easier to grasp the intent of the code.

\subsection{Data Structures}
There are two primary data structures: one for virtual connections and one for
the device as a whole.

\paragraph{Device.}
\begin{verbatim}
typedef struct {
    int           nfds;
    struct pollfd *activefds;
    MPID_Request  *unexpected_recv;    /* List of receives */
    MPID_Request  *posted_recv;        /* List of posted receives */
    } MPID_Device;
\end{verbatim}
The receive lists are held on the device to simplify handling of wildcard
receives.  

\paragraph{Connections.}

\begin{verbatim}
typedef struct { 
    int ref_count; /* Lets us know how many communicators are using
                      this connection */
    int fd;        /* fd for the socket */
    MPID_Request *sending_head,   /* Queue of pending sends */
                 *sending_tail;
    MPID_Request *active_receive; /* Active receive, if any */
    int lpid;      /* Local process id for the partner of this connection
                      (used to implement group routines) */
    } MPID_VC;
\end{verbatim}
The pending sends have a head and a tail pointer because it is a queue and we
want the operations of insert and delete on this queue to be fast.

\subsection{iWrite}
iWrite is called to continue a data transfer.  This is used by the
segment processing code to handle the incremental packing and sending
of noncontiguous datatypes.

\begin{verbatim}
CH3_iWrite( MPID_VC *, MPID_Request * )
    if request is not the active request on this VC, internal error
    try to write on fd associated with MPID_VC.  
    record amount of data written in request.
    If all data written
        switch(request->state)
            0: decrement request->busy and remove this request from
               the active list
            >0: invoke MPID_CH3_Request_update( request )
    return
\end{verbatim}
The request update routine handles incremental packing; typically, it
uses \code{MPIR_Segment_pack} to pack the next segment of bytes and
then calls \code{CH3_iWrite} to continue writing the data (note that
the request is left on the active list in case some other thread want
to start sending data on this connection).

\subsection{iStartMsg}
iStartMsg is called to send a message.  It returns a request if the message is
not completely sent.

\begin{verbatim}
MPID_Request *CH3_iStartMsg( MPID_VC *, void *header, int header_count )
   if there is no active request on this VC
      write the header.
      if the entire header is written, return null
      else /* see note below */
          create a request
          fill it in with the remaining data to write
          make this the active request
          return request               
   else
      create a request
      fill it in with the remaining data to write
      add to the pending send queue
      return request
\end{verbatim}
One possible variation is to eliminate the ``else'' branch labeled ``set note
below'' and let the code fall through into else branch that creates and
inserts the request into the pending send queue.

\begin{verbatim}
CH3_iStartMsgv( MPID_VC *, struct iovec *iov, int count )
Like CH3_iStartMsg, but for all of the data in the iov
\end{verbatim}

\begin{verbatim}
CH3_iRead( MPID_VC *, MPID_Request * )
\end{verbatim}

\begin{verbatim}
CH3_Progress( int is_blocking )
\end{verbatim}
(code shown earlier in this document)

\begin{verbatim}
CH3_Init( )
\end{verbatim}
This routine must
\begin{itemize}
\item Make any calls to BNR
\item Set the size, rank, and \code{MPID_VC} fields in
  \mpids{MPIR\_Process}{comm\_world}, \mpids{MPIR\_Process}{comm\_self}, and
  \mpids{MPIR\_Process}{comm\_parent}. 
\end{itemize}

\begin{verbatim}
CH3_Finalize( )
\end{verbatim}

\begin{verbatim}
CH3_iPut( MPID_VC *, void *buf, int count, MPID_RAint offset, 
          MPID_RAint cmpl_flag )
\end{verbatim}
The remote addresses (\code{MPID_RAint}) cannot be of type \code{MPI_Aint}
because \code{MPI_Aint} 
is the size of an address on the calling system, not necessarily on the target system.
\end{document}
