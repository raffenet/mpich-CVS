\documentclass[dvipdfm,11pt]{article}
\usepackage[dvipdfm]{hyperref} % Upgraded url package
\parskip=.1in

% Formatting conventions for contributors
% 
% A quoting mechanism is needed to set off things like file names, command
% names, code fragments, and other strings that would confuse the flow of
% text if left undistinguished from preceding and following text.  In this
% document we use the LaTeX macro '\texttt' to indicate such text in the
% source, which normally produces, when used as in '\texttt{special text}',
% the typewriter font.

% It is particularly easy to use this convention if one is using emacs as
% the editor and LaTeX mode within emacs for editing LaTeX documents.  In
% such a case the key sequence ^C^F^T (hold down the control key and type
% 'cft') produces '\texttt{}' with the cursor positioned between the
% braces, ready for the special text to be typed.  The closing brace can
% be skipped over by typing ^e (go to the end of the line) if entering
% text or ^C-} to just move the cursor past the brace.

% LaTeX mode is usually loaded automatically.  At Argonne, one way to 
% get several useful emacs tools working for you automatically is to put
% the following in your .emacs file.

% (require 'tex-site)
% (setq LaTeX-mode-hook '(lambda ()
%          		 (auto-fill-mode 1)
%          		 (flyspell-mode 1)
%          		 (reftex-mode 1)
% 			 (setq TeX-command "latex")))


\begin{document}
\markright{MPICH2 Installer's Guide}
\title{{\bf MPICH2 Installer's Guide}\thanks{This work was supported by the
    Mathematical, Information, and Computational Sciences Division
    subprogram of the Office of Advanced Scientific Computing Research,
    SciDAC Program, Office of Science, U.S. Department of Energy, under
    Contract
    W-31-109-ENG-38.}\\
  Version 1.0.1b\\
  Mathematics and Computer Science Division\\
  Argonne National Laboratory}

\author{William Gropp\\
Ewing Lusk\\
David Ashton\\
Darius Buntinas\\
Ralph Butler\\
Anthony Chan\\
Rob Ross\\
Rajeev Thakur\\
Brian Toonen}

\maketitle
\cleardoublepage

\pagenumbering{roman}
\tableofcontents
\clearpage

\pagenumbering{arabic}
\pagestyle{headings}

%% Here is a basic outline for the document.   (Bill's original outline)

%% 0. Quick start with ``best practices''.  Each step has a reference to
%% more detailed information later in the document.
%% 1. Acquiring and unpacking.  Using a ``fast'' directory location and
%%    VPATH
%% 1a. Reporting problems
%% 2. Choosing a device (defer a detailed discussion of each until later)
%% 3. configure, make, and install.  Always use --prefix 
%% show only basic options for configure
%%    3a. Optional include of device-specific information
%%    3b. Optional include of pm-specific information
%%    3c. Optional ``fast'' version
%%    3d. Shared libraries
%% 4. Testing and benchmarking
%% 4a. make testing
%% 4b. Getting, building, and using mpptest and netpipe
%% 5. Special options
%% 6. Troubleshooting
%% Appendix:
%% A. Summary of configure options (particularly the enable and with options)


\section{Introduction}
\label{sec:intro}
This manual describes how to obtain and install MPICH2, the MPI-2
implementation from Argonne National Laboratory.  (Of course, if you are
reading this, chances are good that you have already obtained it and
found this document, among others, in its \texttt{doc} subdirectory.)
This \emph{Guide} will explain how to install MPICH so that you and others can use it to
run MPI applications.  Some particular features are different
if you have system administration privileges (can become ``root'' on a
Unix system), and these are explained here.  It is not necessary to have
such privileges to build and install MPICH2.  In the event of problems,
send mail to \texttt{mpich2-maint@mcs.anl.gov}.  Once MPICH2 is
installed, details on how to run MPI jobs are covered in the \emph{MPICH2
User's Guide}, found in this same \texttt{doc} subdirectory.

MPICH2 has many options.  We will first go through a recommended,
``standard'' installation in a step-by-step fashion, and later describe
alternative possibilities.  This \emph{Installer's Guide} is for MPICH2
Release 1.0.  We are reserving the 1.0 designation for when every last
feature of the MPI-2 Standard is implemented, but most features are
included.  See the \texttt{RELEASE\_NOTES} file in the top-level
directory for details.


\section{Quick Start}
\label{sec:quick}

In this section we describe a ``default'' set of installation steps.  It
uses the default set of configuration options, which builds the \texttt{sock}
communication device and the \texttt{MPD} process manager, for as many
of languages C, C++, Fortran-77, and Fortran-90 compilers as it can
find, with compilers chosen automatically from the user's environment,
without tracing and debugging options.  It uses the \texttt{VPATH}
feature of \texttt{make}, so that the build process can take place on
a local disk for speed.

\subsection{Prerequisites}
\label{sec:prerequisites}

For the default installation, you will need:
\begin{enumerate}
\item A copy of the distribution, \texttt{mpich2.tar.gz}.
\item A C compiler.
\item A fortran-77, Fortran-90, and/or C++ compiler if you wish to write
  MPI programs in any of these languages.
\item Python 2.2 or later version, for building the default process
  management system, MPD.  In addition, you will need 
  PyXML and an XML parser such as expat (in order
  to use mpiexec with the MPD process manager).  Most systems have
  Python, PyXML, and expat pre-installed, but you can get them free from
  \texttt{www.python.org}.  You may assume they are there unless the
  \texttt{configure} step below complains.
\item Any one of a number of Unix operating systems, such as IA32-Linux.
  MPICH2 is most extensively tested on Linux;  there remain some
  difficulties on systems we do not currently have access to.  Our
  \texttt{configure} script attempts to adapt MPICH2 to new systems. 
\end{enumerate}
Configure will check for these prerequisites and try to work around
deficiencies if possible.  (If you don't have Fortran, you will
still be able to use MPICH2, just not with Fortran applications.)

This default installation procedure builds and installs MPICH2 ready for
C, C++, Fortran 77, and Fortran 90 programs, using the MPD process
manager (and it 
builds and installs MPD itself), without debugging options.  Regardless
of where the source resides, the build takes place on a local file
system, where compilation is likely to be much faster than on a
network-attached file system, but the installation directory that is
accessed by users can be on a shared file system.  For other options,
see the appropriate sections later in the document.

\subsection{From A Standing Start to Running an MPI Program}
\label{sec:steps}
Here are the steps from obtaining MPICH2 through running your own
parallel program on multiple machines.

\begin{enumerate}
\item 
Unpack the tar file.
\begin{verbatim}
    tar xfz mpich2.tar.gz
\end{verbatim}
If your tar doesn't accept the z option, use
\begin{verbatim}
    gunzip -c mpich2.tar.gz | tar xf -
\end{verbatim}
Let us assume that the directory where you do this is
\texttt{/home/you/libraries}.  It will now contain a subdirectory named
\texttt{mpich2-1.0}.

\item
Choose an installation directory (the default is \texttt{/usr/local/bin)}:
\begin{verbatim}
    mkdir /home/you/mpich2-install
\end{verbatim}
It will be most convenient if this directory is shared by all of the
machines where you intend to run processes.  If not, you will have
to duplicate it on the other machines after installation.  Actually, if
you leave out this step, the next step will create the directory for you.

\item
Choose a build directory.  Building will proceed \emph{much} faster if
your build directory is on a file system local to the machine on which
the configuration and compilation steps are executed.  It is preferable
that this also be separate from the source directory, so that the
source directories remain
clean and can be reused to build other copies on other machines.
\begin{verbatim}
    mkdir /tmp/you/mpich2-1.0
\end{verbatim}

\item
Configure MPICH2, specifying the installation directory, and running
the \texttt{configure} script in the source directory:
\begin{verbatim}
    cd  /tmp/you/mpich2-1.0
    /home/you/libraries/mpich2-1.0/configure \
            -prefix=/home/you/mpich2-install |& tee configure.log
\end{verbatim}
where the \texttt{$\backslash$} means that this is really one line.  (On
\texttt{sh} and its derivatives, use \verb+2>&1 | tee configure.log+
instead of \verb+|& tee configure.log+).  Other configure options are
described below.  Check the \texttt{configure.log} file to make sure
everything went well.  Problems should be self-explanatory, but if not,
send \texttt{configure.log} to \texttt{mpich2-maint@mcs.anl.gov}.
The file \texttt{config.log} is created by \texttt{configure} and
contains a record of the tests that \texttt{configure} performed.  It
is normal for some tests recorded in \texttt{config.log} to fail.  

\item
Build MPICH2:
\begin{verbatim}
    make |& tee make.log
\end{verbatim}
This step should succeed if there were no problems with the preceding
step.  Check \texttt{make.log}.  If there were problems, send
\texttt{configure.log} and \texttt{make.log} to
\texttt{mpich2-maint@mcs.anl.gov}.

\item
Install the MPICH2 commands:
\begin{verbatim}
    make install |& tee install.log
\end{verbatim}
This step collects all required executables and scripts in the \texttt{bin}
subdirectory of the directory specified by the prefix argument to
configure. 

\item
Add the \texttt{bin} subdirectory of the installation directory to your path:
\begin{verbatim}
    setenv PATH /home/you/mpich2-install/bin:$PATH
\end{verbatim}
for \texttt{csh} and \texttt{tcsh}, or 
\begin{verbatim}
    export PATH=/home/you/mpich2-install/bin:$PATH
\end{verbatim}
for \texttt{bash} and \texttt{sh}.  Check that everything is in order at
this point by doing
\begin{verbatim}
    which mpd
    which mpicc
    which mpiexec
    which mpirun
\end{verbatim}
All should refer to the commands in the \texttt{bin} subdirectory of your
install directory.  It is at this point that you will need to
duplicate this directory on your other machines if it is not
in a shared file system such as NFS.

\item
MPICH2, unlike MPICH, uses an external process manager for scalable
startup of large MPI jobs.  The default process manager is called
MPD, which is a ring of daemons on the machines where you will run
your MPI programs.  In the next few steps, you will get this ring up
and tested.
The instructions given here will probably be enough to get you started.
If not, you should refer to Appendix~\ref{app:mpd} for troubleshooting help.
More details on interacting with MPD can be found by running
\texttt{mpdhelp} or any mpd command with the \texttt{--help} option, or by
viewing the README file in \texttt{mpich2/src/pm/mpd}.  The information
provided includes how to list running jobs, kill, suspend, or otherwise
signal them, and how to use the \texttt{gdb} debugger via special
arguments to \texttt{mpiexec}.

For security reasons, mpd looks in your home directory for a file named
\texttt{.mpd.conf} containing the line
\begin{verbatim}
    secretword=<secretword>
\end{verbatim}
where \verb+<secretword>+ is a string known only to yourself.  It should
not be your normal Unix password.  Make this file readable and writable
only by you:
\begin{verbatim}
    cd $HOME
    touch .mpd.conf
    chmod 600 .mpd.conf
\end{verbatim}
Then use an editor to place a line like:
\begin{verbatim}
  secretword=mr45-j9z
\end{verbatim}
into the file.
(Of course use a different secret word than \verb+mr45-j9z+.)

\item
The first sanity check consists of bringing up a ring of one mpd on
the local machine, testing one mpd command, and bringing the ``ring''
down. 
\begin{verbatim}
    mpd & 
    mpdtrace
    mpdallexit
\end{verbatim}
The output of mpdtrace should be the hostname of the machine you are
running on.  The \texttt{mpdallexit} causes the mpd daemon to exit.
If you encounter problems,
you should check the troubleshooting section in Appendix~\ref{app:mpd}.

\item
The next sanity check is to run a non-MPI program using the daemon.
\begin{verbatim}
    mpd & 
    mpiexec -n 1 /bin/hostname
    mpdallexit
\end{verbatim}
This should print the name of the machine you are running on.  If not,
you should check the troubleshooting section in Appendix~\ref{app:mpd}.

\item
Now we will bring up a ring of mpd's on a set of machines.  Create
a file consisting of a list of machine names, one per line.  Name this
file \texttt{mpd.hosts}.  These hostnames will be used as targets for
\texttt{ssh} or \texttt{rsh}, so include full domain names if necessary.  Check that you
can reach these machines with \texttt{ssh} or \texttt{rsh} without
entering a password.  You can test by doing
\begin{verbatim}
    ssh othermachine date
\end{verbatim}
or
\begin{verbatim}
    rsh othermachine date
\end{verbatim}
If you cannot get this to work without entering a password, you will
need to configure \texttt{ssh} or \texttt{rsh} so that this can be done,
or else use the workaround for \texttt{mpdboot} in the next step.

\item
Start the daemons on (some of) the hosts in the file mpd.hosts
\begin{verbatim}
  mpdboot -n <number to start> -f mpd.hosts
\end{verbatim}
The number to start can be less than 1 + number of hosts in the
file, but cannot be greater than 1 + the number of hosts in the
file.  One mpd is always started on the machine where \texttt{mpdboot} is
run, and is counted in the number to start, whether or not it occurs
in the file.  By default, \texttt{mpdboot} will only start one mpd per
machine even if the machine name appears in the hosts file multiple times.
The -1 option can be used to override this behavior, but there is typically
no reason for a user to need multiple mpds on a single host.
The -1 option exists mostly to support internal testing.

Check to see if all the hosts you listed in mpd.hosts are in the output
of 
\begin{verbatim}
    mpdtrace
\end{verbatim}
and if so move on to step 12.

There is a workaround if you cannot get \texttt{mpdboot} to work because of
difficulties with \texttt{ssh} or \texttt{rsh} setup.  You can start the daemons ``by
hand'' as follows:
\begin{verbatim}
   mpd &            # starts the local daemon
   mpdtrace -l      # makes the local daemon print its host
                    # and port in the form <host>_<port>
\end{verbatim}
Then log into each of the other machines, put the \texttt{install/bin}
directory in your path, and do:
\begin{verbatim}
   mpd -h <hostname> -p <port> &
\end{verbatim}
where the hostname and port belong to the original mpd that you
started.  From each machine, after starting the mpd, you can do 
\begin{verbatim}
   mpdtrace
\end{verbatim}
to see which machines are in the ring so far.  More details on
\texttt{mpdboot} and other options for starting the mpd's are in
\texttt{mpich2-1.0/src/pm/mpd/README}.

In case of persistent difficulties getting the ring of mpd's up and
running on the machines you want, please see Appendix~\ref{app:mpd}.
There we discuss the mpd's in more detail, together with some programs
for testing the configuration of your systems to make sure that they
allow the mpd's to run.

\item
Test the ring you have just created:
\begin{verbatim}
    mpdtrace
\end{verbatim}
The output should consist of the hosts where MPD daemons are now
running.  You can see how long it takes a message to circle this
ring with 
\begin{verbatim}
    mpdringtest
\end{verbatim}
That was quick.  You can see how long it takes a message to go
around many times by giving mpdringtest an argument:
\begin{verbatim}
    mpdringtest 100
    mpdringtest 1000
\end{verbatim}

\item
Test that the ring can run a multiprocess job:
\begin{verbatim}
    mpdrun -n <number> hostname
\end{verbatim}
The number of processes need not match the number of hosts in the
ring;  if there are more, they will wrap around.  You can see the
effect of this by getting rank labels on the stdout:
\begin{verbatim}
    mpdrun -l -n 30 hostname
\end{verbatim}
You probably didn't have to give the full pathname of the hostname
command because it is in your path.  If not, use the full pathname:
\begin{verbatim}
    mpdrun -l -n 30 /bin/hostname
\end{verbatim}

\item
Now we will run an MPI job, using the \texttt{mpiexec} command as specified
in the MPI-2 standard.  There are some examples in the install
directory, which you have already put in your path, as well as in
the directory \texttt{mpich2-1.0/examples}.  One of them is the classic \texttt{cpi}
example, which computes the value of $\pi$ by numerical integration in
parallel.   
\begin{verbatim}
    mpiexec -n 5 cpi
\end{verbatim}
As with \texttt{mpdrun} (which is used internally by \texttt{mpiexec}), the number of
processes need not match the number of hosts.  The \texttt{cpi} example will
tell you which hosts it is running on.  By default, the processes
are launched one after the other on the hosts in the mpd ring, so it
is not necessary to specify hosts when running a job with \texttt{mpiexec}.

There are many options for \texttt{mpiexec}, by which multiple executables
can be run, hosts can be specified (as long as they are in the mpd
ring), separate command-line arguments and environment variables can
be passed to different processes, and working directories and search
paths for executables can be specified.  Do
\begin{verbatim}
    mpiexec --help
\end{verbatim}
for details. A typical example is:
\begin{verbatim}
    mpiexec -n 1 master : -n 19 slave
\end{verbatim}
or
\begin{verbatim}
    mpiexec -n 1 -host mymachine master : -n 19 slave
\end{verbatim}
to ensure that the process with rank 0 runs on your workstation.

The arguments between `:'s in this syntax are called ``argument sets,''
since they apply to a set of processes.  \textbf{Change this to match
new global and local arguments described in User's Guide.}  There can be
an extra argument set for arguments that apply to all the processes,
which must come first.  For example, to get rank labels on
standard output, use
\begin{verbatim}
    mpiexec -l : -n 3 cpi
\end{verbatim}
This first `:' is optional, since \texttt{mpiexec} knows which are the
global arguments and knows they are first.  So you can also use
\begin{verbatim}
    mpiexec -l : -n 3 cpi
\end{verbatim}
The \texttt{mpirun} command from the original MPICH is still available,
although it does not support as many options as \texttt{mpiexec}.  You might
want to use it in the case where you do not have the XML parser
required for the use of \texttt{mpiexec}.
\end{enumerate}

If you have completed all of the above steps, you have successfully
installed MPICH2 and run an MPI example.  


\subsection{Common Non-Default Configuration Options}
\label{sec:non-default}

\begin{verbatim}
enable-g, enable-fast, devices, pms, etc.
\end{verbatim}
Reference Section~\ref{configure-options}.


\subsection{Shared Libraries}
\label{sec:shared-libraries}

Shared libraries are currently only supported by gcc and tested under
Linux.  To have shared libraries created when MPICH2 is built, specify
the following when MPICH2 is configured:
\begin{verbatim}
    configure --enable-sharedlibs=gcc
\end{verbatim}


\subsection{What to Tell the Users}
\label{sec:telling}

Now that MPICH2 has been installed, the users have to be informed of how
to use it.  Part of this is covered in the \emph{User's Guide}.  Other
things users need to know are covered here.  (E.g., whether they need to
run their own mpd rings or use a system-wide one run by root.)


\section{Migrating from MPICH1}
\label{sec:migrating}

MPICH2 is an all-new rewrite of MPICH1.  Although the basic steps for
installation have remained the same (\texttt{configure}, \texttt{make},
\texttt{make install}), a number of things have changed.  In this
section we attempt to point out what you may be used to in MPICH1 that
are now different in MPICH2.

\subsection{Configure Options}
\label{sec:configure-options}

The arguments to \texttt{configure} are different in MPICH1 and MPICH2;
the \texttt{Installer's Guide} discusses \texttt{configure}.  In
particular, the newer \texttt{configure} in MPICH2 does not support the
\verb+-cc=<compiler-name>+ (or \texttt{-fc}, \texttt{-c++}, or
\texttt{-f90}) options.  Instead, many of the items that could be
specified in the command line to configure in MPICH1 must now be set by
defining an environment variable.  E.g., while MPICH1 allowed
\begin{verbatim}
    ./configure -cc=pgcc
\end{verbatim}
MPICH2 requires
\begin{verbatim}
    setenv CC pgcc
\end{verbatim}
(or \verb+export CC=pgcc+ for \texttt{ksh} or \verb+CC=pgcc ; export CC+
for strict \texttt{sh}) before \texttt{./configure}.  Basically, every
option to the MPICH-1 configure that does not start with
\texttt{--enable} or \texttt{--with} is not available as a configure
option in MPICH2.  Instead, environment variables must be used.  This is
consistent (and required) for use of version 2 GNU \texttt{autoconf}.

\subsection{Other Differences}
Other differences between MPICH1 and MPICH2 include the handling of
process managers and the choice of communication device.

\section{Installing and Managing Process Managers}
\label{sec:process-managers}
MPICH2 has been designed to work with multiple process managers; that
is, although you can start MPICH2 jobs with \texttt{mpiexec}, there are
different mechanisms by which your processes are started.  An interface
(called PMI) isolates the MPICH2 library code from the process manager.
Currently three process managers are distributed with MPICH2
\begin{description}
\item[mpd] This is the default, and the one that is described
  in~Section~\ref{sec:steps}.  It consists of a ring of daemons.
\item[smpd] This one can be used for both Linux and Windows.  It is the
  only process manager that supports the Windows version of MPICH2.
\item[gforker] This is a simple process manager that creates all
  processes on a single machine.  It is useful for both debugging and on
  shared memory multiprocessors.
\end{description}

\subsection{mpd}
\label{sec:mpd}

\subsubsection{Configuring mpd}
\label{sec:configuring-mpd}

The \texttt{mpd} process manager can be explicitly chosen at configure
time by adding
\begin{verbatim}
    --with-pm=mpd
\end{verbatim}
to the \texttt{configure} argments.  This is not necessary, since
\texttt{mpd} is the default.

The \texttt{mpd} process manager supports the use of the TotalView
parallel debugger from Etnus.  If \texttt{totalview} is in your
\texttt{PATH} when MPICH2 is configured, then an interface module will
be automatically compiler, linked and installed so that you can use
TotalView to debug MPICH jobs (See the \texttt{User's Guide} under
``Debugging''.  You can also explicitly enable or disable this
capability with \texttt{--enable-totalview} or
\texttt{--disable-totalview} as arguments to \texttt{configure}.

\subsubsection{Using mpd}
\label{sec:using-mpd}

In Section~\ref{sec:steps} you installed the mpd ring.  Several commands
can be used to use, test, and manage this ring.  You can find out about
them by running \texttt{mpdhelp}, whose output looks like this:

\begin{small}
\begin{verbatim}
The following mpd commands are available.  For usage of any specific one,
invoke it with the single argument --help .

mpd           start an mpd daemon
mpdtrace      show all mpd's in ring
mpdboot       start a ring of daemons all at once
mpdringtest   test how long it takes for a message to circle the ring 
mpdallexit    take down all daemons in ring
mpdcleanup    repair local Unix socket if ring crashed badly
mpdrun        start a parallel job
mpdlistjobs   list processes of jobs (-a or --all: all jobs for all users)
mpdkilljob    kill all processes of a single job
mpdsigjob     deliver a specific signal to the application processes of a job

Each command can be invoked with the --help argument, which prints usage
information for the command without running it.
\end{verbatim}
\end{small}
So for example, to see a complete list of the possible arguments for
\texttt{mpdboot}, you would run
\begin{verbatim}
    mpdboot --help
\end{verbatim}


\subsubsection{Options for \texttt{mpd}}

\begin{description}
\item[-ncpus] is used when allowing MPD to pick the hosts: it tells MPD
  how many processes should be started by each MPD in the ring as the
  processes are started in round-robin fashion.
\end{description}

\subsubsection{Running MPD as Root}
\label{sec:mpd-root}

How to run mpd as root for other people to use.  Test whether all that
is necessary is for root to be the one who runs the install step.

\subsection{SMPD}
\label{sec:smpd}

\subsubsection{Configuration}
\label{sec:smpd_configure}

You may add the following configure options, 
\texttt{--with-pm=smpd --with-pmi=smpd}, 
to build and install the smpd process manager. The process manager, smpd, 
will be installed to the bin sub-directory of the installation directory 
of your choice specified by the \texttt{--prefix} option.

smpd process managers run on each node as stand-alone daemons and need to
be running on all nodes that will participate in MPI jobs.  smpd process 
managers are not connected to each other and rely on a known port to 
communicate with each other.  Note: If you want multiple users to use the 
same nodes they must each configure their smpds to use a unique port per 
user. 

smpd uses a configuration file to store settings.  The default location is 
\verb+~/.smpd+.  This file must not be readable by anyone other than 
the owner and contains at least one required option - the access passphrase.
This is stored in the configuration file as \texttt{phrase=<phrase>}. Access 
to running smpds is authenticated using this passphrase and it must 
not be your user password.

\subsubsection{Usage and administration}
\label{sec:smpd_usage}

Users will start the smpd daemons before launching mpi jobs.  To get an 
smpd running on a node, execute 
\begin{verbatim}
    smpd -s
\end{verbatim}
Executing this for the first time will prompt the user to create a 
\verb+~/.smpd+ configuration file and passphrase if one does not 
already exist.

Then users can use \texttt{mpiexec} to launch MPI jobs.

All options to \texttt{smpd}:

\begin{description}
\item[\texttt{smpd -s}]\mbox{}\\
Start the smpd service/daemon for the current user.  You can add 
\texttt{-p <port>} to specify the port to listen on.  All smpds must use
the same port and if you don't use the default then you will have to
add \texttt{-p <port>} to mpiexec or add the \texttt{port=<port>} to the 
\texttt{.smpd} configuration file.

\item[\texttt{smpd -r}]\mbox{}\\
Start the smpd service/daemon in root/multi-user mode.  This is not yet
implemented.

\item[\texttt{smpd -shutdown [host]}]\mbox{}\\
Shutdown the smpd on the local host or specified host.  Warning: this will
cause the smpd to exit and no mpiexec or smpd commands can be issued to the
host until smpd is started again.

%\item[\texttt{smpd -start}]\mbox{}\\
%Start the Windows smpd service on the local host.
%
%\item[\texttt{smpd -stop}]\mbox{}\\
%Stop the Windows smpd on the local host.
%
%\item[\texttt{smpd -console [host]}]\mbox{}\\
%Connect to a specific smpd to issue console commands.  The currently 
%supported commands are: %get, set, delete, stat, status, shutdown and validate.
%\begin{enumerate}
%\item[\texttt{get <var>}]\mbox{}
%\item[\texttt{set <var>=<value>}]\mbox{}
%\item[\texttt{delete <var>}]\mbox{}
%\item[\texttt{status}]\mbox{}
%\item[\texttt{stat <var>}]\mbox{}
%\item[\texttt{shutdown}]\mbox{}
%\item[\texttt{validate}]\mbox{}
%\end{enumerate}

\end{description}

\subsection{gforker}
\label{sec:forker}
\texttt{gforker} is a simple process manager that runs all processes
on a single node; it uses the system \texttt{fork} and \texttt{exec}
calls to create the new processes.  

\section{Testing}
\label{sec:testing}
Running basic tests in the examples directory, the MPICH2 tests,
obtaining and running the assorted test suites.  


\subsection{Using the Intel Test Suite}
\label{sec:intel}

These instructions are local to our test environment at Argonne.

How to run a select set of tests from the Intel test suite:

\begin{small}
\begin{verbatim}
1) checkout the Intel test suite (cvs co IntelMPITEST) (outside users
   should access the most recent version of the test suite from the
   test suite web page).

2) create a testing directory separate from the IntelMPITEST source
directory

3) cd into that testing directory

4) make sure the process manager (e.g., mpd) is running

5) run "<ITS_SRC_DIR>/configure --with-mpich2=<MPICH2_INSTALL_DIR>", where
<ITS_SRC_DIR> is the path to the directory Intel test suite source (e.g.,
/home/toonen/Projects/MPI-Tests/IntelMPITEST) and <MPICH2_INSTALL_DIR> is
the directory containing your MPICH2 installation

6) mkdir Test; cd Test

7) find tests in <ITS_SRC_DIR>/{c,fortran} that you are interested in
running and place the test names in a file.  For example:

% ( cd /home/toonen/Projects/MPI-Tests/IntelMPITEST/Test ; \
    find {c,fortran} -name 'node.*' -print | grep 'MPI_Test' 
    | sed -e 's-/node\..*$--' ) |& tee testlist
Test/c/nonblocking/functional/MPI_Test
Test/c/nonblocking/functional/MPI_Testall
Test/c/nonblocking/functional/MPI_Testany
Test/c/nonblocking/functional/MPI_Testsome
Test/c/persist_request/functional/MPI_Test_p
Test/c/persist_request/functional/MPI_Testall_p
Test/c/persist_request/functional/MPI_Testany_p
Test/c/persist_request/functional/MPI_Testsome_p
Test/c/probe_cancel/functional/MPI_Test_cancelled_false
Test/fortran/nonblocking/functional/MPI_Test
Test/fortran/nonblocking/functional/MPI_Testall
Test/fortran/nonblocking/functional/MPI_Testany
Test/fortran/nonblocking/functional/MPI_Testsome
Test/fortran/persist_request/functional/MPI_Test_p
Test/fortran/persist_request/functional/MPI_Testall_p
Test/fortran/persist_request/functional/MPI_Testany_p
Test/fortran/persist_request/functional/MPI_Testsome_p
Test/fortran/probe_cancel/functional/MPI_Test_cancelled_false
%

8) run the tests using ../bin/mtest:

% ../bin/mtest -testlist testlist -np 6 |& tee mtest.log
%

NOTE: some programs hang if less they are run with less than 6 processes.

9) examine the summary.xml file.  look for '<STATUS>fail</STATUS>' to see if
any failures occurred.  (search for '>fail<' works as well)

\end{verbatim}
\end{small}

\section{Benchmarking}
\label{sec:benchmarking}
netpipe, mpptest, others (SkaMPI).

\section{MPE}
\label{sec:mpe}

This section describes what MPE is and its potentially separate installation.  It
includes discussion of Java-related problems.

\section{Windows Version}
\label{sec:windows}

\subsection{Binary distribution}
\label{sec:winbin}

The Windows binary distribution uses the Microsoft Installer.  Download and 
execute mpich2-1.x.xxx.msi to install the binary distribution.  The default 
installation path is \texttt{C:$\backslash$Program Files$\backslash$MPICH2}. 
You must have administrator privileges to install mpich2.msi.  The installer 
installs a Windows service to launch MPICH applications and only administrators
may install services.  This process manager is called smpd.exe.  Access to 
the process manager is passphrase protected.  The installer asks for this 
passphrase.  Do not use your user password.  The same passphrase must be 
installed on all nodes that will participate in a single MPI job.

Under the installation directory are three sub-directories: \texttt{include},
 \texttt{bin}, and \texttt{lib}.  The \texttt{include} and \texttt{lib} 
directories contain the header files and libraries necessary to compile MPI 
applications.  The \texttt{bin} directory contains the process manager, 
\texttt{smpd.exe}, and the the MPI job launcher, \texttt{mpiexec.exe}.  The
dlls that implement MPICH2 are copied to the Windows system32 directory.

You can install MPICH in unattended mode by executing 
\begin{verbatim}
    msiexec /q /I mpich2-1.x.xxx.msi
\end{verbatim}

The smpd process manager for Windows runs as a service that can launch jobs 
for multiple users.  It does not need to be started like the unix version 
does.  The service is automatically started when it is installed and when 
the machine reboots.  smpd for Windows has additional options:
\begin{description}
\item[\texttt{smpd -start}]\mbox{}\\
Start the Windows \texttt{smpd} service.
\item[\texttt{smpd -stop}]\mbox{}\\
Stop the Windows \texttt{smpd} service.
\item[\texttt{smpd -install}]\mbox{}\\
Install the \texttt{smpd} service.
\item[\texttt{smpd -remove}]\mbox{}\\
Remove the \texttt{smpd} service.
\item[\texttt{smpd -register\_spn}]\mbox{}\\
Register the Service Principal Name with the domain controller.
This command enables passwordless authentication using kerberos.  It must be 
run on each node individualy by a domain administrator.
\end{description}

\subsection{Source distribution}
\label{sec:winsrc}

In order to build MPICH2 from the source distribution under Windows, 
you must have MS Developer Studio .NET 2003 or later, perl and optionally 
Intel Fortran 8 or later.

\begin{itemize}
\item
Download \texttt{mpich2-1.x.tar.gz} and unzip it.
\item
Bring up a Visual Studio Command prompt with the compiler environment 
variables set.
\item
Run \texttt{winconfigure.wsf}. If you don't have a Fortran compiler add the 
\texttt{--remove-fortran} option to \texttt{winconfigure} to remove all the Fortran 
projects and dependencies.  Execute \texttt{winconfigure.wsf /?} to see all
available options.
\item 
    open \texttt{mpich2$\backslash$mpich2.sln}
\item
    build the \texttt{ch3sockDebug mpich2} solution
\item
    build the \texttt{ch3sockDebug mpich2s} project
\item
    build the ch3sockRelease mpich2 solution
\item
    build the ch3sockRelease mpich2s project
\item
    build the Debug mpich2 solution
\item
    build the Release mpich2 solution
\item
    build the fortDebug mpich2 solution
\item
    build the fortRelease mpich2 solution
\item
    build the gfortDebug mpich2 solution
\item
    build the gfortRelease mpich2 solution
\item
    build the sfortDebug mpich2 solution
\item
    build the sfortRelease mpich2 solution
\item
    build the channel of your choice.  The options are \texttt{shm}, \texttt{ssm}, \texttt{sshm}, \texttt{ib}, \texttt{essm}, and \texttt{mt}.  
The shm channel is for small numbers of processes that will run on a single 
machine using shared memory.  The shm channel should not be used for more 
than about 8 processes.  The sshm (scalable shared memory) is for use with 
more than 8 processes.  The \texttt{ssm} (sock shared memory) channel is for clusters 
of smp nodes.  This channel should not be used if you plan to over-subscribe 
the CPU's.  If you plan on launching more processes than you have processors 
you should use the default \texttt{sock} channel or the \texttt{essm} channel.  The \texttt{ssm} channel 
uses a polling progress engine that can perform poorly when multiple processes 
compete for individual processors.  The \texttt{essm} channel is derived from the ssm channel
with the addition of OS event objects to avoid spinning in the progress engine.  The \texttt{mt
channel} is the multi-threaded socket channel.  The \texttt{ib} channel is for clusters with Infiniband
interconnects from Mellanox.

\end{itemize}

\subsection{\texttt{cygwin}}
\label{sec:cygwin}

MPICH2 can also be built under \texttt{cygwin} using the source
distribution and the unix commands described in previous sections.  This
will not build the same libraries as described in this section.  It will 
build a ``unix'' distribution that runs under \texttt{cygwin}.

\section{All Configure Options}
\label{configure-options}
Here is a list of all the configure options currently recognized by the
top-level configure.  It is the MPICH-specific part of the output of 
\begin{verbatim}
    configure --help
\end{verbatim}
Not all of these options may be fully supported yet.  Explain all of
them \ldots\marginpar{The text includes the output of configure from
  some time ago.  It may be preferable to either force the Makefile to
  get an up-to-date version or even better to provide something more
  informative that explains the options in more depth}

\begin{small}
\begin{verbatim}
--enable and --with options recognized:
--enable-cache  - Turn on configure caching
--enable-echo  - Turn on strong echoing. The default is enable=no. 
--enable-strict - Turn on strict debugging with gcc
--enable-coverage - Turn on coverage analysis using gcc and gcov
--enable-error-checking=level - Control the amount of error checking.  
level may be 
    no        - no error checking
    runtime   - error checking controllable at runtime through environment 
                variables
    all       - error checking always enabled
--enable-error-messages=level - Control the amount of detail in error 
  messages.  Level may be
    all       - Maximum amount of information
    generic   - Only generic messages (no information about the specific
                instance)
    class     - One message per MPI error class
    none      - No messages
--enable-timing=level - Control the amount of timing information 
collected by the MPICH implementation.  level may be
    none    - Collect no data
    all     - Collect lots of data
    runtime - Runtime control of data collected
The default is none.
--enable-threads=level - Control the level of thread support in the 
MPICH implementation.  The following levels are supported.
    single - No threads (MPI_THREAD_SINGLE)
    funneled - Only the main thread calls MPI (MPI_THREAD_FUNNELED)
    serialized - User serializes calls to MPI (MPI_THREAD_SERIALIZED)
    multiple[:impl] - Fully multi-threaded (MPI_THREAD_MULTIPLE)
The default is funneled.  If enabled and no level is specified, the
level is set to multiple.  If disabled, the level is set to single.
When the level is set to multiple, an implementation may also be
specified.  The following implementations are supported.
    global_mutex - a single global lock guards access to all MPI functions.
    global_monitor - a single monitor guards access to all MPI functions.
The default implementation is global_mutex.
--enable-g=option - Control the level of debugging support in the MPICH
implementation.  option may be a list of common separated names including
    none     - No debugging
    handle   - Trace handle operations
    dbg      - Add compiler -g flags
    meminit  - Preinitialize memory associated structures and unions to
               eliminate access warnings from programs like valgrind
    all      - All of the above choices
--enable-fast - pick the appropriate options for fast execution.  This
                turns off error checking and timing collection
--enable-f77 - Enable Fortran 77 bindings
--enable-f90 - Enable Fortran 90 bindings
--enable-cxx - Enable C++ bindings
--enable-romio - Enable ROMIO MPI I/O implementation
--enable-nmpi-as-mpi - Use MPI rather than PMPI routines for MPI routines,
 such as the collectives, that may be implemented in terms of other MPI 
 routines
--with-device=name - Specify the communication device for MPICH.
--with-pmi=name - Specify the pmi interface for MPICH.
--with-pm=name - Specify the process manager for MPICH.
      Multiple process managers may be specified as long as they all use
      the same pmi interface by separating them with colons.  The 
      mpiexec for the first named process manager will be installed.
      Example: --with-pm=gforker:mpd builds the two process 
      managers gforker and mpd; only the mpiexec from gforker
      is installed into the bin directory.
--with-thread-package=package - Thread package to use.  Supported thread
packages include:
    posix or pthreads - POSIX threads
    solaris - Solaris threads (Solaris OS only)
The default package is posix.
--with-logging=name - Specify the logging library for MPICH.
--with-mpe - Build the MPE (MPI Parallel Environment) routines
--enable-weak-symbols - Use weak symbols to implement PMPI routines (default)
--with-htmldir=dir - Specify the directory for html documentation
--with-docdir=dir - Specify the directory for documentation
--with-cross=file - Specify the values of variables that configure cannot
determine in a cross-compilation environment
--with-flavor=name - Set the name to associate with this flavor of MPICH
--with-namepublisher=name - Choose the system that will support 
                             MPI_PUBLISH_NAME and MPI_LOOKUP_NAME.  Options
                             include
                               no (no service available)
                               pmiext  (service using a pmi extension,
                                        usually only within the same MPD ring)
                               file:directory
                               ldap:ldapservername
                             Only no and file have been implemented so far.
--enable-sharedlibs=kind - Enable shared libraries.  kind may be
    gcc     - Standard gcc and GNU ld options for creating shared libraries
    libtool - GNU libtool 
    none    - same as --disable-sharedlibs
Only gcc is currently supported

--enable-dependencies - Generate dependencies for sourcefiles.  This
            requires that the Makefile.in files are also created
            to support dependencies (see maint/updatefiles)
\end{verbatim}
\end{small}

\appendix


\section{Troubleshooting the mpd's}
\label{app:mpd}



\subsection{Getting Started with mpd}
\label{sec:getting-started}

\texttt{mpd} stands for multi-purpose daemon.  We sometimes use the term
mpd to refer to the combination of mpd daemon and its helper programs that
collectively form a process management system for executing
parallel jobs, including mpich jobs.  The mpd daemon must
run on each host where you wish to execute parallel programs.
The mpd daemons form a ring to facilitate rapid startup.
Therefore, each host must be configured in such a way that the
mpd's can connect to each other and pass messages via sockets.
Sometimes this configuration can be a bit tricky to get right.
In this section, we will walk slowly through a series of steps
that will help to ensure success in running mpd's on a large cluster.

\begin{enumerate}
\item Install mpich2, and thus mpd.

\item Make sure the mpich2 bin directory is in your path.
Below, we will refer to it as \texttt{MPDDIR}.

\item At a shell prompt (\$ below), type:
\begin{verbatim}
    $ mpd &
\end{verbatim}
If the executable is not found, make sure that you have
\texttt{MPDDIR} in your path and that mpd is in that dir.

If you get something like:
\begin{small}
\begin{verbatim}
    configuration file /home/you/.mpd.conf not found
    A file named .mpd.conf must be present in the users home
    directory (/etc/mpd.conf if root) with read and write access
    only for the user, and must contain at least a line with:
    secretword=<secretword>
\end{verbatim}
\end{small}
then create a file named \texttt{.mpd.conf} (note the leading dot) in your
home directory using an editor.  That file must contain a line
\begin{verbatim}
    secretword=<secretword>
\end{verbatim}
where you replace \verb+<secretword>+ with some string
you like.  Then make sure the file has mode 600 (\texttt{chmod 600 .mpd.conf}).

Then, try mpd again.

% $   Comment to end fake math mode

If mpd still fails to start, you may have a configuration problem that
\texttt{mpdcheck} will be able to help you with (see
Section~\ref{sec:mpdcheck}).

To stop the mpd:
\begin{verbatim}
   $ mpdallexit
\end{verbatim}


\item Now, run mpdtrace:
\begin{verbatim}
    $ mpdtrace -l
\end{verbatim}

This should print something like:
\begin{verbatim}
    yourhost_1234
\end{verbatim}
where the 1234 is a port number that mpd is using to listen for
connections from other mpds.

\item Try to run a parallel (non-MPI) job:
\begin{verbatim}
    $ mpiexec -n 2 hostname    
\end{verbatim}
We assume \texttt{}hostname is in your path.

\item Try to run a parallel mpi job:
\begin{verbatim}
  $ mpiexec -n 2 /MPI_EXAMPLES_DIR/cpi    
\end{verbatim}

\end{enumerate}


\subsection{mpdcheck}
\label{sec:mpdcheck}

\texttt{mpdcheck} is a program that tries to verify that your set
of machines is configured properly to support a ring of mpd's.  So,
before trying to start a whole set of mpd's on a collection of machines
with \texttt{mpdboot}, we have found that it is typically a \emph{very}
good idea to run a few tests with \texttt{mpdcheck} that help to ensure
later success.

If you are troubleshooting, it is best to select two machines (call them
m1 and m2) for which you are having problems connecting into an mpd ring,
and use the pair to do debugging with mpdcheck.  Sometimes we find that
folks get a pair of machines working correctly, and then have trouble
bringing a third one into the ring.  In that case, it is generally best
to try this troubleshooting section with the new machine and just one
of the original ones.

There are cluster configurations that can be a bit tricky to get right.
For example, if you have a cluster which has a \texttt{head node} that
is special because it has two interfaces (and thus two IP addresses),
the head node must be configured such that mpd's running on it and
the \texttt{internal} nodes can form a ring.  Among other things, this
implies that processes running on the internal nodes will need to be
able to identify (via DNS, /etc/hosts, etc.) and reach the head node.
While this seems obvious, it requires some care.  If the head node
generally identifies itself by its \texttt{external} address, that can
cause confusion when a process runs there, but wishes to identify its
location to a process running on an internal node.

Currently \texttt{mpdcheck} does not try to identify all possible methods
by which mpd may be made to work on a pair of machines.  It simply tries
to see if the configuration is such that a simple client-server pair
of processes can connect and communicate.  Thus, we recommend below
that when running the client-server trials below (-s and -c options),
that you run them ``in both directions'', i.e. first run the server on m1
and the client on m2; then, run the server on m2 and the client on m1.

\begin{enumerate}
\item \verb+ $ mpdcheck -h +

This prints a fairly long help message but gives you some
idea about what mpdcheck might do for you and an
explanation of some of the runs we will do here.

\item \verb+ $ mpdcheck +

With no args, mpdcheck tries to determine if there seem to
be configuration problems with the current machine that
would cause issues when mpd's on multiple hosts try to
connect and communicate.  The goal here is to get no
output.  Output indicates potential problems.  If you find
the messages too short/cryptic, you can use the \texttt{-l} option
(long messages) and get better info.  You will probably want to run
this once on each of the pair of machines which you are debugging.

\item Create an \texttt{mpd.hosts} file using an editor.  For now, just
    list the local machine's name on one line and the name of
    another machine in your cluster on the next line.  (Later,
    we will list all machines in the cluster.)  Now, try:
\begin{verbatim}
    $ mpdcheck -f mpd.hosts
\end{verbatim}
    If this produces no error output, try:
\begin{verbatim}
    $ mpdcheck -f mpd.hosts -ssh
\end{verbatim}
These tests will try to verify that the localhost can
discover the other host, and with the \texttt{-ssh} option try to
run some \texttt{ssh} tests between the 2 machines to make sure
that those kinds of things work also.

\item Next, you should try running
the previous test from the second machine in the file.

\item If all above went fine above, you can probably skip this step.
This step was attempted automatically in the previous one.  But,
if there were problems, you may find it useful to rerun by hand and
keep the output.  On your two machines in the \texttt{mpd.hosts}
file (call them m1 and m2), try the following:

Do this on m1 and read the output for host and port:
\begin{verbatim}
    $ mpdcheck -s
\end{verbatim}
Do this on m2:
\begin{verbatim}
    $ mpdcheck -c host port
\end{verbatim}
where you use the host and port printed by \texttt{mpdcheck} on m1.
Then, try running this client-server test from m2.

\item Try running a pair of mpd's on the two machines.  First, on both
  machines:
\begin{verbatim}
    $ mpdallexit
\end{verbatim}
just to make sure you have no old mpd's running.

Run mpd on m1 and use the -e option to cause mpd to echo
the port it is using:
\begin{verbatim}
    $ mpd -e &        
\end{verbatim}
    Then, run mpd on m2 and cause it to enter the ring at m1:
\begin{verbatim}
    $ mpd -h m1 -p the_echoed_port_at_m1 &
    $ mpdtrace
\end{verbatim}
The \texttt{mpdtrace} should show both mpds in the ring.  If so,
you should be able to run a parallel job:
\begin{verbatim}
    $ mpiexec -n 2 hostname        
\end{verbatim}
and see both hostnames printed.

\item \verb+ $ mpdcheck -pc +
With the -pc arg, mpdcheck prints configuration files and other
info on the current machine.  This output, along with some of the previous
ones mentioned, may be useful info to provide if you are planning to request
debugging help.

\end{enumerate}


\subsection{mpdboot}
\label{sec:mpdboot}

You are now ready to try to use \texttt{mpdboot} to start mpd's on a
set of machines.  To keep it simple, try using just the two
hosts listed in your existing \texttt{mpd.hosts} file from above.

\begin{enumerate}

\item  \verb+$ mpdallexit +
Next, boot a single mpd on the local machine.
\begin{verbatim}
    $ mpdboot -n 1
    $ mpdtrace -l
\end{verbatim}

\item \verb+ $ mpdallexit +
\begin{verbatim}
    $ mpdboot -n 2    
\end{verbatim}
See if mpd's are running on both machines.
\begin{verbatim}
    $ mpdtrace -l
    $ mpiexec -n 2 hostname
\end{verbatim}

\end{enumerate}

If \texttt{mpdboot} works on the two machines, it will probably work on
all of them.  But, there could be configuration problems on one machine,
for example.  An easy way to check, is to gradually add them to
\texttt{mpd.hosts} and try an \texttt{mpdboot} with a \texttt{-n} arg
that uses them all each time.

\texttt{mpdboot} can be used to start mpd's on machines that have multiple
interfaces.  Each mpd needs to be able to identify the interface that it uses
to communicate with other mpd's in the ring.  We do not actually use interface
names (e.g. eth0), but use hostnames or actual IP addresses associated with
the address.  We refer to this information as the \texttt{ifhn}, the
interface-hostname.  To specify the ifhn on the local machine, i.e. for the
local mpd, use the \texttt{--ifhn} arg:
\begin{verbatim}
    $ mpdboot --ifhn=192.168.1.1 -n 2
\end{verbatim}
This example uses the IP address associated with an alternative interface.
Similarly you can specify alternative interfaces for machines listed in an
mpd.hosts file.  For example:
\begin{verbatim}
    frontend:1 ifhn=192.168.1.1
    backend1:2
\end{verbatim}
This file indicates that the frontend machine in the cluster has a single 
cpu and uses its itnerface with IP address to communicate with nodes in the
cluster.  Host backend1 has 2 cpus and uses its default (or only) interface
for communication in the cluster..

\end{document}

%
% Comments on subclassing the document
% We can use \ifdevname ... \fi and \ifpmname ... \fi, as in
% \ifdevchiii .. \fi and \ifpmmpd ... \fi
% (these will still need to be defined)
% There should also be a way to select ``all'' in such a way that the
% document can still flow well, such as
% \ifdevall ... \else \ifdevchiii \else \ifdevmm \fi \fi \fi




