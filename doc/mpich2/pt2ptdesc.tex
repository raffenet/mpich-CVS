\newif\ifcodefirst
%\codefirsttrue
\codefirstfalse

\section{Scenarios}
\label{sec:pt-2-pt-scenarios}

To best understand how the point-to-point communication routines work, we will
describe several scenarios that illustrate how various communication methods
may implement communication.  We start with one of the more complex cases and
then discuss optimizations for special cases such as sending and receiving
contiguous messages.

\subsection{Nonblocking Send and Receive with Complex Datatypes.}
This represents one of the more complex cases; most communication will offer
some opportunities for exploiting special cases such as simple datatypes
representing contiguous data or blocking communication.  

Question: this discussion assumes that only contiguous data can be handled.
We may want to consider the more general \code{iovec} (array of structures
contain a pointer and length in bytes for each member).  However, this is not
an efficient way to handle common vector datatypes or fixed blocksize hindexed
datatypes.  How important is it to handle the case where a modest-sized
\code{iovec} array can be used?  

(Still to do: modify code samples to allow for the short iovec case.)

The datatype is assumed to be complex enough that the device cannot handle it
directly; for example, it may be an hindexed type with a large number (e.g.,
10000) of entries, or a simple resized struct datatype and a large count.
The code for this scenario may look something like this:
\begin{verbatim}
req = MPI_REQUEST_NULL;
if (rank == dest) {
    MPI_Irecv( buffer, count1, datatype1, tag, source, comm, &req );
}
else if (rank == source) {
    MPI_Isend( buffer, count2, datatype2, tag, dest, comm, &req );
}
MPI_Wait( &req, &status );
\end{verbatim}

% In the following, a \emph{segment} is a description of part of the data, for
% example, the first 32k bytes.  A \emph{stream} is communication made up of
% segments.

%   Question: there is no explicit stream here; that is, the stream handling is
%   done through calls to \mpidfunc{MPID_Rhcv} and the way in which the
%   communication agent responds to those messages.  Does it make sense to have
%   an explicit notion of a stream here?  Note that we do want something more
%   that we have here for some of the collective routines such as
%   \mpifunc{MPI_Bcast} (resend data before unpacking) and
%   \mpifunc{MPI_Allreduce} (operate on data before resending).

%\subsubsection{Implementation of Point-to-Point}
%\label{sec:pt2pt-implementation}
%This section is a placeholder for pseudocode for the various routines.  They
%are organized by method.

The following sample implementation assumes a homogeneous system (all
processes use the same data representation).  

\subsubsection{\tcpname.}
This method is a prototype for a TCP method.  

The basic communication is provided by
\begin{enumerate}
\item Buffered read and write.  These are roughly like the Unix \code{fread}
  and \code{fwrite}.  Just as for those routines, using buffered routines can
  reduce the number of system calls and data motion.  One major difference is
  that these are nonblocking in the MPI sense.  That is, the buffered read
  returns a pointer into the internal buffer, and the buffered write accepts
  pointers without making a temporary copy.  The write varient sets an integer
  flag when the data is written.

  Note that this functionality is provided (I believe) by the current active
  queue code.

\item Stream read and write.  These provide a way to receive and send data that
  may not be in contiguous locations.  Roughly, a stream used for
  sending data:
    \begin{enumerate}
    \item packs some number of bytes into a contiguous
      buffer\label{stream:step1} 
    \item writes those bytes to a socket
    \item when all of those bytes are written, returns to
      step~\ref{stream:step1} to 
    get the next group of bytes
    \item when all bytes are written, a completion routine is called
    that may, for example, free the buffer used to pack data.
    \end{enumerate}

    All of this happens without any further action by the upper levels
    of the code.  A special case of a stream is one in which the data
    is contiguous; in this case, no intermediate copy of the data is
    made.

    A stream is initialized by providing a \code{segment}.  A
    \code{segment} is used to handle packing and unpacking of general 
    datatypes, and is initialized with the usual MPI tuple of
    \code{buffer} address, \code{count}, and \code{datatype}.  The
    actual \code{segment} data structure is always part of a larger 
    structure, such as an \mpidconst{MPID_Request}.  In addition, 
    a local buffer is allocated (if necessary) to be used for packing
    or unpacking.  (In other methods, such as the \shmemname, this
    memory may be special, such as memory shared between processes or
    registered to a network driver.)  

    A stream is sent by specifying a destination process.  All data, including
    streams and data sent as a single contiguous buffer, 
    starts with a \mpidconst{MPID_Hid_data} packet that contains a
    matching request number.

    A stream used for reading data is similar, with data read into a
    contiguous buffer and then unpacked instead.  In addition, and bytes in
    the read buffer are used first; then one or more calls to \code{read} are
    used for the remainder of the data.  A further refinement is to allow the
    use of \code{readv} once the read buffer is flushed.  

    To implement these stream operations, the current active queue code may
    need to be modified.

    Question: We could use \code{MSG_PEEK} and \code{recv} instead of
    \code{read} to see how much data is available without reading it.  Can we
    use this, or is it better to just read some of the data (one system call)?
    Rob tells us that there is a way to get the number of bytes in the buffer.


  Question: Where is the buffer to pack into allocated?  Is this part of the 
  \mpidconst{MPID_Segment} structure?  Part of the \mpidconst{MPID_Stream}
  structure? How do we make sure that the choice is
  good for this particular method (e.g., are segments allocated with a source
  or destination rank and a group/communicator)?  Note that the ADI-3 provides
  a \mpidfunc{MPID_Segment_init_pack} (and an unpack) that takes the
  communicator and rank for the destination.  In a multi-method device, each
  device must provide this function (unless it is happy with generic memory).

  Note that it is very useful to know if a datatype represents contiguous
  data; a \mpids{MPI_Datatype}{is_contig} flag value may be helpful.  Note
  that in the case of resized datatypes, a type that is contiguous with a
  \code{count} of $1$ may not be contiguous with a \code{count} greater than
  one (e.g., when the MPI UB (upperbound) is past the last data byte).  Do we
  want a separate flag for that?  Or should we optimize only for 
  the case of a datatype that is contiguous regardless of \code{count}?

    A refinement that is not discussed in detail here is to allow for
    \emph{double 
    buffering}; in the case of a stream used for sending, the next
    group of bytes to be sent is packed before the first is entirely
    sent, overlapping the communication with the packing operation.  In fact,
    this optimization can be implemented within the stream operations that are
    discussed below without changing any of the sample code here.

  The stream routines look roughly like:
\begin{description}
\item[\mpidfunc{MPID_Stream_send_init}]Initialize the
  \mpids{MPID_Request}{stream} structure (usually within a
  \mpidconst{MPID_Request} for sending.
\item[\mpidfunc{MPID_Stream_recv_init}]Likewise, for receiving.
\item[\mpidfunc{MPID_Stream_isend_tcp}]Send a stream without waiting for
  (local) completion.  The arguments for this function are a \code{rank} and
  \code{communicator}, indicating the destination process, a packet type and 
  \code{iovec} (similar to \mpidfunc{MPID_Rhcv}), a previously initialized
  \code{stream}, and two arguments that describe what to do when the operation
  completes (locally).
  When all of the data has been sent, invoke the
  specified completion routine for the stream.  The \code{NULL} function has a
  special meaning and is used as a short-hand for ``set a location (flag) to
  one''.  The \code{_tcp} suffix indicates that this is the TCP-specific
  routine.  
\item[\mpidfunc{MPID_Stream_irecv_tcp}]Likewise, for receiving a stream.
\item[\mpidfunc{MPID_Stream_discard_tcp}]Like
  \mpidfunc{MPID_Stream_irecv_tcp}, but the data is discarded.  This is used,
  for example, for unexpected \mpifunc{MPI_Rsend}s.
\end{description}
  Note that there is no separate ``wait'' or ``test'' routine for a stream.
  This is different from what is currently in the ADI3 document, and reflects
  the need for this interface to support nonblocking operations (the stream
  discussion in the ADI3 document has been optimized for implementation of
  collective operations, all of which (in MPI-1) are blocking).
    
\item Direct read and write.  These are roughly like the Unix
  \code{read} and \code{write}.  They are not used directly, but are a special
  case of the stream read and write.

\item Socket agent.  This can be thought of as a \code{select}
    or \code{poll} loop that advances the communication.  In the case
    of buffered write, for example, it sends more data whenever the
    socket becomes available for writing.

    ``Whenever'' should be interpreted loosely.  If the socket
    agent is running in a separate thread, this may happen when the
    thread is scheduled.  If a polling implementation is used, this
    will happen when the agent is called.

    Question: Can we use \code{poll} always for Unix?  Should we?  If not, how
    do we want to choose between \code{poll} and \code{select}?
    Note that the reason for using \code{poll} is that newer versions of Unix
    allow you to configure the number of fd's to be larger than that supported
    by \code{FD_SET}, rendering \code{select} nearly useless.

\end{enumerate}

These are the low-level communication services that are used to
implemement a TCP version of a two-sided communication method.
These can be implemented with an ``active queue'' system.  In this
approach, each I/O operation adds an element to a FIFO queue.  Each
file descriptor has separate read and write queues.  As operations
complete, the queue element is removed.  Special operations, such as
streams, stay in their position in the queue until all data has been
transfered.  This allows us to use the underlying flow control in TCP,
rather than adding an extra layer of our own, when moving large
amounts of data.

When a low-level communication queue element is removed, a special handler is
invoked.  The 
most common handler is the do-nothing handler (actually a \code{NULL}, so no
function call is made).  The second most common
sets a flag to one; this is normally used to set the
\code{request_ptr->xfer_completed} flag.  For efficiency, this could be a
special case of the \code{NULL} handler with a non-null argument.
For a stream involving a noncontiguous datatype, it could call a true
routine to free up any temporary buffers (this is, in fact, part of the
interface proposed here).

Errors must be handled gracefully.  If at all possible, it should be possible
to continue beyond an error.  For example, in the TCP case, if a socket closes
(e.g., due to congestion or a timeout), the lowest-level code should try to
reestablish the connection, without generating an error event.  (It may wish
to record the fact that the socket closed, particularly in a
performance-diagnostic mode.)  Only if a connection cannot be reestablished
within a reasonable time (defined by a runtime parameter) should an error be
propagated up to the MPI levels.  The procedure for passing an error will be
discussed later.

\implementation{{General Notes}}
The basic structures include the following:
\begin{verbatim}
typedef struct { 
     volatile void *ptr;
     void *tempptr;
     MPID_Msg_format msg_format;   /* Only for heterogeneous systems */
     } MPID_Eager_info;   /* Info for eager delivery */
typedef struct { 
     int sender_id;
     int receiver_id;
} MPID_Rndv_info;         /* Info for rendezvous delivery */
typedef struct {
     void          *ptr;
     int           count;
     MPID_Datatype *dtype;
     MPID_Comm     *comm_ptr;
} MPID_Buf_info;          /* Info describing the user's buffer */

#define MPID_REQ_REGULAR_MASK 0x1
#define MPID_REQ_PERSIST_MASK 0x2
#define MPID_REQ_USER_MASK    0x4
typedef enum { MPID_REQ_ISEND=1, MPID_REQ_IRECV=3, MPID_REQ_PERSIST_ISEND=0,
               MPID_REQ_PERSIST_IRECV=2, MPID_REQ_USER=4 } MPID_Request_kind;

/* All packet types start with the same values */
typedef struct {
  int16_t kind;
  int16_t len;
} MPID_Hid_general_t;

typedef union {
  MPID_Hid_general_t         general;
  MPID_Hid_data_t            data;
  MPID_Hid_eager_t           eager;
  MPID_Hid_request_to_send_t rtosend;
  ... many other types ...
} MPID_Packet;

typedef struct _MPID_Stream {
  /* Remember the source (send) or destination (recv) buffer */
  void             *user_buffer;
  /* Describe the (count,datatype) */
  MPID_Dataloop    dataloop[MAX_DATALOOP];
  int              count, cur_sp;
  /* Used to hold contig form of data */
  void             *tmp_buf;  
  /* Completion routine */
  void (*completion_fn)( struct _MPID_Stream *, void * );
  /* and argument */
  void *completion_arg;
  ... other stuff ...
} MPID_Stream;

typedef struct {
     MPID_Request_kind kind;
     volatile int      busy;
     volatile int      xfer_completed;
     int               self;    /* integer id of request */
     int               context_id;
     MPI_Status        status;  /* Stores tag, source, error, and n bytes */
     ...
     MPID_Eager_info   eager;
     MPID_Rndv_info    rndv;
     MPID_Buf_info     buf;
     ...
     MPID_Stream       stream;
     ...
     MPID_Packet       packet;
     } MPID_Request;
\end{verbatim}

Notes:
\begin{enumerate}
\item \code{volatile} is required on \code{ptr} in the \code{MPID_Eager_info}
  structure only for multithreaded versions, and then only because the code
  below looks at \code{ptr} to see if data is available after an eager send.
  If a separate flag indicated this, only the flag would have to be
  \code{volatile}. 

\item The \code{comm_ptr} is part of the \code{MPID_Buf_info} because it may
  be necessary to decrement the reference count on a communicator once a
  message has been received, just as the reference count on the datatype must
  be decremented.  

\item The values on the \code{MPID_Request_kind} are chosen to allow single
  bit tests to distinguish between persistant and non-persistant requests.

\item The \code{busy} and \code{xfer_completed} flags in \code{MPID_Request}
  need be 
  \code{volatile} only when either (a) the process is multithreaded or (b) the
  device shares memory, particularly requests, among the processes.

\item Packets are used to send information from one process to another.
  Packets have two components: a packet header and a packet payload.  The
  payload contains user data; many packets consist only of a header. 
  The packet type \code{MPID_Hid_eager_t} is one of the few packet types that
  contain both a header and a payload. 

  Question: some things might be simpler if all packet headers were the same
  size.  For example, they could all be the same number of cache lines.
  This has some costs but may make other things simpler.  Note that it can
  make packet headers \emph{shorter}: there is no need to record the length of
  the packet header since they are all the same.

\item Should the \code{MPID_Eager_info} and \code{MPID_Rndv_info} be in a
  union?  That would help reduce the overall size of an \code{MPID_Request}.
  One goal should be to keep an \code{MPID_Request} to a single cache-line;
  that will improve the performance, particular for the shared-memory devices.
\end{enumerate}

In the case where there are multiple threads, it is necessary to ensure that
only one thread (usually) modifies a request at a time.  This can be
accomplished using (thread) locks or using flags (e.g., a \code{busy} flag).
However, if flags are used, it is necessary to ensure that at pending writes
to memory on the process are completed before the flag is updated.  The macro
\mpidfunc{MPID_MemWrite_ordered} can be used for this.  
For example, for an Alpha processor using \code{gcc},
\mpidfunc{MPID_MemWrite_ordered} might look like this \cite{alpha-asm}:
\begin{verbatim}
#define MPID_MemWrite_ordered(var,value) { \
      asm volatile ("wmb":/*no output*/:/*no input*/); *var = value ; }
\end{verbatim}

Note that in OpenMP, this can use the \code{flush} directive, although that
may require a routine call, since you can't include a \code{\#pragma} within a
\code{\#define}.  In this case, \mpidfunc{MPID_MemWrite_ordered} would be a
function, with the definition:
\begin{verbatim}
void MPID_MemWrite_ordered( int *var, int value )
{
    #pragma omp flush
    *var = value;
}
\end{verbatim}

Question: are there any systems where it helps to flush only
specific variables  (OpenMP allows this)?

A single-threaded implementation might simply use
\begin{verbatim}
#define MPID_MemWrite_ordered( var, value ) 
\end{verbatim}
That is, the entire operation would be eliminated.

Request allocation:

Question: In a multithreaded case, the request id's could be partitioned
among the threads, avoiding the need for a thread lock.  Do we want to
consider that?  This is complicated by the fact that threads are not
required to register themselves with MPI (e.g., with a call to something
like \code{MPI_Thread_initialize}).\index{thread overhead!request allocation}
This probably isn't worth it, but it is a potential bottleneck on systems with
large numbers of CPUs per node and a single MPI process per node.

\implementation{{MPI_Irecv}}

\ifcodefirst
\fileinclude{samples/irecv_tcp.c}
\fi

Notes:
\begin{enumerate}
\item \mpidfunc{MPID_Request_recv_FOA} always returns a request.  If the
  request was found (and not \code{busy}), it is removed from the queue.  If
  one was not found, it is inserted and marked as busy.  Marking it as busy
  keeps another thread from finding the request, through another call to
  \mpidfunc{MPID_Request_recv_FOA}, of course, and attempting to use it before
  all of the fields have been set.  In a single threaded implementation, the
  \code{busy} flag is not needed.  
  Whether the request was found or inserted, 
  the \code{status} fields are filled in, along with the \code{context_id}.
  This ensures that another thread that calls \mpidfunc{MPID_Request_recv_FOA}
  will match this inserted request, but, because of the
  \code{busy}\index{thread overhead!request use} field,
  will wait until the request has been fully filled in.  An alternative is to
  use a thread lock on the request, but that is really unnecessary since only
  two threads (the one that inserts and the one the removes) will ever need to
  synchronize access to the request.

  Question: the following assumes that the fields in
  \mpids{MPID_Request}{status} are already filled in.  Is that what we want?

  Question: Should the \code{busy} flag be an integer or just a bit in the
  request flags?  Making it a bit makes atomic update more difficult, though
  for \code{busy}, it may not matter (no other thread can do anything until
  the \code{busy} bit is cleared.  The reason to consider a bit instead of an
  integer is to keep the size of a \mpidconst{MPID_Request} down.  

\item If a matching request was found, there are two cases.  Either the
  message was delivered eagerly or a rendezvous message was sent. Note that
  in either case the request has been removed from the receive queue already 
  by \mpidfunc{MPID_Request_recv_FOA}.

\item The first check is against the total size of the user's receive buffer.
  In the homogenous case, the size of the total message (saved in the
  \mpids{MPID_Request}{status.count} field of \mpidconst{MPID_Request}) is
  compared with \code{count} times the size in bytes of the user's
  \code{datatype}.  The original size is saved in \code{msg_size} so that the
  eager buffer can be freed and flow control updated.
  If the user's buffer is too small, invokes \mpidfunc{MPID_Err_create_code}
  to return a convenient message (using predefined text).

\item The next test is for the special case of a contiguous buffer.  In this
  case, we can use \code{memcpy} directly.

  Question: do we want to use a private \code{memcpy} that might use, for 
  example, quadword load and store instructions that some system libraries
  might not use?  Since we are likely to want this, we've used
  \mpidfunc{MPID_Memcpy} instead of \code{memcpy}.  Note that \code{mpptest}
  has a \code{-memcpy} switch; \code{-memcpy -int} uses \code{int}s for the
  moves instead of \code{memcpy} and \code{-memcpy -double} uses
  \code{double}s for the moves.  Some quick tests with a version that used
  \code{long long} instead of \code{int} and that set the options
  \code{-Msafeptr=auto,arg -Munroll=n:8} for \code{pgcc} showed that the
  \code{memcpy} and \code{long long} loop version had similar performance at
  1024 bytes but the (inlined) \code{long long} version was twice as fast at
  128 bytes.  \code{double}s are \emph{terrible} on I86.

\item Otherwise, the user's buffer is not contiguous, and the data must be
  unpacked.  This is a simple case since all of the data is present, and 
  can be handled with \mpidfunc{MPID_Unpack}.

  Question: note that \mpidfunc{MPID_Unpack} has slightly different arguments
  from the one in the ADI-3 document.  Are these the correct ones?  In a
  homogeneous system, the \code{msg_format} value is never needed.
  \code{msg_format} is used to indicate the format used by the sender; this
  might be ``senders format'' or XDR or external32 etc.  Homogeneous systems
  will always use ``senders format.''

\item Once the data has been moved from the eager buffer, the eager buffer 
 can be freed.  This call also updates any flow control information (e.g.,
 updating the number of free eager memory buffers available).  The request is
 now marked as complete.

  Question: Are there other values that we want to mark here?  For example, 
  a one in \code{xfer_completed} may mean ``data received but cleanup not done'' and
  a two might mean ``finished''.

\item If the request was found but the data was not available, we need to ask
  the sender to send us the data.  We do this with the \code{packet} that is
  within the \mpidconst{MPID_Request}.  We use the integer index of the
  request rather than its address because it is both addressing independent
  (e.g., on heterogeneous systems, we don't need to worry about mixes of 32
  and 64 bit pointers) and may even be shorter (e.g., we could use a 16 bit
  \code{short} for the value).  We send the request id of the receiving
  request back to the sender so that the data can be properly stored when it
  arrives (see the discussion of \mpidconst{MPID_Hid_data} in the
  communication agent).

  Question: if the message is truncated (user's buffer is too small), do 
  we want to tell the sender?  We aren't required to, and it could be
  difficult in the case of an eager send (the sender's call has long since
  completed).  

  Question: do we want to remember the source, rather than having to look up
  the source again through the communicator?

  Question: in the case that the receive data is contiguous, we could just
  send the sender the memory location of where to deposit the data.  Do we
  want to do that here or leave that for the remote memory case?

  Note that \code{MPID_Rhcv_tcp} must remember \code{vector} if it can't send
  the data immediately (e.g., because the socket is full).  However, if it can
  send it immediately, no copy needs ever be made.

\item If no matching request is found, the information needed to store the data
  when it does arrive must be saved.  We must also increment the reference
  counters on the datatype and the communicator to ensure that they aren't
  deleted before the communication completes.
  The \mpifunc{MPI_Irecv} can now return.  Completion of the message transfer
  is now the responsibility of the communication agent (see below). 

  Question: Should we go ahead and create the segment for the unpack, at least
  for non-contiguous datatypes?

  Question: Should the segement init increment the reference count for the
  datatype to ensure that no \mpifunc{MPI_Type_free} frees it before the
  segment code is done using it?

  Question: In the \mpifunc{MPI_Recv} case, we never need to increment the
  datatype and communicator reference counters.  Is there any significant 
  benefit to avoiding that step, particularly for very short messages?
  Probably not if the message is found; if the message is not found, the
  situation is murkier.  We should avoid the update to the permanent
  communicators and datatypes (writes are expensive).
\end{enumerate}

\ifcodefirst
\else
\fileinclude{samples/irecv_tcp.c}
\fi

\implementation{{MPI_Isend}}

\ifcodefirst
\fileinclude{samples/isend_tcp.c}
\fi

Notes:
\begin{enumerate}
\item For now, we assume that there are no ``speculative receives,'' that is,
  a receive that sends a message to the designated source of the message
  indicating that the receiver is prepared to receive a message.  Thus, 
  \mpidfunc{MPID_Request_send_FOA} always allocates, never finds, a request.

\item There are two cases.  Short messages are sent eagerly if flow control
  allows it; all other messages are sent by rendezvous.  
  The function \mpidfunc{MPID_Flow_limit} both tests the message size against
  the flow control limits, and if the limits allows it, updates the flow
  control limits to reserve that space.  This is needed for multithreaded
  codes to ensure that two threads do not both reserve the same eager buffer
  space.  Note that flow limits are used to limit the amount of space consumed
  at the destination; this is different from trying to limit the amount of
  data within the socket buffer.  For example, consider the code
\begin{verbatim}
    if (rank == 0) {
        for (i=0; i<1000; i++) MPI_Iprobe( ... );
    } 
    else if (rank == 1) {
        for (i=0; i<1000; i++) MPI_Isend( buf, 100, MPI_INT, ... );
    }
\end{verbatim}
  Each message sent by process 1 will fit within an empty socket buffer.  
  On process 0, some number of messages will be received eagerly.  However, at
  some point, process 0 will stop accepting messages from process 1 because
  the space for buffering them is exhausted.  Using \mpidfunc{MPID_Flow_limit}
  causes the \mpifunc{MPI_Isend} to shift to rendezvous mode even for short
  messages.  (This assumes that we can accept 1000 envelopes).

\item In the first case, the message is too large to send.  In this case,
  information on the message data is saved and a request-to-send packet is
  sent using \code{MPID_Rhcv_tcp}.  Note that the reference counts for the
  communicator and datatype are incremented to ensure that they are not
  deleted until the message is sent.

  If speculative receives are implemented, \mpidfunc{MPID_MemWrite_ordered}
  should be called at the end of this code to clear the \code{busy} flag.

  Question: do we really need to save the communicator?  We don't need the
  \code{context_id}, and only need the communicator to convert a rank in to a
  specific connection.  If we save the connection, we don't need to increment
  the reference count on the communicator.

  Question: Do we want to create the segment now instead of just saving the 
  \code{buffer, count, datatype}?  One reason not to is to try and inject the 
  message into the network as early as possible.

\item In the other case, the message may be sent eagerly.  There are two
  subcases here.  In the first, the message is either contiguous or small
  enough to be packed into a single temporary buffer.  The other case is a
  larger message.  We could ignore the larger message case by setting the
  eager limit to a small value, but we still would want to handle the large
  message case for \code{MPI_Rsend}, so we might as well do it here.

%   An important generalization is the case of a short \code{iovec} datatype;
%   that is, one that is not contiguous but is described by an \code{struct
%   iovec} array with a small number (e.g., 7) of elements.  This is handled in
%   the same way as the contiguous case.

\item We can generalize the contiguous case to datatypes that are described by
  short arrays of \code{struct iovec}, for example, seven elements or less.
  This allows the header to use one element and the datatype the remaining
  seven.  Longer arrays may not be handled efficiently by the operating
  system. 

\item In the case where the data is contiguous and is sent directly with
  \code{MPID_Rhcv_tcp}, the request is marked complete only when the data has
  been sent.  In the case where data is packed up, the request is complete as
  soon as the \code{MPID_Rhcv_tcp} call is made since the user's buffer is now
  available for reuse.  

\item In the case where the data is longer and noncontiguous, we must send it
  using incremental packing.  This uses a special data transfer routine,
  \mpidfunc{MPID_Stream_isend}.  The stream description is saved in
  the \mpids{MPID_Request}{stream} field in the \mpidconst{MPID_Request}.

\item We need some way to indicate that the \code{MPID_Request} itself can be
  reused.  Setting the \code{xfer_completed} flag indicates that the user's buffer
  can be reused.  Do we want to replace the \code{NULL} with something like
  \code{\&request_ptr->available}?
\end{enumerate}

\ifcodefirst
\else
\fileinclude{samples/isend_tcp.c}
\fi

\implementation{{Communication Agent}}
The communication agent in MPICH2 replaces the \code{MPID_Device_check} in
MPICH-1.  In a version of MPICH2 that uses polling, there will be a similar
routine that executes the communication agent code.  Other implementations may
execute the same code in a separate thread; in those implementations, there
will be no separate polling routine.

In a multi-method device, it is often easiest if each method's communication 
agent runs in a separate thread.  That allows the operating system to 
schedule the threads.  If the agents are running in the same thread (and 
particularly, if they are running in the main thread in a polling-mode 
implementation), each agent must normally be executed in the
\code{NONBLOCKING} (or \code{EXPECTING}) mode.

\ifcodefirst
\fileinclude{samples/agent_tcp.c}
\fi

Notes:

\begin{enumerate}
\item The routine \code{GetNextPacketHeader} returns a pointer to the next
  packet header, along with the \code{source} of the message.  This routine
  guarantees 
  that the entire packet header (the first \code{vector} element sent with
  \mpidfunc{MPID_Rhcv_tcp}) has been delivered.  
  One possible implementation (for variable-length packet headers) is for
  \code{GetNextPacketHeader} to look at the 
  first two shorts; the second of these is the length of the packet header.
  If less than a full packet has been delivered, \code{GetNextPacketHeader}
  does not 
  return a packet.  This simplifies the handling in the rest of the
  communication agent without requiring that \code{GetNextPacketHeader}
  understand the different packet types.

  The code for \code{GetNextPacketHeader} might look roughly like
\begin{verbatim}
while (1) {
    if (any buffer has at least 4 bytes &&
        buffer state is ``waiting for packet header'') {
        short *p = <head of buffer>
        if (p[1] >= <data in buffer>) {
            <data in buffer> -= p[1];
            <head of buffer> += p[1];
            (handle circular buffer)
            if (packet has payload)
                set buffer state to ``waiting for payload''
            return (MPID_Packet_t *)p;
        }
    }
    if (blocking == MPID_NONBLOCKING) return 0;
    <wait for data for either 0 seconds (MPID_NONBLOCKING) or 2*round-trip>
    if (still no data and blocking == MPID_EXPECTING) return 0;
}
\end{verbatim}

  Question: what is the source value?  Is it the local process id (e.g., the
  connection number)?

  Question: If we piggy-back flow-control information in the packets,
  \code{GetNextPacketHeader} could process that, eliminating that step from the
  communication agent code.  This flow control information would contain
  information such as ``2k of eager buffers released''.

\item The blocking parameter to \code{GetNextPacketHeader} has the following
  meanings:
  \begin{description}
  \item[\code{MPID_BLOCKING}:]Block until a packet is available.
  \item[\code{MPID_NONBLOCKING}:]Return a packet if one is available, otherwise
    return \code{NULL}.
  \item[\code{MPID_EXPECTING}:]Like \code{MPID_NONBLOCKING}, except
    \code{GetNextPacketHeader} may wait for a short time before returning.  For
    example, it may wait the time it takes a message to make a round trip in a
    case where a response is expected from another process.
  \end{description}

\item When \code{GetNextPacketHeader} returns with a packet, the data that
  \code{packet} points at is valid until the next time that
  \code{GetNextPacketHeader} is called.  This allows
  \code{GetNextPacketHeader} to read 
  into an internal buffer and return a pointer into that buffer, avoiding an
  extra memory copy.  This works only if only one thread at a time calls the
  communication agent.  However, the restriction that only one thread call the
  communication agent at a time is fairly natural.  In fact, the MPI thread
  mode \mpiconst{MPI_THREAD_SERIALIZED} expresses this; in such a case, the
  communication agent does not need acquire a thread lock in the case where
  many threads may call the communication agent (e.g., in a polling mode
  implementation where there is no separate thread running the communication
  agent).

\item \code{GetNextPacketHeader} must also keep track of the total message
  size that 
  a packet starts, so that subsequent calls to \code{GetNextPacketHeader} will
  not 
  interpret data as a packet header.  This is covered in more detail under
  \mpidconst{MPID_Hid_data}.  A consequence of this is that
  \code{GetNextPacketHeader} might process additional data before returning a
  new packet.

  Note: an alternative is for \code{GetNextPacketHeader} to return a special
  kind of packet for streaming data, allowing the communication agent to
  process data as it is read.  The reason that we don't do this is that for
  some kinds of messages, we want to bypass any buffering, and that requires
  putting some message handling into the lower level communication functions.
  Question: Do we want to change the name to something that indicates that it
  does more than just read a packet?

  Note also that the processing implicit in \mpidfunc{MPID_Stream_isend} or
  \mpidfunc{MPID_Stream_irecv} may happen inside of
  \code{GetNextPacketHeader}.  Continuing the code fragment describing
  \code{GetNextPacketHeader} from above, at the top we might have
\begin{verbatim}
   foreach (socket with an active send stream) {
       if (stream buffer empty) {
           pack data into stream buffer
       }
       <try to write current stream buffer>
       if (success and at end of stream) {
           mark stream as complete
           invoke completion routine
       }
   }
   foreach (socket with an active recv stream) {
       <try to receive data to fill stream buffer>
       if (stream buffer full) {
           unpack data from stream buffer
           if (success and at end of stream) {
               mark stream as complete
               invoke completion routine
           }
       }
   }
\end{verbatim}


\item Question: Is \code{GetNextPacketHeader} fair?  How do we ensure that
  \mpifunc{MPI_Testsome} is efficiently implemented?

\item Under Linux, very short messages are often delayed, even when
  \code{TCP_NODELAY} is set (see
  \url{http://www.icase.edu/coral/LinuxTCP.html}).  Loncaric found that it is 
  advantageous 
  to always send at least 100 bytes.  We could accomodate this by either
  padding all of the packet types out to 100 bytes or always sending at least
  100 bytes with \code{MPID_Rhcv_tcp}.  In that case, we must ensure that
  \code{GetNextPacketHeader} knows to skip over the ``extra'' bytes, possibly
  by 
  changing the \code{len} field in the \code{packet} and sending the extra
  bytes.  

\item Note that \code{GetNextPacketHeader} guarantees only that the packet
  header is available.  The payload in packet types that contain payload, such
  as \mpidconst{MPID_Hid_eager_t} or \mpidconst{MPID_Hid_data_t}, may not be
  available.  

\item If we use a test on \mpidconst{MPID_THREAD_LEVEL} to check to see if we
  need to lock the agent, we also need to use that same mutex in any routine
  that we might provide to change the thread level.  If we only permit the
  choice of thread level within \mpifunc{MPI_Thread_init}, then we don't need
  to worry about this.  Of course, we'll have a configure option
  \cfgoption{--enable-single_threaded} that eliminates all thread locks needed
  to support multiple user threads at compile time.

\item The sample code shows inlined code for each packet type.  An
  implementation is more likely to use functions to handle each packet type,
  at least for the less common cases (e.g., \mpidconst{MPID_Hid_control} and
  error return on a ready-send).

\item In the multithreaded case but polling case, we use a single mutex on the
  agent.  Instead, we could have \code{GetNextPacketHeader} set a mutex on that
  source (e.g., socket), which would then be released by the agent when the
  packet was fully handled.  This would allow multiple threads to run the
  agent simultaneously on different sources.
\end{enumerate}

\mpidconst{MPID_Hid_eager}:
\begin{enumerate}
\item This is a message followed by data.  The \code{lpacket} points to the
  packet header.  Note that while the data is immediately behind the packet,
  there is no guarantee that it has arrived yet (more on this below).

\item We use an array reference on the \code{context_id} to find the
  corresponding communicator.  Another approach would be to store the
  communicator pointer in the request, and allow the matching logic in
  \mpidfunc{MPID_Request_recv_FOA} find the corresponding request.  
  If \mpidfunc{MPID_Request_recv_FOA} needs the communicator (e.g., in the
  multimethod case to find the appropriate method queue), it could do the
  lookup on the \code{context_id} on its own.

\item There are two cases here depending on whether a matching request was
  found by \mpidfunc{MPID_Request_recv_FOA}.  

\item If a request was found, there is an available user buffer.  We need to
  tell the socket layer to transfer data to that buffer.  This is relatively
  easy if the datatype is contiguous (and only slightly more difficult if it
  is easily described by an \code{iovec}).  Handling the more general datatype
  case requires a little more care.  In either case, we need to tell the
  socket layer where the data should be put, how many bytes to process, and
  how to move them.  This is a \emph{stream}; the low-level code is
  responsible for handling this.

%             /* Initiate an unpack.  Note that the data may not all 
%                have arrived yet.  This must:
%                1. Process all available data (up to lpacket->msg_bytes)
%                2. If not all data is available, indicate to low-level
%                   communication that the data is stream.
%                3. Indicate to stream what field to set in the request
%                   when all data is read and unpacked.
%             */

\item If a request was not found, then one was allocated and returned by
  \mpidfunc{MPID_Request_recv_FOA}.  

\item Check to see if this is a ready-send.  If so, then there is an error
  because a matching request was not found.  Return an error indication to the
  sending process and tell the socket layer to discard the data.
  Note that this process must read the data since it is being sent.  Since
  this is an error case, there is no reason to try and optimize this case by
  asking the sender to stop sending the data.

  Question: do we also want to generate an error message?  Do we want to
  invoke the error handler on the intended communicator on the receiving
  process?  

\item If it isn't a ready-send, then we need to allocate space to hold the
  data and copy it in.  We allocate the space with
  \mpidfunc{MPID_EagerAlloc}. 

  We cannot
  use \code{malloc} instead of \mpidfunc{MPID_EagerAlloc}, at least on all
  platforms (and we must enforce 
  resource limits) because the \code{malloc} might fail, and our flow-control
  promise is that there is space available.  If we depend on \code{malloc}, we
  need to provide for a negative acknowledgement on eager messages.

  Note: we could leave the data in the internal buffer at least briefly,
  avoiding one copy.  The added complexity of remembering where to move the
  data when we needed to probably outweighs the small gain in efficiency.

\item Once the data is allocated (the allocation always succeeds as long as
  the flow control algorithm is correct), a stream is set up to receive the
  data as bytes into the allocated buffer.  

\item Once all of the bytes of the stream have been read, the function
  \code{MPID_Eager_complete_func}, which was passed to
  \mpidfunc{MPID_Stream_irecv_tcp}, is called with the fifth argument of
  \mpidfunc{MPID_Stream_irecv_tcp}, which is the pointer to the request.  This
  allows the communication agent to 
  \begin{enumerate}
  \item Indicate that the data is available by setting the eager buffer
    pointer
  \item Clear the busy flag
  \item For multithreaded processes, where one thread is waiting on this
    request, signal the waiting thread.
  \end{enumerate}

\end{enumerate}

\mpidconst{MPID_Hid_request_to_send}:
\begin{enumerate}
\item This starts in the same was as \mpidconst{MPID_Hid_eager}, with a call
  to \mpidfunc{MPID_Request_recv_FOA}.  

\item If the request is found, this executes the same steps as in the
  \mpifunc{MPI_Irecv} case for found and not an eager message (e.g., it
  returns an \mpidconst{MPID_Hid_ok_to_send} packet).

\item Otherwise, it saves the information needed to request the data at a
  later time. Note
  that the \mpids{MPID_Hid_request_to_send}{length} field is needed to
  implement \mpifunc{MPI_Iprobe}. 
\end{enumerate}

\mpidconst{MPID_Hid_ok_to_send}:
\begin{enumerate}
\item First, the request is identified from the request id in the packet.

\item The data will be sent with a packet header of
  \mpidconst{MPID_Hid_data}.  There are two subcases:
  \begin{enumerate}
  \item The data is contiguous.  In this case, a simple \code{MPID_Rhcv_tcp}
    call can send all of the data.

  \item The data is not contiguous.  In this case, we create a message
    stream.  

   Question: do we need to call a routine to release any stream or segment
   resources when we are done?  Or do we make this automatic with the stream
   routines? 

  \end{enumerate}
\end{enumerate}

\mpidconst{MPID_Hid_data}:
\begin{enumerate}
\item This indicates data that matches a request.  The request id is part of
  the packet header.  The data must be received with a stream since all of the
  data may not be available yet.  

  Question: Do we need to call a routine at completion to release any stream
  or segment resources when we are done? Or do we make this automatic with the
  stream routines?

\end{enumerate}

\mpidconst{MPID_Hid_control}:
\begin{enumerate}
\item This case is for miscellaneous control messages.  The only control
  message defined so far is the ready-send error.  Others might include abort,
  exit, are-you-alive, etc.
\end{enumerate}

\ifcodefirst
\else
\fileinclude{samples/agent_tcp.c}
\fi

\implementation{{MPI_Wait}}

\ifcodefirst
\fileinclude{samples/wait_tcp.c}
\fi

Notes:
\begin{enumerate}
\item In the single-threaded case, we need only check the
  \mpids{MPID_Request}{xfer_completed} flag. 
  If set, then copy out the \mpiconst{MPI_Status} data (if the \code{status}
  pointer is not \mpiconst{MPI_STATUS_NULL}) and free the request.  Otherwise, 
  call the communication agent in blocking mode (wait until something happens)
  and then check again.

\item In the multi-threaded case, if the request is not complete, we want to
  transfer control to another thread, particularly to the thread running the
  communication agent if there is one, so that the message can be completed.
  This allows the implementation to avoid spinning on the
  \mpids{MPID_Request}{xfer_completed} flag.  However\index{thread overhead!request
    completion}, it does force the use of a thread mutex to guard the
  \code{xfer_completed} flag and a thread condition variable to release the
  particular request.  (We are probably missing guards around \code{xfer_completed}
  elsewhere as well.)

  The sample code shows the use of condition variables with pthreads.
  The routine \code{pthread_cond_wait} atomically releases the
  \code{request_mutex} and 
  waits for the condition variable \code{cond}.  The condition variable is set
  when another thread executes \code{pthread_cond_broadcast} (or
  \code{pthread_cond_signal} to this particular thread).
  Note that this algorithm requires either a mutex per request or (less
  scalably) a mutex 
  on all requests (but local to the calling MPI process).  
  Note that a single mutex may be more appropriate for the multiple completion
  routines such as \mpifunc{MPI_Waitsome} or \mpifunc{MPI_Waitall}.
  The above approach also requires that the communication agent execute a
  \code{pthread_cond_signal} to release this waiting thread.  Note that by
  storing the thread id of the waiting process, we can avoid calling
  \code{pthread_cond_signal} when no process is waiting.

  Another alternative is to simply busy wait on \mpids{MPI_Request}{xfer_completed}.
  
  Note that pthreads are not available on all platforms; we may need to
  implement other approaches as well.
  
  Question: How well do pthread condition variables work on our important Unix
  platforms (e.g., Linux, Solaris, AIX, etc.)?  Is there a similar Windows NT
  approach, or is something different required?

\item Missing from this routine is any processing that may be required after
  the data transfer completes.  Note that, particularly in the case of
  \mpidfunc{MPID_Rhcv}, the \code{xfer_completed} field is set once data is
  transfered.  However, if some temporary buffer was allocated for sending or
  a datatype reference count was incremented, some further steps must be taken
  when finishing the request off.
\end{enumerate}

\ifcodefirst
\else
\fileinclude{samples/wait_tcp.c}
\fi

\pagerule\par
The remaining text is leftover from a previous version and will be mined as
appropriate for new text.  Read at your own peril.

\pagerule\par

\subsubsection{\shmemname}
(not done)

We still need streams here, but since we're responsible for
everything, we may need to send messages back and forth for each
segment.  This may add an additional message type,
\mpidconst{MPID_Hid_next_data_t}.  However, this could be hidden within the
lowest level code, making streams a good abstraction in this case as well.

One approach to a shared memory device is to implement a \tcpname-set of
operations for all control messages (the \mpidconst{MPID_Hid_xxx}), but
arrange for data exchanges to use shared memory.  The description here takes
this one step further; the message queues are in shared memory, allowing any
process to update the queues.

\paragraph{Implementing Streams in Shared Memory.}
Streams are implemented by allocating \emph{two} (contiguous) regions in
shared memory.  Call these region 1 and region 2.  The (send) stream starts by
placing data in region 1.  It then informs the destination process that data
is available in that region by providing the address and length of that
region (probably by sending it a message).  The sending process then fills in
region 2 with the next section of data and informs the destionation that that
region is ready.  On the receiving side, as each region is read, the receiver
indicates that to the sender.  Because two buffers are used, one process can
be filling the one while the other process is reading the other buffer.  This
can effectively double the bandwidth of a transfer that make use of a single
buffer.


Issues:
\begin{enumerate}
\item How does the sending process indicate that the next buffer is ready?
  Does it set a flag at the end of the buffer?  Does it send a message?  The
  advantage of sending a message is that it allows the destination process to
  check one location (or queue) for input, regardless of how many streams and
  other operations are taking place.  Since the message is short, it could be
  sent in a single cache line, making the cost very low (particular in an
  implementation that used lock-free queues for messages).  

\end{enumerate}


\paragraph{Allocating Eager Buffer Space.}
In this device, the message queues are kept in shared memory, allowing a
sending process to immediately know whether a posted receive exists.  In this
case, an eager message can be delivered by allocating the eager buffer from
shared memory.  The sending process then fills the eager buffer with the data,
sets the eager buffer fields in the request (e.g., \code{MPID_Eager_info}),
and clears the \code{busy} flag in the request.  When the eager message is
finally received, the eager buffer is returned to shared memory.

\mpifunc{MPI_Irecv}:
The initial steps are the same as in the \tcpname\ case.  A major difference
is caused by the fact that the requests can be (and we will assume are)
present in shared memory.  In this case, each process can check for a matching
queue element directly. The two cases are
\begin{enumerate}
\item The request was not found.  This is the same as the \tcpname\ case.
\item The request was found.  This is close to the \tcpname\ case,
  particularly for the complex-datatype case.  The particular cases are
  \begin{enumerate}
  \item The data has already been delivered.  It is either within the request
    (very short) or in a shared-data buffer used for eager messages.  As in
    the \tcpname\ case, copy the data into the user's buffer using
    \mpidfunc{MPID_Unpack}.  Note that in the multi-method case, if the device
    is heterogeneous, we still need the \mpidconst{recved_format} etc. fields
    since the data may have been packed for a communicator containing
    processes with different data represenations.
    Question: who frees the eager buffer space?  Does the receiver free it
    after unpacking the data, or do we ask the sender to free it?  We might
    want the sender to free it if the sender allocated it.

  \item The data has not been delivered.  This follows the \tcpname\ case,
    since the data must be delivered by having the sender place some of it in
    shared memory, then signal the receiving process, and so on.  The
    receiving process creates the corresponding segment (including one or more
    buffers in shared memory?) and sends the address and size of that buffer
    to the sender using \mpidfunc{MPID_Rhcv} to deliver a message to the
    sender's incoming message queue.  Further processing is handled by the
    communication agent (including any double buffering of the transfer).
  \end{enumerate}
\end{enumerate}


\mpifunc{MPI_Isend}:

Check remote queue for matching receive request (using
\mpidfunc{MPID_Request_send_FOA}?).  
\begin{enumerate}
\item If not found, insert (an unexpected receive).  There are two cases: a
  short message that uses an eager delivery, and a long message that requires
  a rendezvous.  If an eager delivery, allocate the space in shared memory,
  copy into the eager buffer, and set the corresponding eager fields in the
  request.  If a rendezvous, set the matching data
  including the sender's 
  request id, use \mpidfunc{MPID_MemWrite_ordered} to clear the busy flag, and
  return.  
\item If found, match (remove from queue) and begin transfer.  There are
  several cases:
    \begin{enumerate}
    \item The destination buffer is in shared memory and the datatype is
      simple (not the case in this example, but one that must be considered). 
      In this case, copy to the destination buffer (basically
      \mpidfunc{MPID_Pack}), followed by \mpidfunc{MPID_MemWrite_ordered} to set
      the \code{xfer_completed} flag in the receive request.  Also mark the
      send request 
      as completed.
    \item The destination buffer is not in shared memory, but the total
      message length is small.  In this case, use \mpidfunc{MPID_Pack} to pack
      the message into a shared-memory buffer (Question: who allocates this?
      The sender?  The receiver?) and mark the send request as complete.
      Use \mpidfunc{MPID_MemWrite_ordered} to mark the receive request as having
      the data but not yet complete (e.g., a final step is needed to move the
      data from shared memory into the user's buffer).  Question: how do we
      indicate this in the request?
    \item The destination buffer is not in shared memory and the message
      length is large.  In this case, the message must be delivered in
      segments, using shared memory to effect the transfer.  One option is to
      place the first segment into a designated shared-memory buffer (who
      allocates it?) and indicate its location in the receive request.  All
      further transfers will be accomplished with the communication agent.
    \end{enumerate}
    Subsequent transfers will be handled by the communication agent.
\end{enumerate}

\mpifunc{MPI_Wait}:

Again, this is much like the \tcpname\ case.  Note, however, that the
\mpids{MPID_Request}{xfer_completed} field can, in some cases, be set by the
sending process (particularly for contiguous receive buffers that are in
shared memory), so this field must be marked \code{volatile}, even in the
single-threaded process case.

Also note the case that all of the data (or the last segment) has been
delivered (by being placed 
into shared memory), but the final \mpidconst{MPID_Unpack} to move that data
into the user's buffer has not been performed.  How is this indicated?  
In the \shmemname\ case, do we indicate
this with a bit or field in the \mpidconst{MPID_Request}?  For example, is
there a \mpids{MPID_Request}{data_available} field that is used to indicate
that memory is available?  This field could be used instead of an explicit
message to manage the communication.  

Question: If we use a \code{data_available} field, how do we avoid
busy-waiting?  Do we simply poll and yield?

\texttt{Communication agent:}

% The communication agent is responsible for making progress on a stream (the
% sending of segments).  There is no special action on the receiving end.  On
% the sending end, when the \mpidconst{MPID_Hid_ok_to_send} is received, the
% send is started.  Note that a handshake is required on the transfer of data
% from the sender to the receiver because the buffer that is used for the
% transfer must be explicitly filled by the sender and emptied by the receiver.
% Question: Do we want to double buffer?  How does that change the messages
% exchanged between the sender and receiver?

% If the communication agent is in a separate thread, we can wake up, spin a
% little (checking the incoming message queue), and yield.  If single-threaded,
% we should spin for roughly the round-trip message time (an internal message,
% not an MPI message) and then yield (on Linux, with \code{sched_yield}).
% Question: do we want the option of using something like the System V
% semaphores to implement a condition variable for the communication agent, so
% that updates to the incoming message queues would wake up the agent?

% Question: How do we notify the agent that we are ready for another segment?
% Does the communication agent wait on something?  What?  What about the case
% where there are 100 pending requests?
% An easy way may simply be to send a message, following the approach in
% \tcpname.  A more complex approach, introduced in the discussion of
% \mpifunc{MPI_Wait} above, is to use a field within the request (or even in the
% shared-memory transfer area itself) to indicate that a segment has been copied
% into or out of shared memory.  

% Note that waiting on fields in individual request items is ok in this simple
% example, but becomes less suitable when there are a significant number of
% pending requests.  Question: we could establish a bit vector of pending
% requests (each bit represents a request and the position matches the index);
% this could be atomically updated into indicate that a process's 
% request was ready for operation.  However, this may not be much better than
% sending a message to the communication agent.

The major role of the communication agent is to respond to messages, and to
ensure that the stream operations make progress.

\subsubsection{\shmemallname}
(not done)

Note that in this case (all memory available), the simple transfers can be
made directly, once the send and receive requests are matched.  Also simple
are the cases where either the sender or the receiver provides a contiguous
datatype; in that case, the other process executes either
\mpidfunc{MPID_Unpack} \mpidfunc{MPID_Pack}.  Only in the case where both
processes specify complex datatypes may it be necessary to transfer data
through a cannonical, contiguous representation.

\subsubsection{\vianame}
(not done)

This method is very similar to the \tcpname\ method, with the difference that
data can be written into and read from remote memory that has been registered.

The principle difference between this method and the \tcpname\ method is in
the way in which messages are delivered and the ability to deliver contiguous
(and perhaps simple \code{iovec} structures) directly.  Because of this, there
are two different types of rendezvous:
\begin{enumerate}
\item A contiguous to contiguous (or \code{iovec} to \code{iovec}) transfer
\item A stream transfer, requiring a copy into an intermediate buffer.
  This has two subcases:
  \begin{enumerate}
  \item One process has contiguous data
  This case is much like the \shmemname\ case, since the data in contiguous
  form can be moved directly with a remote write or read, and the process with
  the non-contiguous data either packs or unpacks it as appropriate.
  \item Neither process has contiguous data.  This is similar to the
  stream implementation for the \tcpname\ case (data is packed into a
  contiguous buffer, copied to a remote contiguous buffer, and then unpacked).
  \end{enumerate}
\end{enumerate}
These suggest special cases that can be optimized.

\mpifunc{MPI_Irecv}:

Follow the approach in \tcpname.  
If a message must be sent (e.g., a
\mpidconst{MPID_Hid_ok_to_send}), \mpidfunc{MPID_Rhcv} sends it by writing to
a pre-registered location.  Flow control provides information on where to
write (e.g., multiple incoming locations can be allocated, and either
information on each message updates what is free or separate flow control
messages are sent).   

Note that because the data is noncontiguous, it must be delivered into some
intermediate memory in contiguous form\footnote{We assume that either the
  method cannot handle any noncontiguous data or that the noncontiguous
  datatype in this example cannot be directly handled.}.  We further assume
that memory to be used for such transfers has already been registered; the
location of that memory can be communicated back to the sender in the
\mpidconst{MPID_Hid_ok_to_send} message.  

Question: who manages this pool of pre-registered memory?  Note that since
target memory must be registered, the receiver must be the one that specifies
this.  

Further processing is handled by the communication agent.

\mpifunc{MPI_Isend}:

Follow the approach in \tcpname.

Note that since we don't know whether the destination is going to provide a
contiguous, registered-memory buffer or not, we shouldn't take the step of
transfering the first segment into an internal registered-memory buffer.

\mpifunc{MPI_Wait}:

Follow the approach in \tcpname.  

\texttt{Communication agent:}

The agent must respond to messages in much the same way as \tcpname.  However,
the process of sending a stream of segments may be different. 

Question: how do we want to tell the process at the other end of the stream to
read the current block of data?

\subsection{Nonblocking Send and Receive with Contiguous Data.}
(not complete)

The major difference in this case is for the \shmemname\ and \vianame\ cases,
where it may be possible to send data directly from one user buffer to
another.  

\begin{via}
  Question: How do we indicate that a message has been transfered?  Do we want
  to use a remote write to set the \mpids{MPID_Request}{xfer_completed} field
  directly, rather than sending a message?  Is it better to receive a
  completion message from the sender or to spin-wait on
  \mpids{MPID_Request}{xfer_completed}? 

\end{via}

\subsection{Blocking Send and Receive with Noncontiguous Data.}
\label{sec:blocking-optimization}
(not complete)

Once a transfer is initiated, particularly in the \shmemname\ and \vianame\
cases, instead of sending a separate handshake message, we could set a flag
value in the transfer buffer itself, particularly for the cases involving
non-contiguous data where the transfer buffer is not the same as the user's
buffer.  

By initiated here, we mean once the first block has actually been
transferred.  This ensures that the sender has received the
\mpidconst{MPID_Hid_ok_to_send} and has started to act on it.  If the sender
knows (or has been told in the \mpidconst{MPID_Hid_ok_to_send} message) that
the receiver is blocking, it can switch to this alternate method for
indicating that data has been transferred.

Note also that in this case the reference counts for the datatype and
communicator in the operation do not need to be changed.



\subsection{Cancel}
Cancelling a receive is relatively easy (unless speculative receives
are implemented).   This simply removes a posted but unmatched receive
from the receive queue.  This needs a
\mpidfunc{MPID_Request_recv_remove} call.

Cancelling a send requires more effort, particularly when the receive
queue is not directly accessible to the sender (as it is in the
\shmemname\ method).  

\subsubsection{\tcpname}
\fileinclude{samples/cancel_tcp.c}

Notes:
\begin{enumerate}
\item A request is identified by the pair of sender request id and
source process.  Question: should this be the local process id or the
pair of context id and rank in communicator?

\item Where are the packets allocated for the cancel acknowledgement?
How are they recovered once 
they are sent? One possibility is to first link the packets onto a
pending list and include a flag in the packet that is used as the
completion flag by the \code{MPID_Rhcv_tcp} call.  Then this list can
be occassionally checked for complete items, particularly when such a
packet must be allocated.  This list then become a list of ``items to
be freed soon''.

\item The code shows a request \code{state}.  Do we really need this?
If we do, we need to check this elsewhere.  We could use this to
remember other states, such as ``delete temporary buffers when
complete''.

\end{enumerate}

\subsubsection{\shmemname}
(not done)
(direct access to receiver's queue may make this relatively easy)

Question: Do we just want to have \mpidfunc{MPID_Request_recv_cancel}
and \mpidfunc{MPID_Request_send_cancel} (instead of the current
\mpidfunc{MPID_Request_cancel})? 

\subsubsection{\shmemallname}
(not done)

\subsubsection{\vianame}
(not done)
(Is this like \tcpname?)

\subsection{Multiple Completion}
(not done)

(This section needs to examine testsome and similar routines)

