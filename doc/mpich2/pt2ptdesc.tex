\subsubsection{Scenarios}
\label{sec:pt-2-pt-scenarios}

To best understand how the point-to-point communication routines work, we will
describe several scenarios that illustrate how various communication methods
may implement communication.

\paragraph{Nonblocking Send and Receive with Complex Datatypes.}
This represents one of the more complex cases; most communication will offer
some opportunities for exploiting special cases such as simple datatypes
representing contiguous data or blocking communication.  

The datatype is assumed to be complex enough that the device cannot handle it
directly; for example, it may be an hindexed type with a large number (e.g.,
10000) of entries, or a simple resized struct datatype and a large count.
The code for this scenario may look something like this:
\begin{verbatim}
req = MPI_REQUEST_NULL;
if (rank == dest) {
    MPI_Irecv( buffer, count1, datatype1, tag, source, comm, &req );
}
else if (rank == source) {
    MPI_Isend( buffer, count2, datatype2, tag, dest, comm, &req );
}
MPI_Wait( &req, &status );
\end{verbatim}

In the following, a \emph{segment} is a description of part of the data, for
example, the first 32k bytes.  A \emph{stream} is communication made up of
segments.

\begin{mmadi}
\begin{tcp}
\mpifunc{MPI_Irecv}:
Use \mpidfunc{MPID_Request_recv_FOA} to check the queue for a matching
request.   
\begin{enumerate}
\index{thread overhead!request allocation}
\item No matching request is found, so one is added.
This request is returned with a ``busy'' flag set (is this a separate word or
a bit \mpidconst{MPID_REQUEST_BUSY} in \mpids{MPI_Request}{_flag}?); this is
necessary so that no incoming message will match this request until it is
ready.  However, the fields \mpids{MPI_Request}{tag},
\mpids{MPI_Request}{source}, and \mpids{MPID_Request}{context_id} are already
filled in.  

Create a segment that describes the tuple \code{(buffer, count1, datatype1)}.
The segment contains the original \mpids{MPID_Segment}{buffer} and a filled-in
stack \mpids{MPID_Segment}{loopinfo} for processing the datatype.  This
segment is attached to the \mpidconst{MPID_Request}.  Question: is it a field
in the request, or is a pointer to it provided?  

Once the segment is attached, the request can be marked ready.
This requires an ordered write to the flag value\index{thread overhead!ready
  flag}; that is, there must be a write barrier to flush any pending write
operations before the flag is written.
Question: do we want a special ordered-assignment operation that can issue the
appropriate operation?  E.g., a \mpidfunc{MPID_Write_ordered} macro that takes
the assignment as an argment?  For example,
\begin{verbatim}
#define MPID_Write_ordered(stmt) { asm(wsync); stmt; }
\end{verbatim}
Note that in OpenMP, this can use the \code{flush} directive, although that
may require a routine call, since you can't include a \code{\#pragma} within a
\code{\#define}.  Question: are there any systems where it helps to flush only
specific variables  (OpenMP allows this)?
Return from \mpifunc{MPI_Irecv}.

\item A matching request is found (the send request arrived before the recv).

This means that a request to send or the entire message has already been
received. 
The request is \emph{removed} from the receive queue so that no other thread
can match it (and any send-cancel will now fail).

First, check the amount of data sent against the size of the buffer specified
by the user, where size is (roughly) \code{count * datatype1->size}.  If the
amount of data sent is larger than that, set the error return to error class
\mpiconst{MPI_ERR_TRUNCATE}.   
Question: how do we ensure that no buffer overruns occur during the rest of
this operation?

There are two cases:
\begin{enumerate}
\item All data has already been delivered.  
Data is in a buffer pointed at by \mpids{MPID_Request}{recved_ptr} with length
(in bytes) of \mpids{MPID_Requets}{recved_bytes}.  This is copied into the
destination buffer according to the \code{datatype1} using
\code{MPID_Unpack}.  Note that in a heterogeneous system, the format of the
data must also be remembered (in \mpids{MPID_Request}{recved_format}).
The buffer that holds the received data must be freed, and any flow control
updated (e.g., with \mpidfunc{MPID_Flow_recv}).  The request is marked as
complete (also using the \mpidfunc{MPID_Write_ordered} macro).
Question: should the status fields be filled in now, or should that be left
until later in case the user specified \mpiconst{MPI_STATUS_NULL}?
Question:  Note that no \mpidconst{MPID_Segment} is required in this case
unless as an argument to \mpidfunc{MPID_Unpack}.  Is this what we want?

\item All data has not been delivered.  In this case, a request to deliver the
  data must be sent.  
  
  First, create a \mpidconst{MPID_Segment} describing the receive buffer.
  Use that segment to create a (receive) stream.
  Use \mpidfunc{MPID_Rhcv} to send a request back to the
  sender, including the sender's request id
  (\mpids{MPID_Hid_Request_to_send}{request_id}) and an id that can be used to
  identify this request.  

  Question: should this simply by the index of the \mpiconst{MPI_Request}?
  That has the advantage of being addressing-independent, as well as saving a
  few bytes (a short should be sufficient).  The id of the receiving request
  is needed to know where to mark the message as complete as well as to access
  any other segment/stream information.

  Question: there is no explicit stream here; that is, the stream handling is
  done through calls to \mpidfunc{MPID_Rhcv} and the way in which the
  communication agent responds to those messages.  Does it make sense to have
  an explicit notion of a stream here?  Note that we do want something more
  that we have here for some of the collective routines such as
  \mpifunc{MPI_Bcast} (resend data before unpacking) and
  \mpifunc{MPI_Allreduce} (operate on data before resending).

  The \mpifunc{MPI_Irecv} can now return.  Completion of the message transfer
  is now the responsibility of the communication agent.  (See below.)

\end{enumerate}

\end{enumerate}
\mpifunc{MPI_Isend}:

Use \mpidfunc{MPID_Request_send_FOA} to allocate a request (this allows for
the speculative receive case; for now, we will assume that this routine simply
allocates a new request).  

There are two cases:
\begin{enumerate}
\item The data can be sent eagerly.

Use \mpidfunc{MPID_Pack} to pack the data up.  Use \mpidfunc{MPID_Rhcv} to
send the envelope and data.  Mark the request as completed
(\mpids{MPID_Request}{complete}). 
Question: how do we get the pointer to the buffer to pack into?

Question: who releases the pack buffer?  Is that something the
\mpidfunc{MPID_Rhcv} should do once the data is truely sent?  Alternately,
should \mpidfunc{MPID_Rhcv} know how to mark the request as completed, and
have a separate cleanup step that releases any local resources as part of
freeing the request.

\item The data must be sent with rendezvous.
Create a segment and associated stream.  Save these in the request.
Mark the send request as ready (\mpidfunc{MPID_Write_ordered}) and use
\mpidfunc{MPID_Rhcv} to send a \mpidconst{MPID_Hid_Request_to_send} message to
the destination.  All further processing will be handled by the communication
agent. 
\end{enumerate}

\mpifunc{MPI_Wait}:

Wait on the request to complete.  There are several cases:
\begin{enumerate}
\item Single threaded code.  Check the \mpids{MPID_Request}{completed} flag.
  If set, then copy out the \mpiconst{MPI_Status} data (if the \code{status}
  pointer is not \mpiconst{MPI_STATUS_NULL}) and free the request.  Otherwise, 
  call the communication agent in blocking mode (wait until something happens)
  and then check again.

\item Multithreaded code.  Here we need to wait for the
  \mpids{MPID_Request}{completed} field to be set; we assume that the
  communication agent is in a separate thread.  One approach in a pthreads
  environment is to use a 
  condition variable as follows:\index{thread overhead!request completion}
\begin{verbatim}
    while (1) {
        pthread_mutex_lock( &request_mutex );
        if (request->completed) {
            pthread_mutex_unlock( &request_mutex );
            if (status) memcpy( status, request->status );
            MPID_Request_free( request );
            *request = MPI_REQUEST_NULL;
            return MPI_SUCCESS;
            }
        else 
            pthread_cond_wait( &request_mutex, &cond );
    }
\end{verbatim}  
Note that this requires either a mutex per request or (less scalably) a mutex
on all requests.

Another alternative is to simply busy wait on \mpids{MPI_Request}{completed}.
\end{enumerate}

Communication Agent:

On the receiving end, the communication agent is responsible for handling an
incoming \mpidconst{MPID_Hid_request_to_send} message.  
\mpidfunc{MPID_Request_recv_FOA} is called to find a matching request, if
any.  There are two cases.
\begin{enumerate}
\item A matching request was found.  The request is removed from the receive
  queue (marked matched?), and an \mpidconst{MPID_Hid_ok_to_send} message is
  returned using \mpidfunc{MPID_Rhcv}.
\item A matching request was not found.  The information on the message from
  the envelope (e.g., length, request id, tag, source) is saved in the
  request. 
\end{enumerate}

It also handles the case of an eager message (\mpidconst{MPID_Hid_short}).
The sequence of operations is similar, except that in the case of an
unexpected message, the data is copied into a contiguous buffer.  In addition,
the flow-control values must indicate the consumption of the eager buffer
space.  

Question: What allocates the buffers for unexpected eager messages?

The communication agent is responsible for transferring the message once it is
matched.  This is initiated when the sender process receives a
\mpidconst{MPID_Hid_ok_to_send} message.
The communication agent takes this message and extracts the
\mpids{MPID_Hid_ok_to_send}{request_id} field and finds the associate
request.  It then initiates a stream send.  This consists of:
\begin{enumerate}
\item Consult \mpidconst{MPID_Segment} for the next group of contiguous bytes
  to send.  If the user's datatype is not contiguous, pack into contiguous
  location with \mpidfunc{MPID_Pack}.  Update the segment to indicate the
  current position in the buffer (by maintaining the dataloop stack). 

\item Use \mpidfunc{MPID_Rhcv} to return the data.  For methods that use Unix
  sockets, \code{writev}\index{writev} can be used to send the data.  The
  message type for the data is \mpidconst{MPID_Hid_data}; this must contain
  the receiver's request id so that the destination can be identified.

\item Question:  Double buffering is possible, where the Segment code is given
  an opportunity to prepare the next buffer before the next
  \mpidconst{MPID_Hid_ok_to_send} message arrives.  Do we want to do this?
  Only sometimes?

\end{enumerate}
These steps are repeated until all data is sent (each step happens only in
response to receiving an \mpidconst{MPID_Hid_ok_to_send} packet).

At the receiving process, data arrives as \mpidconst{MPID_Hid_data} type.  
Using the \mpids{MPID_Hid_data}{recv_request_id}, this is matched to a request
and thus to a segment.  Typically, the data is received into a contiguous
buffer which is then handed, with the segment, to \mpidfunc{MPID_Unpack}.
Once the data unpack completes, a \mpidconst{MPID_Hid_ok_to_send} request is
returned to the sender using \mpidfunc{MPID_Rhcv} unless all of the message
has been received.

An alternative to providing a handshake here is for the sender to send
\mpidconst{MPID_Hid_data} messages as fast as possible, pausing only when it
is unable to send more data.  In this varient, only only
\mpidconst{MPID_Hid_request_to_send} message is sent from the receiver to the
sender. 

\end{tcp}

\begin{shmem}
(not complete)

One approach to a shared memory device is to implement a \tcpname-set of
operations for all control messages (the \mpidconst{MPID_Hid_xxx}), but
arrange for data exchanges to use shared memory.  The description here takes
this one step further; the message queues are in shared memory, allowing any
process to update the queues.

\mpifunc{MPI_Irecv}:
The initial steps are the same as in the \tcpname\ case.  A major difference
is caused by the fact that the requests can be (and we will assume) are
present in shared memory.  In this case, each process can check for a matching
queue element.


\mpifunc{MPI_Isend}:

Check remote queue for matching receive request
\begin{enumerate}
\item If not found, insert, set the matching data including the sender's
  request id, use \mpidfunc{MPID_Write_ordered} to clear the busy flag, and
  return. 
\item If found, match (remove from queue) and begin transfer.  There are
  several cases:
    \begin{enumerate}
    \item The destination buffer is in shared memory and the datatype is
      simple (not the case in this example, but one that must be considered). 
      In this case, copy to the destination buffer (basically
      \mpidfunc{MPID_Pack}, followed by \mpidfunc{MPID_Write_ordered} to set
      the completed flag in the receive request.  Also mark the send request
      as completed.
    \item The destination buffer is not in shared memory, but the total
      message length is small.  In this case, use \mpidfunc{MPID_Pack} to pack
      the message into a shared-memory buffer (Question: who allocates this?
      The sender?  The receiver?) and mark the send request as complete.
      Use \mpidfunc{MPID_Write_ordered} to mark the receive request as having
      the data but not yet complete (e.g., a final step is needed to move the
      data from shared memory into the user's buffer).
    \item The destination buffer is not in shared memory and the message
      length is large.  In this case, the message must be delivered in
      segments, using shared memory to effect the transfer.  One option is to
      place the first segment into a designated shared-memory buffer (who
      allocates it?) and indicate its location in the receive request.  All
      further transfers will be accomplished with the communication agent.
    \end{enumerate}
\end{enumerate}

\mpifunc{MPI_Wait}:

Communication agent:

The communication agent is responsible for making progress on a stream (the
sending of segments).  

Question: How do we notify the agent that we are ready for another segment?
An easy way may simply be to send a message, following the approach in
\tcpname. 
\end{shmem}

\begin{via}
(not done)
\mpifunc{MPI_Irecv}:

\mpifunc{MPI_Isend}:

\mpifunc{MPI_Wait}:

Communication agent:

\end{via}
\end{mmadi}
