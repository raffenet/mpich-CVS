\subsubsection{Scenarios}
\label{sec:pt-2-pt-scenarios}

To best understand how the point-to-point communication routines work, we will
describe several scenarios that illustrate how various communication methods
may implement communication.  We start with one of the more complex cases and
then discuss optimizations for special cases such as sending and receiving
contiguous messages.

\paragraph{Nonblocking Send and Receive with Complex Datatypes.}
This represents one of the more complex cases; most communication will offer
some opportunities for exploiting special cases such as simple datatypes
representing contiguous data or blocking communication.  

Question: this discussion assumes that only contiguous data can be handled.
We may want to consider the more general \code{iovec} (array of structures
contain a pointer and length in bytes for each member).  However, this is not
an efficient way to handle common vector datatypes or fixed blocksize hindexed
datatypes.  How important is it to handle the case where a modest-sized
\code{iovec} array can be used?  

The datatype is assumed to be complex enough that the device cannot handle it
directly; for example, it may be an hindexed type with a large number (e.g.,
10000) of entries, or a simple resized struct datatype and a large count.
The code for this scenario may look something like this:
\begin{verbatim}
req = MPI_REQUEST_NULL;
if (rank == dest) {
    MPI_Irecv( buffer, count1, datatype1, tag, source, comm, &req );
}
else if (rank == source) {
    MPI_Isend( buffer, count2, datatype2, tag, dest, comm, &req );
}
MPI_Wait( &req, &status );
\end{verbatim}

% In the following, a \emph{segment} is a description of part of the data, for
% example, the first 32k bytes.  A \emph{stream} is communication made up of
% segments.

%   Question: there is no explicit stream here; that is, the stream handling is
%   done through calls to \mpidfunc{MPID_Rhcv} and the way in which the
%   communication agent responds to those messages.  Does it make sense to have
%   an explicit notion of a stream here?  Note that we do want something more
%   that we have here for some of the collective routines such as
%   \mpifunc{MPI_Bcast} (resend data before unpacking) and
%   \mpifunc{MPI_Allreduce} (operate on data before resending).

%\subsubsection{Implementation of Point-to-Point}
%\label{sec:pt2pt-implementation}
%This section is a placeholder for pseudocode for the various routines.  They
%are organized by method.

The following sample implementation assumes a homogeneous system (all
processes use the same data representation).  

\paragraph{\tcpname.}
This method is a prototype for a TCP method.  

The basic communication is provided by
\begin{enumerate}
\item Buffered read and write.  These are roughly like the Unix
\code{fread} and \code{fwrite}.  Just as for those routines, using buffered
routines can reduce the number of system calls and data motion.  

\item Stream read and write.  These provide a way to receive and send data that
may not be in contiguous locations.  Roughly, a stream used for
    sending data:
    \begin{enumerate}
    \item packs some number of bytes into a contiguous buffer
    \item writes those bytes to a socket
    \item when all of those bytes are written, returns to step 1 to
    get the next group of bytes
    \item when all bytes are written, a completion routine is called
    that may, for example, free the buffer used to pack data.
    \end{enumerate}

    All of this happens without any further action by the upper levels
    of the code.  A special case of a stream is one in which the data
    is contiguous; in this case, no intermediate copy of the data is
    made.

    A stream is initialized by providing a \code{segment}.  A
    \code{segment} is used to handle packing and unpacking of general 
    datatypes, and is initialized with the usual MPI tuple of
    \code{buffer} address, \code{count}, and \code{datatype}.  The
    actual \code{segment} data structure is always part of a larger 
    structure, such as an \mpidconst{MPID_Request}.  In addition, 
    a local buffer is allocated (if necessary) to be used for packing
    or unpacking.  (In other methods, such as the \shmemname, this
    memory may be special, such as memory shared between processes or
    registered to a network driver.)  

    A stream is sent by specifying a destination process.  All streams
    start with a \mpidconst{MPID_Hid_data} packet that contains a
    matching request number.

    A stream used for reading data is similar, with data read into a
    contiguous buffer and then unpacked instead.  In addition, and bytes in
    the read buffer are used first; then one or more calls to \code{read} are
    used for the remainder of the data.  A further refinement is to allow the
    use of \code{readv} once the read buffer is flushed.  

    Question: We could use \code{MSG_PEEK} and \code{recv} instead of
    \code{read} to see how much data is available without reading it.  Can we
    use this, or is it better to just read some of the data (one system call)?


  Question: Were is the buffer to pack into allocated?  Is this part of the 
  \mpidconst{MPID_Segment} structure?  Part of the \mpidconst{MPID_Stream}
  structure? How do we make sure that the choice is
  good for this particular method (e.g., are segments allocated with a source
  or destination rank and a group/communicator)?  Note that the ADI-3 provides
  a \mpidfunc{MPID_Segment_init_pack} (and an unpack) that takes the
  communicator and rank for the destination.  In a multi-method device, each
  device must provide this function (unless it is happy with generic memory).

  Note that it is very useful to know if a datatype represents contiguous
  data; a \mpids{MPI_Datatype}{contiguous} flag value may be helpful.  Note
  that in the case of resized datatypes, a type that is contiguous with a
  \code{count} of $1$ may not be contiguous with a \code{count} greater than
  one.  Do we want a separate flag for that?  

    A refinement that is not used here is to allow for \emph{double
    buffering}; in the case of a stream used for sending, the next
    group of bytes to be sent is packed before the first is entirely
    sent, overlapping the communication with the packing operation.

\item Direct read and write.  These are roughly like the Unix
  \code{read} and \code{write}.  They are not used directly, but are a special
  case of the stream read and write.

\item Socket agent.  This can be thought of as a \code{select}
    or \code{poll} loop that advances the communication.  In the case
    of buffered write, for example, it sends more data whenever the
    socket becomes available for writing.

    ``Whenever'' should be interpreted loosely.  If the socket
    agent is running in a separate thread, this may happen when the
    thread is scheduled.  If a polling implementation is used, this
    will happen when the agent is called.

    Question: Can we use \code{poll} always for Unix?  Should we?  If not, how
    do we want to choose between \code{poll} and \code{select}?

\end{enumerate}

These are the low-level communication services that are used to
implemement a TCP version of a two-sided communication method.
These can be implemented with an ``active queue'' system.  In this
approach, each I/O operation adds an element to a FIFO queue.  Each
file descriptor has separate read and write queues.  As operations
complete, the queue element is removed.  Special operations, such as
streams, stay in their position in the queue until all data has been
transfered.  This allows us to use the underlying flow control in TCP,
rather than adding an extra layer of our own, when moving large
amounts of data.

When a low-level communication queue element is removed, a special handler is
invoked.  The 
most common handler is the do-nothing handler (actually a \code{NULL}, so no
function call is made).  The second most common
sets a flag to one; this is normally used to set the
\code{request_ptr->complete} flag.  For efficiency, this could be a
special case of the \code{NULL} handler with a non-null argument.
For a stream involving a noncontiguous datatype, it could call a true
routine to free up any temporary buffers.

\implementation{{General Notes}}
The basic structures include the following:
\begin{verbatim}
typedef struct { void *ptr;
     MPID_Msg_format msg_format;   /* Only for heterogeneous systems */
     } MPID_Eager_info;   /* Info for eager delivery */
typedef struct { 
     int sender_id;
     int receiver_id;
} MPID_Rndv_info;
typedef struct {
     void          *ptr;
     int           count;
     MPID_Datatype *dtype;
} MPID_Buf_info;

typedef enum { MPID_REQ_ISEND, MPID_REQ_IRECV, MPID_REQ_PERSIST_ISEND,
               MPID_REQ_PERSIST_IRECV, MPID_REQ_USER } MPID_Request_kind;

/* All packet types start with the same values */
typedef struct {
  short kind;
  short len;
} MPID_Hid_general_t;

typedef union {
  MPID_Hid_general_t         general;
  MPID_Hid_data_t            data;
  MPID_Hid_request_to_send_t rtosend;
  ...
} MPID_Packet;
typedef struct {
     MPID_Request_kind kind;
     volatile int      busy;
     int               self;    /* integer id of request */
     int               context_id;
     MPI_Status        status;  /* Stores tag, source, error, and n bytes */
     ...
     MPID_Eager_info   eager;
     MPID_Rndv_info    rndv;
     MPID_Buf_info     buf;
     ...
     MPID_Packet       packet;
     } MPID_Request;
\end{verbatim}

In the case where there are multiple threads, it is necessary to ensure that
only one thread (usually) modifies a request at a time.  This can be
accomplished using (thread) locks or using flags (e.g., a \code{busy} flag).
However, if flags are used, it is necessary to ensure that at pending writes
to memory on the process are completed before the flag is updated.  The macro
\mpidfunc{MPID_MemWrite_ordered} can be used for this.  
For example, for an Alpha processor, \mpidfunc{MPID_MemWrite_ordered} 
might look like this \cite{alpha-asm}:
\begin{verbatim}
#define MPID_MemWrite_ordered(var,value) { \
      asm volatile ("wmb":/*no output*/:/*no input*/); var = value ; }
\end{verbatim}

Note that in OpenMP, this can use the \code{flush} directive, although that
may require a routine call, since you can't include a \code{\#pragma} within a
\code{\#define}.  In this case, \mpidfunc{MPID_MemWrite_ordered} would be a
function, with the definition:
\begin{verbatim}
void MPID_MemWrite_ordered( int *var, int value )
{
    #pragma omp flush
    *var = value;
}
\end{verbatim}

Question: are there any systems where it helps to flush only
specific variables  (OpenMP allows this)?

Request allocation:

Question: In a multithreaded case, the request id's could be partitioned
among the threads, avoiding the need for a thread lock.  Do we want to
consider that?  This is complicated by the fact that threads are not
required to register themselves with MPI (e.g., with a call to something
like \code{MPI_Thread_initialize}).\index{thread overhead!request allocation}

\implementation{{MPI_Irecv}}

\fileinclude{samples/irecv_tcp.c}

Notes:
\begin{enumerate}
\item \mpidfunc{MPID_Request_recv_FOA} always returns a request.  If the
  request was found (and not \code{busy}), it is removed from the queue.  If
  one was not found, it is inserted and marked as busy.  In either case, 
  the \code{status} fields are filled in, along with the \code{context_id}.
  This ensures that another thread that calls \mpidfunc{MPID_Request_recv_FOA}
  will match this inserted request, but, because of the
  \code{busy}\index{thread overhead!request use} field,
  will wait until the request has been fully filled in.  An alternative is to
  use a thread lock on the request, but that is really unnecessary since only
  two threads (the one that inserts and the one the removes) will ever need to
  synchronize access to the request.

  Question: the following assumes that the fields in
  \mpids{MPID_Request}{status} are already filled in.  Is that what we want?

  Question: Should the \code{busy} flag be an integer or just a bit in the
  request flags?

\item If a matching request was found, there are two cases.  Either the
  message was delivered eagerly or a rendezvous message was sent. Note that
  in either case the request has been removed from the receive queue already 
  by \mpidfunc{MPID_Request_recv_FOA}.

\item The first check is against the total size of the user's receive buffer.
  In the homogenous case, the size of the total message (saved in the
  \mpids{MPID_Request}{status.count} field of \mpidconst{MPID_Request}) is
  compared with \code{count} times the size in bytes of the user's
  \code{datatype}.  The original size is saved in \code{msg_size} so that the
  eager buffer can be freed and flow control updated.
  If the user's buffer is too small, invokes \mpidfunc{MPID_Err_create_code}
  to return a convenient message (using predefined text).

\item The next test is for the special case of a contiguous buffer.  In this
  case, we can use \code{memcpy} directly.

  Question: do we want to use a private \code{memcpy} that might use, for 
  example, quadword load and store instructions that some system libraries
  might not use?

\item Otherwise, the user's buffer is not contiguous, and the data must be
  unpacked.  This is a simple case since all of the data is present, and 
  can be handled with \mpidfunc{MPID_Unpack}.

  Question: note that \mpidfunc{MPID_Unpack} has slightly different arguments
  from the one in the ADI-3 document.  Are these the correct ones?  In a
  homogeneous system, the \code{msg_format} value is never needed.

\item Once the data has been moved from the eager buffer, the eager buffer 
 can be freed.  This call also updates any flow control information (e.g.,
 updating the number of free eager memory buffers available).  The request is
 now marked as complete.

  Question: Are there other values that we want to mark here?  For example, 
  a one in \code{complete} may mean ``data received but cleanup not down'' and
  a two might mean ``finished''.

\item If the request was found but the data was not available, we need to ask
  the sender to send us the data.  We do this with the \code{packet} that is
  within the \mpidconst{MPID_Request}.  We use the integer index of the
  request rather than its address because it is both addressing independent
  (e.g., on heterogeneous systems, we don't need to worry about mixes of 32
  and 64 bit pointers) and may even be shorter (e.g., we could use a 16 bit
  \code{short} for the value).  We send the request id of the receiving
  request back to the sender so that the data can be properly stored when it
  arrives (see the discussion of \mpidconst{MPID_Hid_data} in the
  communication agent).

  Question: if the message is truncated (user's buffer is too small), do 
  we want to tell the sender?

  Question: do we want to remember the source, rather than having to look up
  the source again through the communicator?

  Question: in the case that the receive data is contiguous, we could just
  send the sender the memory location of where to deposit the data.  Do we
  want to do that here or leave that for the remote memory case?

  Note that \code{MPID_Rhcv_tcp} must remember \code{vector} if it can't send
  the data immediately (e.g., because the socket is full).  However, if it can
  send it immediately, no copy needs ever be made.

\item If no matching request is found, the information needed to store the data
  when it does arrive must be saved.  We must also increment the reference
  counters on the datatype and the communicator to ensure that they aren't
  deleted before the communication completes.
  The \mpifunc{MPI_Irecv} can now return.  Completion of the message transfer
  is now the responsibility of the communication agent. 

  Question: Should we go ahead and create the segment unpack, at least for 
  non-contiguous datatypes?

  Question: In the \mpifunc{MPI_Recv} case, we never need to increment the
  datatype and communicator reference counters.  Is there any significant 
  benefit to avoiding that step, particularly for very short messages?
\end{enumerate}

\implementation{{MPI_Isend}}

\fileinclude{samples/isend_tcp.c}

Notes:
\begin{enumerate}
\item For now, we assume that there are no ``speculative receives,'' that is,
  a receive that sends a message to the designated source of the message
  indicating that the receiver is prepared to receive a message.  Thus, 
  \mpidfunc{MPID_Request_send_FOA} always allocates, never finds, a request.

\item There are two cases.  Short messages are sent eagerly if flow control
  allows it; all other messages are sent by rendezvous.  
  The function \mpidfunc{MPID_Flow_limit} both tests the message size against
  the flow control limits, and if the limits allows it, updates the flow
  control limits to reserve that space.  This is needed for multithreaded
  codes to ensure that two threads do not both reserve the same eager buffer
  space.  

\item In the first case, the message is too large to send.  In this case,
  information on the message data is saved and a request to send packet is
  sent using \code{MPID_Rhcv_tcp}.  Note that the reference counts for the
  communicator and datatype are incremented to ensure that they are not
  deleted until the message is sent.

  If speculative receives are implemented, \mpidfunc{MPID_MemWrite_ordered}
  should be called at the end of this code to clear the \code{busy} flag.

  Question: do we really need to save the communicator?  We don't need the
  \code{context_id}, and only need the communicator to convert a rank in to a
  specific connection.  If we save the connection, we don't need to increment
  the reference count on the communicator.

\item In the other case, the message may be sent eagerly.  There are two
  subcases here.  In the first, the message is either contiguous or small
  enough to be packed into a single temporary buffer.  The other case is a
  larger message.  We could ignore the larger message case by setting the
  eager limit to a small value, but we still would want to handle the large
  message case for \code{MPI_Rsend}, so we might as well do it here.

\item In the case where the data is contiguous and is sent directly with
  \code{MPID_Rhcv_tcp}, the request is marked complete only when the data has
  been sent.  In the case where data is packed up, the request is complete as
  soon as the \code{MPID_Rhcv_tcp} call is made since the user's buffer is now
  available for reuse.  

\item In the case where the data is longer and noncontiguous, we must send it
  using incremental packing.  This uses a special data transfer routine,
  \mpidfunc{MPID_Stream_isend}.

\item We can generalize the contiguous case to datatypes that are described by
  short arrays of \code{struct iovec}, for example, seven elements or less.
  This allows the header to use one element and the datatype the remaining
  seven.  Longer arrays may not be handled efficiently by the operating
  system. 

\end{enumerate}

\implementation{{Communication Agent}}
In a multi-method device, it is often easiest if each method's communication 
agent runs in a separate thread.  That allows the operating system to 
schedule the threads.  If the agents are running in the same thread (and 
particularly, if they are running in the main thread in a polling-mode 
implementation), each must normally be executed in the \code{NONBLOCKING} (or 
\code{EXPECTING}) mode.

\fileinclude{samples/agent_tcp.c}

Notes:

\begin{enumerate}
\item The routine \code{GetNextPacket} returns a pointer to the next packet of
  data, along with the \code{source} of the message.  This routine guarantees
  that the entire packet header (the first \code{vector} element sent with
  \mpidfunc{MPID_Rhcv_tcp}) has been delivered.  It does this by reading the
  first two shorts; the second of these is the length of the packet header.
  If less than a full packet has been delivered, \code{GetNextPacket} does not
  return a packet.  This simplifies the handling in the rest of the
  communication agent without requiring that \code{GetNextPacket} understand
  the different packet types.

  Question: what is the source value?  Is it the local process id (e.g., the
  connection number)?

\item The blocking parameter to \code{GetNextPacket} has the following
  meanings:
  \begin{description}
  \item[\code{MPID_BLOCKING}]Block until a packet is available.
  \item[\code{MPID_NONBLOCKING}]Return a packet if one is available, otherwise
    return \code{NULL}.
  \item[\code{MPID_EXPECTING}]Like \code{MPID_NONBLOCKING}, except
    \code{GetNextPacket} may wait for a short time before returning.  For
    example, it may wait the time it takes a message to make a round trip in a
    case where a response is expected from another process.
  \end{description}

\item When \code{GetNextPacket} returns with a packet, the data that
  \code{packet} points at is valid until the next time that
  \code{GetNextPacket} is called.  This allows \code{GetNextPacket} to read
  into an internal buffer and return a pointer into that buffer, avoiding an
  extra memory copy.  This works only if only one thread at a time calls the
  communication agent.  However, the restriction that only one thread call the
  communication agent at a time is fairly natural.  In fact, the MPI thread
  mode \mpiconst{MPI_THREAD_SERIALIZED} expresses this; in such a case, the
  communication agent does not need a acquire a thread lock in the case where
  many threads may call the communication agent (e.g., in a polling mode
  implementation where there is no separate thread running the communication
  agent).

\item \code{GetNextPacket} must also keep track of the total message size that
  a packet starts, so that subsequent calls to \code{GetNextPacket} will not
  interpret data as a packet header.  This is covered in more detail under
  \mpidconst{MPID_Hid_data}.  A consequence of this is that
  \code{GetNextPacket} might process additional data before returning a new
  packet.

  Note: an alternative is for \code{GetNextPacket} to return a special
  kind of packet for streaming data, allowing the communication agent to
  process data as it is read.  The reason that we don't do this is that for
  some kinds of messages, we want to bypass any buffering, and that requires
  putting some message handling into the lower level communication functions.
  Question: Do we want to change the name to something that indicates that it
  does more than just read a packet?

\item Question: Is \code{GetNextPacket} fair?  How do we ensure that
  \mpifunc{MPI_Testsome} is efficiently implemented?

\item Under Linux, very short messages are often delayed; it is advantageous
  to always send at least 100 bytes.  We could accomodate this by either
  padding all of the packet types out to 100 bytes or always sending at least
  100 bytes with \code{MPID_Rhcv_tcp}.  In that case, we must ensure that
  \code{GetNextPacket} knows to skip over the ``extra'' bytes, possibly by
  changing the \code{len} field in the \code{packet} and sending the extra
  bytes.  

\item If we use a test on \mpidconst{MPID_THREAD_LEVEL} to check to see if we
  need to lock the agent, we also need to use that same mutex in any routine
  that we might provide to change the thread level.  If we only permit the
  choice of thread level within \mpifunc{MPI_Thread_init}, then we don't need
  to worry about this.

\item The sample code shows inlined code for each packet type.  An
  implementation is more likely to use functions to handle each packet type,
  at least for the less common cases (e.g., \mpidconst{MPID_Hid_control} and
  error return on a ready-send).
\end{enumerate}

\mpidconst{MPID_Hid_eager}:
\begin{enumerate}
\item This is a message followed by data.  The \code{lpacket} points to the
  packet header.  Note that while the data is immediately behind the packet,
  there is no guarantee that it has arrived yet (more on this below).

\item We use an array reference on the \code{context_id} to find the
  corresponding communicator.  Another approach would be to store the
  communicator pointer in the request, and allow the matching logic in
  \mpidfunc{MPID_Request_recv_FOA} find the corresponding request.  
  If \mpidfunc{MPID_Request_recv_FOA} needs the communicator (e.g., in the
  multimethod case to find the appropriate method queue), it could do the
  lookup on the \code{context_id} on its own.

\item There are two cases here depending on whether a matching request was
  found by \mpidfunc{MPID_Request_recv_FOA}.  

\item If a request was found, there is an available user buffer.  We need to
  tell the socket layer to transfer data to that buffer.  This is relatively
  easy if the datatype is contiguous (and only slightly more difficult if it
  is easily described by an \code{iovec}).  Handling the more general datatype
  case requires a little more care.  In either case, we need to tell the
  socket layer where the data should be put, how many bytes to process, and
  how to move them.  This is a \emph{stream}; the low-level code is
  responsible for handling this.

%             /* Initiate an unpack.  Note that the data may not all 
%                have arrived yet.  This must:
%                1. Process all available data (up to lpacket->msg_bytes)
%                2. If not all data is available, indicate to low-level
%                   communication that the data is stream.
%                3. Indicate to stream what field to set in the request
%                   when all data is read and unpacked.
%             */

\item If a request was not found, then one was allocated and returned by
  \mpidfunc{MPID_Request_recv_FOA}.  

\item Check to see if this is a ready-send.  If so, then there is an error
  because a matching request was not found.  Return an error indication to the
  sending process and tell the socket layer to discard the data.

  Question: do we also want to generate an error message?  Do we want to
  invoke the error handler on the intended communicator on the receiving
  process?  

\item If it isn't a ready-send, then we need to allocate space to hold the
  data and copy it in.  We allocate the space with
  \mpidfunc{MPID_EagerAlloc}. 

  We cannot
  use \code{malloc} instead of \mpidfunc{MPID_EagerAlloc}, at least on all
  platforms (and we must enforce 
  resource limits) because the \code{malloc} might fail, and our flow-control
  promise is that there is space available.  If we depend on \code{malloc}, we
  need to provide for a negative acknowledgement on eager messages.

  Note: we could leave the data in the internal buffer at least briefly,
  avoiding one copy.  The added complexity of remembering where to move the
  data when we needed to probably outweighs the small gain in efficiency.

\item Once the data is allocated (the allocation always succeeds as long as
  the flow control algorithm is correct), a stream is set up to receive the
  data as bytes into the allocated buffer.  

\item Once all of the bytes of the stream have been read, the function
  \code{MPID_Eager_complete_func}, which was passed to
  \mpidfunc{MPID_Stream_irecv_tcp}, is called with the fifth argument of
  \mpidfunc{MPID_Stream_irecv_tcp}, which is the pointer to the request.  This
  allows the communication agent to 
  \begin{enumerate}
  \item Indicate that the data is available by setting the eager buffer
    pointer
  \item Clear the busy flag
  \item For multithreaded processes, where one thread is waiting on this
    request, signal the waiting thread.
  \end{enumerate}

\end{enumerate}

\mpidconst{MPID_Hid_request_to_send}:
\begin{enumerate}
\item This starts in the same was as \mpidconst{MPID_Hid_eager}, with a call
  to \mpidfunc{MPID_Request_recv_FOA}.  

\item If the request is found, this executes the same steps as in the
  \mpifunc{MPI_Irecv} case for found and not an eager message (e.g., it
  returns an \mpidconst{MPID_Hid_ok_to_send} packet).

\item Otherwise, it saves the information needed to request the data at a
  later time. Note
  that the \mpids{MPID_Hid_request_to_send}{length} field is needed to
  implement \mpifunc{MPI_Iprobe}. 
\end{enumerate}

\mpidconst{MPID_Hid_ok_to_send}:
\begin{enumerate}
\item First, the request is identified from the request id in the packet.

\item The data will be sent with a packet header of
  \mpidconst{MPID_Hid_data}.  There are two subcases:
  \begin{enumerate}
  \item The data is contiguous.  In this case, a simple \code{MPID_Rhcv_tcp}
    call can send all of the data.

  \item The data is not contiguous.  In this case, we create a message
    stream.  

   Question: do we need to call a routine to release any stream or segment
   resources when we are done?

  \end{enumerate}
\end{enumerate}

\mpidconst{MPID_Hid_data}:
\begin{enumerate}
\item This indicates data that matches a request.  The request id is part of
  the packet header.  The data must be received with a stream since all of the
  data may not be available yet.  

  Question: Do we need to call a routine at completion to release any stream
  or segment resources when we are done?

\end{enumerate}

\mpidconst{MPID_Hid_control}:
\begin{enumerate}
\item This case is for miscellaneous control messages.  The only control
  message defined so far is the ready-send error.
\end{enumerate}

\implementation{{MPI_Wait}}

\fileinclude{samples/wait_tcp.c}

Notes:
\begin{enumerate}
\item In the single-threaded case, we need only check the
  \mpids{MPID_Request}{complete} flag. 
  If set, then copy out the \mpiconst{MPI_Status} data (if the \code{status}
  pointer is not \mpiconst{MPI_STATUS_NULL}) and free the request.  Otherwise, 
  call the communication agent in blocking mode (wait until something happens)
  and then check again.

\item In the multi-threaded case, if the request is not complete, we want to
  transfer control to another thread, particularly to the thread running the
  communication agent if there is one, so that the message can be completed.
  This allows the implementation to avoid spinning on the
  \mpids{MPID_Request}{complete} flag.  However\index{thread overhead!request
    completion}, it does force the use of a thread mutex to guard the
  \code{complete} flag and a thread condition variable to release the
  particular request.  

  The sample code shows the use of condition variables with pthreads.
  The routine \code{pthread_cond_wait} atomically releases the
  \code{request_mutex} and 
  waits for the condition variable \code{cond}.  The condition variable is set
  when another thread executes \code{pthread_cond_broadcast} (or
  \code{pthread_cond_signal} to this particular thread).
  Note that this algorithm requires either a mutex per request or (less
  scalably) a mutex 
  on all requests (but local to the calling MPI process).  
  Note that a single mutex may be more appropriate for the multiple completion
  routines such as \mpifunc{MPI_Waitsome} or \mpifunc{MPI_Waitall}.
  The above approach also requires that the communication agent execute a
  \code{pthread_cond_signal} to release this waiting thread.  Note that by
  storing the thread id of the waiting process, we can avoid calling
  \code{pthread_cond_signal} when no process is waiting.

  Another alternative is to simply busy wait on \mpids{MPI_Request}{completed}.
  
  Note that pthreads are not available on all platforms; we may need to
  implement other approaches as well.
  
  Question: How well do pthread condition variables work on our important Unix
  platforms (e.g., Linux, Solaris, AIX, etc.)?  Is there a similar Windows NT
  approach, or is something different required?

\end{enumerate}

\pagerule\par
The remaining text is leftover from a previous version and will be mined as
appropriate for new text.  Read at your own peril.

\pagerule\par

\paragraph{\shmemname}
(not done)

We still need streams here, but since we're responsible for
everything, we may need to send messages back and forth for each
segment.  This may add an additional message type,
\mpidconst{MPID_Hid_next_data_t}.  

One approach to a shared memory device is to implement a \tcpname-set of
operations for all control messages (the \mpidconst{MPID_Hid_xxx}), but
arrange for data exchanges to use shared memory.  The description here takes
this one step further; the message queues are in shared memory, allowing any
process to update the queues.

\mpifunc{MPI_Irecv}:
The initial steps are the same as in the \tcpname\ case.  A major difference
is caused by the fact that the requests can be (and we will assume are)
present in shared memory.  In this case, each process can check for a matching
queue element directly. The two cases are
\begin{enumerate}
\item The request was not found.  This is the same as the \tcpname\ case.
\item The request was found.  This is close to the \tcpname\ case,
  particularly for the complex-datatype case.  The particular cases are
  \begin{enumerate}
  \item The data has already been delivered.  It is either within the request
    (very short) or in a shared-data buffer used for eager messages.  As in
    the \tcpname\ case, copy the data into the user's buffer using
    \mpidfunc{MPID_Unpack}.  Note that in the multi-method case, if the device
    is heterogeneous, we still need the \mpidconst{recved_format} etc. fields
    since the data may have been packed for a communicator containing
    processes with different data represenations.
    Question: who frees the eager buffer space?  Does the receiver free it
    after unpacking the data, or do we ask the sender to free it?  We might
    want the sender to free it if the sender allocated it.

  \item The data has not been delivered.  This follows the \tcpname\ case,
    since the data must be delivered by having the sender place some of it in
    shared memory, then signal the receiving process, and so on.  The
    receiving process creates the corresponding segment (including one or more
    buffers in shared memory?) and sends the address and size of that buffer
    to the sender using \mpidfunc{MPID_Rhcv} to deliver a message to the
    sender's incoming message queue.  Further processing is handled by the
    communication agent (including any double buffering of the transfer).
  \end{enumerate}
\end{enumerate}


\mpifunc{MPI_Isend}:

Check remote queue for matching receive request (using
\mpidfunc{MPID_Request_send_FOA}?).  
\begin{enumerate}
\item If not found, insert (an unexpected receive), set the matching data
  including the sender's 
  request id, use \mpidfunc{MPID_MemWrite_ordered} to clear the busy flag, and
  return. 
\item If found, match (remove from queue) and begin transfer.  There are
  several cases:
    \begin{enumerate}
    \item The destination buffer is in shared memory and the datatype is
      simple (not the case in this example, but one that must be considered). 
      In this case, copy to the destination buffer (basically
      \mpidfunc{MPID_Pack}), followed by \mpidfunc{MPID_MemWrite_ordered} to set
      the completed flag in the receive request.  Also mark the send request
      as completed.
    \item The destination buffer is not in shared memory, but the total
      message length is small.  In this case, use \mpidfunc{MPID_Pack} to pack
      the message into a shared-memory buffer (Question: who allocates this?
      The sender?  The receiver?) and mark the send request as complete.
      Use \mpidfunc{MPID_MemWrite_ordered} to mark the receive request as having
      the data but not yet complete (e.g., a final step is needed to move the
      data from shared memory into the user's buffer).  Question: how do we
      indicate this in the request?
    \item The destination buffer is not in shared memory and the message
      length is large.  In this case, the message must be delivered in
      segments, using shared memory to effect the transfer.  One option is to
      place the first segment into a designated shared-memory buffer (who
      allocates it?) and indicate its location in the receive request.  All
      further transfers will be accomplished with the communication agent.
    \end{enumerate}
    Subsequent transfers will be handled by the communication agent.
\end{enumerate}

\mpifunc{MPI_Wait}:

Again, this is much like the \tcpname\ case.  Note, however, that the
\mpids{MPID_Request}{complete} field can, in some cases, be set by the
sending process (particularly for contiguous receive buffers that are in
shared memory), so this field must be marked \code{volatile}.

Also note the case that all of the data (or the last segment) has been
delivered (by being placed 
into shared memory), but the final \mpidconst{MPID_Unpack} to move that data
into the user's buffer has not been performed.  How is this indicated?  In the
\tcpname\ case, this is handled with the explicit delivery of an
\mpidconst{MPID_Hid_data} message.  In the \shmemname\ case, do we indicate
this with a bit or field in the \mpidconst{MPID_Request}?  For example, is
there a \mpids{MPID_Request}{data_available} field that is used to indicate
that memory is available?  This field could be used instead of an explicit
message to manage the communication.  

Question: If we use a \code{data_available} field, how do we avoid
busy-waiting?  Do we simply poll and yield?

\texttt{Communication agent:}

The communication agent is responsible for making progress on a stream (the
sending of segments).  There is no special action on the receiving end.  On
the sending end, when the \mpidconst{MPID_Hid_ok_to_send} is received, the
send is started.  Note that a handshake is required on the transfer of data
from the sender to the receiver because the buffer that is used for the
transfer must be explicitly filled by the sender and emptied by the receiver.
Question: Do we want to double buffer?  How does that change the messages
exchanged between the sender and receiver?

If the communication agent is in a separate thread, we can wake up, spin a
little (checking the incoming message queue), and yield.  If single-threaded,
we should spin for roughly the round-trip message time (an internal message,
not an MPI message) and then yield (on Linux, with \code{sched_yield}).
Question: do we want the option of using something like the System V
semaphores to implement a condition variable for the communication agent, so
that updates to the incoming message queues would wake up the agent?

Question: How do we notify the agent that we are ready for another segment?
Does the communication agent wait on something?  What?  What about the case
where there are 100 pending requests?
An easy way may simply be to send a message, following the approach in
\tcpname.  A more complex approach, introduced in the discussion of
\mpifunc{MPI_Wait} above, is to use a field within the request (or even in the
shared-memory transfer area itself) to indicate that a segment has been copied
into or out of shared memory.  

Note that waiting on fields in individual request items is ok in this simple
example, but becomes less suitable when there are a significant number of
pending requests.  Question: we could establish a bit vector of pending
requests (each bit represents a request and the position matches the index);
this could be atomically updated into indicate that a process's 
request was ready for operation.  However, this may not be much better than
sending a message to the communication agent.

\paragraph{\shmemallname}
(not done)

Note that in this case (all memory available), the simple transfers can be
made directly, once the send and receive requests are matched.  Also simple
are the cases where either the sender or the receiver provides a contiguous
datatype; in that case, the other process executes either
\mpidfunc{MPID_Unpack} \mpidfunc{MPID_Pack}.  Only in the case where both
processes specify complex datatypes may it be necessary to transfer data
through a cannonical, contiguous representation.

\paragraph{\vianame}
(not done)

This method is very similar to the \tcpname\ method, with the difference that
data can be written into and read from remote memory that has been registered.

\mpifunc{MPI_Irecv}:

Follow the approach in \tcpname.  
If a message must be sent (e.g., a
\mpidconst{MPID_Hid_ok_to_send}), \mpidfunc{MPID_Rhcv} sends it by writing to
a pre-registered location.  Flow control provides information on where to
write (e.g., multiple incoming locations can be allocated, and either
information on each message updates what is free or separate flow control
messages are sent).   

Note that because the data is noncontiguous, it must be delivered into some
intermediate memory in contiguous form\footnote{We assume that either the
  method cannot handle any noncontiguous data or that the noncontiguous
  datatype in this example cannot be directly handled.}.  We further assume
that memory to be used for such transfers has already been registered; the
location of that memory can be communicated back to the sender in the
\mpidconst{MPID_Hid_ok_to_send} message.  

Question: who manages this pool of pre-registered memory?  Note that since
target memory must be registered, the receiver must be the one that specifies
this.  

Further processing is handled by the communication agent.

\mpifunc{MPI_Isend}:

Follow the approach in \tcpname.

Note that since we don't know whether the destination is going to provide a
contiguous, registered-memory buffer or not, we shouldn't take the step of
transfering the first segment into an internal registered-memory buffer.

\mpifunc{MPI_Wait}:

Follow the approach in \tcpname.  

\texttt{Communication agent:}

The agent must respond to messages in much the same way as \tcpname.  However,
the process of sending a stream of segments may be different. 

Question: how do we want to tell the process at the other end of the stream to
read the current block of data?

\paragraph{Nonblocking Send and Receive with Contiguous Data.}
(not complete)

The major difference in this case is for the \shmemname\ and \vianame\ cases,
where it may be possible to send data directly from one user buffer to
another.  

\begin{via}
  Question: How do we indicate that a message has been transfered?  Do we want
  to use a remote write to set the \mpids{MPID_Request}{complete} field
  directly, rather than sending a message?  Is it better to receive a
  completion message from the sender or to spin-wait on
  \mpids{MPID_Request}{complete}? 

\end{via}

\paragraph{Blocking Send and Receive with Noncontiguous Data.}
\label{sec:blocking-optimization}
(not complete)

Once a transfer is initiated, particularly in the \shmemname\ and \vianame\
cases, instead of sending a separate handshake message, we could set a flag
value in the transfer buffer itself, particularly for the cases involving
non-contiguous data where the transfer buffer is not the same as the user's
buffer.  

By initiated here, we mean once the first block has actually been
transferred.  This ensures that the sender has received the
\mpidconst{MPID_Hid_ok_to_send} and has started to act on it.  If the sender
knows (or has been told in the \mpidconst{MPID_Hid_ok_to_send} message) that
the receiver is blocking, it can switch to this alternate method for
indicating that data has been transferred.



