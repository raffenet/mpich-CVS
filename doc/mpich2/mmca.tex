\includeversion{comment}

\section{Multi-method Communication Architecture}

In this section, we present a multi-method communication architecture
for MPICH2.

What is a ``method'' anyway?  Why are multiple methods important?

Rather than present the entire architecture at once, we will start
with a basic system and modify it as we add capabilities.  The reason
for such a presentation is two-fold.  First, the intricate details of
the architecture are difficult to understand at first glance.  We hope
that by incrementally presenting features of the architecture, the
motivation will be clearer and the reader will be better served.
Second, the organization of this section suggests a plan for
developing a prototype implementation of the multi-method
architecture.  Many of the ideas presented within the section are as
of yet unproven, and would be benefit greatly from rapid prototyping
and performance analysis.

%------------------------------------------------------------------------------

% \subsection{Architecture Overview}

%------------------------------------------------------------------------------

\subsection{Simple Messaging}

% goal: describe components necessary for unoptimized, multi-method
% point-to-point
% 
% narrow scope for this subsection: no wildcards, persistent sends, etc...(look
% at lists below :)).  also not dealing with complex datatypes or heterogeneity.
% 

In this section we will describe the components necessary for an unoptimized,
multi-method, point-to-point (P2P) messaging system.  The scope of this system
is rather restricted with respect to MPI capabilities; wildcards, persistent
sends, request cancellation, complex datatypes, heterogeneity, and approaches
for reducing data copies will all be ignored for now.  We will address these
issues later in the document.

\subsubsection{Overview}

% major steps in point-to-point communication:
% \begin{itemize}
% \item posting
% \item progress
% \item completion
% \end{itemize}
% 
% go through one step at a time
% 
% for each step:
% - components used in that step? (also assumed knowledge?)
% - paragraph summary of what happens in that step
% 

One can break P2P communication into three major steps: message
posting, progress of transfer, and message completion.  In this section we
describe these steps at a high level and introduce some of the components that
will be involved in each step.

% scenario: send posted, recv posted, data arrives after recv posted.
% - trivial mpi program example here
% - note that there is another ordering, and we will discuss that in the
%   walkthrough
% 

For our overview we will concentrate our discussion on a simple scenario.  In
our scenario one process is sending a contiguous data region to another.  Both
are using nonblocking MPI calls, and execution order occurs such that the data
arrives after the receive has been posted.  This is of course not the only
possible ordering, and we will discuss other possibilities in
Section~\ref{sssec:walkthrough}.

\begin{comment}
  \emph{include a figure showing the actual code snippets?  definitely.}
\end{comment}

% posting (async send case)
% - allocation/initialization of MPI request (must initialize completion counter)
% - determine the vc from the communicator (so we can get to method)
% - calling of method function to post

An asynchronous send (\mpifunc{MPI_ISEND}) first results in the allocation and
initialization of a \mpids{MPI_Request} structure (described previously in
XXX).  A \emph{completion counter} is included in this structure, and it is
initialized at this point (to 1).
%

\begin{comment}
  \emph{is it a MPI_Request structure or a MPID_Request structure?  are
  \texttt{MPI_Request}s just references to \texttt{MPID_Request}s?}
\end{comment}

\begin{comment}
  \emph{structure vs. object: the standard refers to \mpids{MPI_Request}
    objects and handles that refer to them.  should we use similar
    terminology.}
\end{comment}

\begin{comment}
  \emph{should we explain why the completion counter is set to a value of 1?}
  I think that should have been addressed in a previous section, so maybe we
  should just footnote it.
\end{comment}

\begin{comment}
  \emph{do we want to say something about the buffer in here?}  The buffer does
  not become an issue until we call the method's post function passing it the
  buffer.
\end{comment}

%
Next the \emph{virtual connection} (VC) is determined from the communicator
passed into the send call.  Through the VC the appropriate method functions
for moving data on this VC are found.

%
Finally the method function for posting a send is called, passing it the buffer
and envelope information supplied to \mpifunc{MPI_SEND}.  A reference to this
function is found in the VC data structure.

\begin{comment}
  \emph{function pointer: didn't we agree that the function pointer table
  would be in the method object?  If so the above text needs to be fixed so
  that the VC is use to find the method and the method is used from that point
  forward.}
\end{comment}

% posting (async recv, posted before data arrives)
% - allocation of MPI req
% - buffers, datatypes
% - vc determination
% - calling method fn to post
% - is there anything really different from above (at this level)?

An asynchronous receive (\mpifunc{MPI_IRECV} is handled in a similar manner.  A
\mpids{MPI_Request} structure is allocated and the completion counter is
initialized (to 1).
%

\begin{comment}
  \emph{buffer?}
\end{comment}

%
The appropriate VC is determined by looking at the communicator passed to the
receive call.  From the VC the appropriate method function for posting a
receive is found and called.
 
% progress 
% - what is the issue?  we want data movement.  it's acceptable to make progress
%   only when mpi calls are made.  we might want to use threads (just mention 
%   the opportunity)
% - progress engine - motivate why we need a component to do this
% - method functions called
% - datatypes, segments (packing and unpacking mentioned in passing?)
% - request matching going on too, mention relationship to vc data structures?
% - trust us, the request matching does need to be cross-method
% 
% [something here is still bugging brian]

\begin{comment}
  \emph{is it ok to use the term operation in here?  is there a better one?  we
    could use the term ``request''.  operation isn't too confusing right now,
    but it may be later.  then again, the MPI standard talks about ``posting
    nonblocking operations''...}
\end{comment}

Progress must be made on posted operations in order for them to complete.
Progress is simply the act of moving control and data packets (should I use
this word?) on order to accomplish the desired operation.  The MPI semantics
allow for progress to only occur when MPI calls are made; however, we might
want to make progress at other times.  This could be accomplished through the
use of threads or other system capabilities (see Section XXXX).

\begin{comment}
  \emph{control and data packets: I think we should avoid using packet until
  we start discussion particular method implementations.}
\end{comment}

In our multi-method P2P communication system a \emph{progress engine}
component is responsible for orchestrating progress on all queued operations.  
This is necessary in order for the system as a whole to balance service across
methods.

Method functions are used to make progress on specific requests (operations in
progress?).  These method functions must utilize \emph{buffer} information
(user buffer location, datatype, count) in order to determine the appropriate
data regions to send or receive.

\begin{comment}
  \emph{I am having difficulty agreeing with the first sentence of the above
    paragraph.  Progress must be made on outstanding requests (operations) but
    will the function(s) involved know be informed about specific requests for
    which progress should be made?}
\end{comment}

Matching of requests occurs within the context of progress as well.  In our
example scenario, our receive was posted before any data from the sender
arrived.  The posted received is associated with the VC on which data will be
passed.  At some point that data does arrive, and the \emph{envelope} from
the incoming message must be matched to the envelope from the posted receive.

\begin{comment}
  \emph{define envelope?  point out that it should have been defined?}  briefly
  mention a definition and then point at another section for details (are there
  any details???)
\end{comment}

\begin{comment}
  \emph{we should probably mention message delivery order somewhere.  maybe
    later?}
\end{comment}

This matching process might seem trivial, but in fact the MPI semantics for
wildcard operations and message ordering combined with the possibility of
multiple methods make matching somewhat complicated.
For this reason we will implement a separate \emph{request arbitrator}
component (do you like that name?) for performing request matching for the
system as a whole (both intra- and inter-method).

\begin{comment}
  \emph{arbitrator: does it really arbitrate?  It is a cool sounding word, but
    it seems that ``request matching'' component is more descriptive of what it
    actually does.}
\end{comment}

% completion
% - notification of completion (via decrement of counter - in MPI request)
% - note that a counter becomes important when we get to collectives, kinda
%   overkill for this.
% - deallocation of MPI request?  nope.  deallocated by wait/test instead.  
%   do we want to mention this here?
% 

Once all data has been copied out of the sender's buffer (either onto the wire
or into some other buffer), the send is considered complete from the
application point of view.  On the receive side things aren't complete until
all data has been received into the buffer supplied to \mpifunc{MPI_RECV}.
%
At this point the completion counter is decremented, and since it is now zero,
the request is considered complete.  The P2P communication system, however,
cannot dallocate the MPI request because this is still in use by the
application as a reference to the operation.
%
A \mpifunc{MPI_TEST} would indicate that
the request completed, and a \mpifunc{MPI_WAIT} could return.
%
When the \mpifunc{MPI_WAIT} is called, the request may be deallocated.
%
The use of a counter when the value may only be one or zero might seem
unnecessary, but we will see in Section~XXXX that having a counter is
necessary for our implementation of collective operations.

\begin{comment}
  \emph{\mpifunc{MPI_TEST} and \mpifunc{MPI_WAIT} do the same thing on
    completion.  The above text needs a little cleanup to make that clear.}
\end{comment}

The components that we introduced here are described in greater detail in the
following section.

\subsubsection{Components}

components needs to perform point-to-point communication:
\begin{itemize}
\item assumed knowledge stuff -- requests, communicators, datatypes, segments, buffers, messages, envelopes
\item virtual connections
\item methods --
interface to convey messages to be sent and received
\item requst matching
\item progress engine --
only poll methods that might have work to do.  need a low cost way to
communicate amount of work between method and progress engine.
\end{itemize}

talk about specific functions as they come up

\subsubsection{Walkthrough}
\label{sssec:walkthrough}


\subsubsection{Summary}

tables describing interfaces

%------------------------------------------------------------------------------

\subsection{Filling out point-to-point}

wildcards
- request matching as a cross-method component (see, we told you!)

persistent sends

request cancellation

threaded implementation and usage???

%------------------------------------------------------------------------------

\subsection{Optimizing Simple Messaging}

% [BRT] To what degree do we want to discuss optimizations of simple
% messaging?  It seems like most of these optimizations apply to any
% device, not just the multi-method device.  On the other hand, we do
% want to support these optimizations, so it seems unwise not to mention
% them and include them in the design.

for contiguous data, send directly from and receive directly into the
user buffer.

packing and unpacking, segments, datatypes...

receiving a message by reading it directly from the memory of the
sending process (e.g., ptrace and /proc/PID/mem) or sending a message
by writing it directly to the memory of the receiving process.

allow the user to \mpifunc{MPI_ALLOC_MEM} to allocate a buffer in shared
memory, reducing the number of data from two to one when using the
shared memory method.

special blocking send/recv optimizations?

%------------------------------------------------------------------------------

