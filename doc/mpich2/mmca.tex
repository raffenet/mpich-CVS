\includeversion{comment}

\section{Multi-method Communication Architecture}

In this section, we present a multi-method communication architecture
for MPICH2.

What is a ``method'' anyway?  Why are multiple methods important?

Rather than present the entire architecture at once, we will start
with a basic system and modify it as we add capabilities.  The reason
for such a presentation is two-fold.  First, the intricate details of
the architecture are difficult to understand at first glance.  We hope
that by incrementally presenting features of the architecture, the
motivation will be clearer and the reader will be better served.
Second, the organization of this section suggests a plan for
developing a prototype implementation of the multi-method
architecture.  Many of the ideas presented within the section are as
of yet unproven, and would be benefit greatly from rapid prototyping
and performance analysis.

%------------------------------------------------------------------------------

% \subsection{Architecture Overview}

%------------------------------------------------------------------------------

\subsection{Simple Messaging}

% goal: describe components necessary for unoptimized, multi-method
% point-to-point
% 
% narrow scope for this subsection: no wildcards, persistent sends, etc...(look
% at lists below :)).  also not dealing with complex datatypes or heterogeneity.
% 

In this section we will describe the components necessary for an unoptimized,
multi-method, point-to-point (P2P) messaging system.  The scope of this system
is rather restricted with respect to MPI capabilities; wildcards, persistent
sends, request cancellation, complex datatypes, heterogeneity, and approaches
for reducing data copies will all be ignored for now.  We will address these
issues later in the document.

\subsubsection{Overview}

% major steps in point-to-point communication:
% \begin{itemize}
% \item posting
% \item progress
% \item completion
% \end{itemize}
% 
% go through one step at a time
% 
% for each step:
% - components used in that step? (also assumed knowledge?)
% - paragraph summary of what happens in that step
% 

One can break P2P communication into three major steps: message
posting, progress of transfer, and message completion.  In this section we
describe these steps at a high level and introduce some of the components that
will be involved in each step.

% scenario: send posted, recv posted, data arrives after recv posted.
% - trivial mpi program example here
% - note that there is another ordering, and we will discuss that in the
%   walkthrough
% 

For our overview we will concentrate our discussion on a simple scenario.  In
our scenario one process is sending a contiguous data region to another.  Both
are using nonblocking MPI calls, and execution order occurs such that the data
arrives after the receive has been posted.  This is of course not the only
possible ordering, and we will discuss other possibilities in
Section~\ref{sssec:walkthrough}.

\begin{comment}
  \emph{include a figure showing the actual code snippets}
\end{comment}

% posting (async send case)
% - allocation/initialization of MPI request (must initialize completion counter)
% - determine the vc from the communicator (so we can get to method)
% - calling of method function to post

An asynchronous send (\mpifunc{MPI_ISEND}) first results in the allocation and
initialization of a \mpids{MPI_Request} object (described previously in
XXX).  A \emph{completion counter} is included in this structure, and it is
initialized at this point (to 1).
%


\begin{comment}
  \emph{we should explain why the completion counter is set to a value of 1, or
    do something to clear this up.}
\end{comment}

\begin{comment}
  \emph{do we want to say something about the buffer in here?}  The buffer does
  not become an issue until we call the method's post function passing it the
  buffer.
\end{comment}

%
Next the \emph{virtual connection} (VC) is determined from the communicator
passed into the send call.  A reference to the appropriate method for moving
data on this VC is found within the VC object.

%
Finally the method function for posting a send is called, passing it the buffer
and envelope information supplied to \mpifunc{MPI_SEND}. 


% posting (async recv, posted before data arrives)
% - allocation of MPI req
% - buffers, datatypes
% - vc determination
% - calling method fn to post
% - is there anything really different from above (at this level)?

An asynchronous receive (\mpifunc{MPI_IRECV} is handled in a similar manner.  A
\mpids{MPI_Request} object is allocated and the completion counter is
initialized (to 1).
%


%
The appropriate VC is determined by looking at the communicator passed to the
receive call.  From the VC object the appropriate method function for posting a
receive is found and called.
 
% progress 
% - what is the issue?  we want data movement.  it's acceptable to make progress
%   only when mpi calls are made.  we might want to use threads (just mention 
%   the opportunity)
% - progress engine - motivate why we need a component to do this
% - method functions called
% - datatypes, segments (packing and unpacking mentioned in passing?)
% - request matching going on too, mention relationship to vc data structures?
% - trust us, the request matching does need to be cross-method
% 
% [something here is still bugging brian]

\begin{comment}
  \emph{is it ok to use the term operation in here?  is there a better one?  we
    could use the term ``request''.  operation isn't too confusing right now,
    but it may be later.  then again, the MPI standard talks about ``posting
    nonblocking operations''...}
\end{comment}

Progress is simply the act of performing a set of operations necessary to
complete a pending request.  For example, a send request might require data to
be moved from memory to the network card so that it may be transmitted over the
network.
%
The MPI semantics allow for progress to only occur when MPI calls
are made; however, we might want to make progress at other times.  This could
be accomplished through the use of threads or other system capabilities (see
Section XXXX).

\begin{comment}
  it would be great if there were clear definitions of ``requests'' and
  ``operations'' around somewhere.
\end{comment}

In our multi-method P2P communication system a \emph{progress engine} component
is responsible for orchestrating progress on all posted requests.  This is
necessary in order for the system as a whole to balance service across methods.
The progress engine invokes a method function, giving the method an opportunity
to make progress on pending requests (operations in progress?).  This method
function must utilize \emph{buffer} information (user buffer location,
datatype, count) provide when the request was posted in order to determine the
appropriate data regions to send or receive.

Matching of requests occurs within the context of progress as well.  In our
example scenario, our receive was posted before any data from the sender
arrived.  The posted received is associated with the VC on which data will be
passed.  At some point that data does arrive, and the \emph{envelope} from
the incoming message must be matched to the envelope from the posted receive.

\begin{comment}
  \emph{define envelope?  point out that it should have been defined?}  briefly
  mention a definition and then point at another section for details (are there
  any details???)
\end{comment}

This matching process might seem trivial, but in fact the MPI semantics for
wildcard operations and message ordering combined with the possibility of
multiple methods make matching somewhat complicated.
For this reason we will implement a separate \emph{request arbitrator}
component (do you like that name?) for performing request matching for the
system as a whole (both intra- and inter-method).

\begin{comment}
  \emph{arbitrator: find something better than matcher that doesn't have the negative conotation of arbitrator.}
\end{comment}

% completion
% - notification of completion (via decrement of counter - in MPI request)
% - note that a counter becomes important when we get to collectives, kinda
%   overkill for this.
% - deallocation of MPI request?  nope.  deallocated by wait/test instead.  
%   do we want to mention this here?
% 

Once all data has been copied out of the sender's buffer (either onto the wire
or into some other buffer), the send is considered complete from the
application point of view.  On the receive side things aren't complete until
all data has been received into the buffer supplied to \mpifunc{MPI_RECV}.
%
At this point the completion counter is decremented, and since it is now zero,
the request is considered complete.  The P2P communication system, however,
cannot deallocate the MPI request because this is still in use by the
application as a reference to the operation.
%
A \mpifunc{MPI_TEST} would indicate that
the request completed, and a \mpifunc{MPI_WAIT} could return.
%
When the \mpifunc{MPI_WAIT} is called, the request may be deallocated.
%
The use of a counter when the value may only be one or zero might seem
unnecessary, but we will see in Section~XXXX that having a counter is
necessary for our implementation of collective operations.

\begin{comment}
  \emph{\mpifunc{MPI_TEST} and \mpifunc{MPI_WAIT} do the same thing on
    completion.  The above text needs a little cleanup to make that clear.}
\end{comment}

The components that we introduced here are described in greater detail in the
following section.

\subsubsection{Components}

components needs to perform point-to-point communication:
\begin{itemize}
\item assumed knowledge stuff -- requests, communicators, datatypes, segments, buffers, messages, envelopes
\item virtual connections
\item methods --
interface to convey messages to be sent and received
\item requst matching
\item progress engine --
only poll methods that might have work to do.  need a low cost way to
communicate amount of work between method and progress engine.
\end{itemize}

talk about specific functions as they come up

\subsubsection{Walkthrough}
\label{sssec:walkthrough}


\subsubsection{Summary}

tables describing interfaces

%------------------------------------------------------------------------------

\subsection{Filling out point-to-point}

wildcards
- request matching as a cross-method component (see, we told you!)

persistent sends

request cancellation

threaded implementation and usage???

%------------------------------------------------------------------------------

\subsection{Optimizing Simple Messaging}

% [BRT] To what degree do we want to discuss optimizations of simple
% messaging?  It seems like most of these optimizations apply to any
% device, not just the multi-method device.  On the other hand, we do
% want to support these optimizations, so it seems unwise not to mention
% them and include them in the design.

for contiguous data, send directly from and receive directly into the
user buffer.

packing and unpacking, segments, datatypes...

receiving a message by reading it directly from the memory of the
sending process (e.g., ptrace and /proc/PID/mem) or sending a message
by writing it directly to the memory of the receiving process.

allow the user to \mpifunc{MPI_ALLOC_MEM} to allocate a buffer in shared
memory, reducing the number of data from two to one when using the
shared memory method.

special blocking send/recv optimizations?

%------------------------------------------------------------------------------

