\subsection{Collective Communication and Computation}
\label{sec:collective-comm}

One of the major changes in MPICH2 is in the implementation of the
collective routines.  The MPICH2 implementation will exploit
pipelining and store and forward algorithms; these are supported by
the XFER interface. 
%\code{MPID_Stream_xxx} routines.  

Since each system may have some feature that provides for even faster
implementation of the collective routines, it will be possible to
substitute a system-specific implementation for any of the collective
routines.  The purpose of the implementations provided with MPICH is
to provide a level of performance that will be adequate
for many users.  

The $\alpha$-tree approach described in
\cite{bern:mpi-collective:hpcn99} should be considered; this is a
simple variation on the binomial tree approach used in the MPICH
implementations of many of the collective routines.  We will consider
combining this with the pipelining and scatter/gather approaches
championed by van de Geijn (\cite{vandegeijn} isn't quite the right
reference but it will do for now).

\subsubsection{Reduction functions}
The reduction functions must use the \code{restrict}\index{restrict} qualifier.

Each reduction operation (e.g., \code{MPI_SUM}) has a corresponding
implementation (e.g., \code{MPIR_Sum}) and is placed in a separate file (e.g.,
\file{opsum.c}.  Each of these must be careful to conditionally include the
Fortran datatypes and Fortran logical operations (see
Section~\ref{sec:fortran}).

Some reduction functions are not defined on a particular datatype.  To
indicate errors, the routine \code{MPID_Op_set_error} is called.
The MPI reduction routine (reduce, allreduce, scan, exscan, and reducescatter)
checks this with \code{MPID_Op_get_error}.  In order to ensure that
separate threads manage their own error flags for reductions, there is
an \mpids{MPIR_PerThread}{op_error} field in the per-thread data
structure.

% An implementation of this might be 
% \begin{verbatim}
% #ifdef MPID_HAS_THREADS
% /* Use thread private storage for the error value.  Allocate on demand */
% extern int MPID_Op_error_key;
% void MPID_Op_error_delete( void *val ) { if (val) free(val); }
% #define MPID_Op_error_init \
%     pthread_key_create( &MPID_Op_error_key, MPID_Op_error_delete )
% #define MPID_Op_error_finalize \
%     pthread_key_delete( MPID_Op_error_key )
% #define MPID_Op_set_error(err) {\
%  int *e = (int*)pthread_get_specific(MPID_Op_error_key); \
%  if (!e) { e = (int*)malloc(sizeof(int));\
%            pthread_set_specific( MPID_Op_error_key, e );}\
%  *e = err; }
% #define MPID_Op_get_error(err_p) \
%     *err_p = *(int*)pthread_get_specific(MPID_Op_error_key)
% #else
% #extern int MPID_Op_error;
% #define MPID_Op_error_init
% #define MPID_Op_error_finalize
% #define MPID_Op_set_error(err) MPID_Op_error = err
% #define MPID_Op_get_error(err_p)  *(err_p)=  MPID_Op_error
% #endif
% \end{verbatim}


\subsubsection{Code Structure for the Implementation of the Collective
  functions} 

The MPICH code uses one gigantic file, \file{intraops.c}, to provide a
generic implementation of each collective operation.  Each
communicator has a structure of pointers to functions.  Unless
otherwise set, each communicator points to the predefined structure
\code{MPIR_intra_collops}\index{MPIR_intra_collops} which is
initialized to point to \emph{all} of 
these functions.  

For MPICH2, each of the MPI functions (in its own file) contains the generic
implementation of the collective operation, based initially on the
point-to-point code similar to that in MPICH-1 and eventually on the
stream-oriented operations.  This will simplify the process of tuning each
operation; it will also reduce the size of (unshared) executables since few if
any programs use all of the collective operations.
See the discussion of the implementation of PMPI.

Question: Now that MPI-2 defines intercommunicator collective
routines, do we want these in the same file as the intracommunicator
routines, or in an alternate file.  E.g., should \file{bcast.c} contain
the intracommunicator implementation of \mpifunc{MPI_Bcast} and
\file{icbcast.c} contain the intercommunicator implementation.
Also, we may select no intercommunicator collectives at configure (or run?)
time to reduce the size of libraries and code.

We may want to have multiple ``generic'' implementations and an easy
way, say with the runtime parameter routines, to select among them at runtime.

We may want to compute and save things like the neighbors for each
collective communication pattern; this can be done either when the
collective operation is first encountered or at communicator creation
time (the descision could be a runtime attribute).  Question: how do
we modularize this?  Is there a ``collective'' 
attribute?

