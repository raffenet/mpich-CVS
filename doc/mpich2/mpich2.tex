%
% Design document for MPICH2
%
% This document should be used with the ADI3 document
\documentclass{article}
\usepackage{/home/gropp/data/share/refman}
\usepackage{/home/gropp/sowing-proj/sowing/docs/doctext/tpage}
\usepackage{epsf}
\textheight=9in
\textwidth=6.1in
\oddsidemargin=.2in
\topmargin=-.50in
\newread\testfile

%
% Modify the way titles are handled for no breaks between pages
%\def\mantitle#1#2#3{\pagerule\nobreak
%\ifmancontents\addcontentsline{toc}{subsection}{#1}\fi}

\makeindex

\begin{document}

\markright{MPICH Design Document}

\def\nopound{\catcode`\#=13}
{\nopound\gdef#{{\tt \char`\#}}}
\catcode`\_=13
\def_{{\tt \char`\_}}
\catcode`\_=11
\def\code#1{{\tt #1}}

\def\mpifunc#1{\code{#1}\index{#1}}
% We may want to separate the MPI and MPID index entries
\def\mpidfunc#1{\code{#1}\index{#1}}
% ditto for constants
\def\mpiconst#1{\code{#1}\index{#1}}
\def\mpidconst#1{\code{#1}\index{#1}}
% Environment variables
\def\eindex#1{\index{#1}}
% Configure option and related topics
\def\cfgindex#1{\index{#1}}

% 
% These provide environments to describe the implementation of each routine
%
\newenvironment{adi3}{\begin{quote}\textbf{ADI3:}\\}{\end{quote}}
\newenvironment{mmadi}{\begin{quote}\textbf{MMADI:}\\}{\end{quote}}
\newenvironment{core}{\begin{quote}\textbf{Core:}\\}{\end{quote}}

%
% For rma, there are these three kinds:
\newcommand\tcpname{Twosided}
\newcommand\shmemname{SharedMem}
\newcommand\vianame{RemoteMem}
\newenvironment{tcp}{\begin{quote}\textbf{\tcpname:}\\}{\end{quote}}
\newenvironment{shmem}{\begin{quote}\textbf{\shmemname:}\\}{\end{quote}}
\newenvironment{via}{\begin{quote}\textbf{\vianame:}\\}{\end{quote}}


%\tpageoneskip
\ANLTMTitle{MPICH2 Design Document\\
Draft of \today}{\em 
William Gropp\\
Ewing Lusk\\
Rob Ross\\
Mathematics and Computer Science Division\\
Argonne National Laboratory}{00}{\today}

\clearpage

\pagenumbering{roman}
\tableofcontents
\clearpage

\pagenumbering{arabic}
\pagestyle{headings}

\section{Introduction}
This document discusses how the MPICH2 implementation is written using
the ADI-3 \cite{adi3man} for the supporting functions.  This document
contains guidelines for the MPICH2 implementation.  One important
purpose of this document is to provide common guidelines for writing
the MPICH code.

See also the Coding Standards docment \cite{coding-standards}.

To date, this document contains primarily comments on the rules for
writing the code.  Few comments on the use of ADI-3 routines have been
added yet.  No part of this document is final.

A major challenge is developing an interface that requires fewer (or
at least simpler) routines to implement.  This is particularly
difficult since the MPI standard is defined to encourage efficient
implementations.  While it is possible to meet the functional
definitions of MPI with fewer routines, achieving performance requires
something relatively close to what MPI defines.

One possibility is to consider a few classes of systems.  Pure
distributed memory is one important case.  Another is shared memory,
or at least some common shared memory.  Of course, multi-method
devices make this more difficult.  Some more discussion of this is in
the RMA section.

% Outline:
\section{General}
This section contains general recommendations are requirements for
writing the code.

\subsection{Error reporting}

MPI routines should check all possible error conditions early, before
calling any ADI routines.  The ADI routines perform little error
checking.  To allow error handling to be enabled or disabled both at compile
time 
and at runtime, the tests should be placed within the following block:
\begin{verbatim}
#ifdef HAVE_ERROR_CHECKING
BEGIN_ERROR_CHECKS
...
END_ERROR_CHECKS
#endif
\end{verbatim}
The macros \code{BEGIN_ERROR_CHECKS} and \code{END_ERROR_CHECKS} can
expand, depending on configuration settings, into either null (i.e.,
no runtime control) or 
\begin{verbatim}
#define BEGIN_ERROR_CHECK if (MPIR_Perform_Error_Tests) {
#define END_ERROR_CHECKS }
\end{verbatim}

There must be a configure option,
\code{--disable-error-checking}\cfgindex{--disable-error-checking},  
that prevents \code{HAVE_ERROR_CHECKING} from being defined.

\subsubsection{Errors to test for}
Most values should be tested to ensure that they are in-range.  For examples,
tags must be nonnegative for sending (and nonnegative or
\mpiconst{MPI_ANY_TAG} for receiving). 

Question: Code the Rajeev wrote often included the following test on pointers:
\begin{verbatim}
if (ptr < (Ptr_type) 0) error
\end{verbatim}
This helps catch some common invalid pointers on many systems, but isn't
correct for some other systems.  Should there be an optional
pointer-validation test?  For example, the function
\mpidfunc{MPID_Valid_pointer} would return true if the pointer was valid and
false otherwise (possibly testing only for reading).  This could do anything
from test against null to changing the \code{SIGSEGV} handler, attempting to
read from the address, resetting the handler, and determining if the handler
was invoked.  

\subsubsection{Choosing Error Handlers and Classes}
Error handlers should be chosen using the following decision tree:
\begin{enumerate}
\item If the routine has a valid \code{MPI_Comm}, \code{MPI_File}, or
  \code{MPI_Win}, use the error handler from that object.  (Question: do we
  have a common way to extract the error handler?  It isn't part of the
  communicator/file/win structures yet.)
\item If the error relates to a request, and the request refers to a valid
  communicator, use that communicator's error handler (e.g., \code{MPI_Wait}).
\item Otherwise, for everything except MPI-IO, use the error handler attached
  to \mpifunc{MPI_COMM_WORLD}  
  (see Section 7.2 in the MPI-1 Standard:: ``MPI calls that are not related to
  any communicator are considered to be attached to the communicator
  \code{MPI_COMM_WORLD}.'' ).
\item For MPI-IO, the default error handler is attached to
  \code{MPI_FILE_NULL} (see Section 9.7 in the MPI-2 standard: ``The default
  file error handler can be changed by specifying \code{MPI_FILE_NULL} as the
  \code{fh} argument to \code{MPI_FILE_SET_ERRHANDLER}''). 
\end{enumerate}

\subsection{Error handling and Fault Tolerance}
In order to support fault tolerance, errors should be handled as
gracefully as possible.  If it is possible to remain in a consistent
state, the process should not abort (unless, of course, the error
handler requires it, as the default \mpifunc{MPI_ERRORS_FATAL} does).  
If it is not possible to recover from an error, then the process
should call \mpidfunc{MPID_Abort} but specify \mpifunc{MPI_COMM_SELF} as the
communicator.  

\subsection{Memory Allocation}
Avoid memory allocation wherever practical.  Use \mpidfunc{MPID_Malloc}
etc. instead of \code{malloc}, as described in the ADI-3 manual
\cite{adi3man}. 

At normal exit from the MPI library in \mpifunc{MPI_Finalize}, the
routine \mpidfunc{MPID_Trdump} must be called if the environment variable
\code{MPICH_TRDUMP}\eindex{MPICH_TRDUMP} is set.  

Question: Should there be separate per-thread and per-process memory
allocators? 

Question: Should we use the same calling sequence as PETSc uses for the memory
allocators?  This will make it easier to use the PETSc options database.

Question: Should there be a memory allocator that remembers allocations within
a routine so that out-of-memory errors can be cleanly handled (e.g., freeing
all memory allocated within the routine)?  This could have something like
\begin{verbatim}
#define MAX_MEM_STACK 16
typedef struct { int n_alloc; void *(ptrs[MAX_MEM_STACK]); } MPID_Mem_stack;
\end{verbatim}
and a memory allocation macro that updated a routine-local version of this
with every allocation.  Then on an error, we could easily free any allocated
memory.

\subsection{MPI Opaque Objects}

Most objects in MPI (with the exception of \mpifunc{MPI_Status}) are
opaque.  In the MPICH2 implementation, opaque objects are represented
by integers.  This simplifies the implementation of the functions for
transfering handles between C/C++ and Fortran.  

Question:  One interesting idea is to encode information in the opaque
handle.  For example, the \code{sizeof} a basic datatype, and whether
a type is basic, could be part of the handle for an
\code{MPI_Datatype}. In that case, an implementation can avoid looking
up the datatype (e.g., by using the integer of the opaque type as an
array index) and instead perform a few simple operations on the handle
(e.g., mask and test and mask and shift).  Similarly, for
communicators that are dups of \code{MPI_COMM_WORLD}, the handle could
contain the \code{context_id}, again avoiding the need to look up the
communicator, since in addition the mapping from rank to local pid for 
communicators similar to \code{MPI_COMM_WORLD} is the identity
mapping.  Thus a single bit test on the opaque handle could eliminate
a number of tests and memory references for an important common case.

The questions are: do we want to make this possible?  If so, do we
want to implement it from the beginning?  Note that encoding
information into the opaque handle makes that handle conversion
functions more complex.

\subsubsection{Proposed Opaque Handle Format}

We're going to use ints as handles to datatypes and to other constructs such
as communicators.  Some of the considerations in this are:
\begin{itemize}
\item fast resolution for some number of user-created constructs
\item particularly fast resolution for built-ins
\item minimal added limitations on number of user-defined constructs 
\end{itemize}

Using the same system for storing all these constructs should be more
space-efficient.

The MPI opaque objects include \mpiconst{MPI_Comm}, \mpiconst{MPI_Group},
\mpiconst{MPI_Datatype}, \mpiconst{MPI_Errhandler}, \mpiconst{MPI_File},
\mpiconst{MPI_Info}, \mpiconst{MPI_Op}, and \mpiconst{MPI_Win}.

Nomenclature is subject to change.

Different architectures have ints of varied sizes, so we must rely on
\code{sizeof(int)} to determine what we have to work with.

We will use the 2 highest bits to encode a type of construct.  These are:
\begin{description}
\item[\code{CONSTRUCT_INVALID}]  (00)
\item[\code{CONSTRUCT_BUILTIN}]  (01)
\item[\code{CONSTRUCT_DIRECT}]   (10)
\item[\code{CONSTRUCT_INDIRECT}] (11)
\end{description}

Using 00 as invalid, especially in the two high bits, will help detect
bad parameters passed to us (although negative ints won't be caught).

\code{CONSTRUCT_BUILTIN} types include the predefined datatypes such as
\code{MPI_BYTE} and \code{MPI_DOUBLE} as well as the predefined communicators
\code{MPI_COMM_WORLD} and \code{MPI_COMM_SELF}.

The next three highest bits will encode the MPI type stored:
\begin{description}
\item[\code{MPI_Comm}]     (000)
\item[\code{MPI_Group}]    (001)
\item[\code{MPI_Datatype}] (010)
\item[\code{MPI_File}]     (011)
\item[\code{MPI_Errhandler}](100)
\item[\code{MPI_Op}]       (101)
\item[\code{MPI_Info}]     (110)
\item[\code{MPI_Win}]      (111)
\end{description}

At this point we can reasonably expect to have at least 11 bits remaining
to work with, usually 27 and for some architectures 59.

There are more than 32 but less than 64 predefined datatypes, so 6 bits are
adequate to encode all of the predefined datatypes.
That leaves us at least 5 bits
(usually 21) to specify the size.  We'll be specifying size in bytes,
which will be adequate (if long double is 16 bytes, then the long complex and
long-double-int types may be 32 bytes, but on these systems, sizeof(int) will
be at least 32 bits, leaving us enough room for the size of these longer
types. 

For predefined Groups and Communicators we simply use the remaining bits
to specify the particular group/communicator (e.g. MPI_COMM_WORLD) and 
either the rank of the process or the size.  Actually, for small integers we
might not have enough bits, so we should think about this a little more.

For the direct accessed values the remaining bits are used as indices into
a table of pointers or an array of preallocated objects (using the array
removes one indirect reference and ensures that storage is available at init
time).  This table should be 
a memory page or two in size, 
minus estimated malloc overhead (if we dynamically allocate).

For indirectly accessed values the remaining bits specify an allocated block
for storing pointers and an index into that block.  We'll split the bits
based on the number of pointers we can store in a block (which will need
to be statically calculable; \code{_SC_PAGE_SIZE} is a starting point, or we
can  do a configure test and define...).

\subsection{MPI and PMPI Routines}
Each routine should implement the \code{PMPI} version of the routine.
Where possible, a weak symbol pragma may be used to define the
\code{MPI} version of the routine.  If weak symbol support is not
available, the \file{Makefile}s will support recompiling each file
with the definition \code{MPICH_MPI_FROM_PMPI} made.  This value
can also be used to protect code that is used by the \code{PMPI}
version of the routine.  For example, 
\begin{verbatim}
#include "mpich_impl.h"

#ifdef MPICH_MPI_FROM_PMPI
#define PMPI_Attr_get MPI_Attr_get
#else
int MPIR_Attr_get_util( int keyval, MPID_Comm *comm, void *attr )
{
...
}
#endif
int PMPI_Attr_get( ... etc ... )
\end{verbatim}

If it is necessary to create the \code{MPI} versions separately, the
object files should be renamed, allowing them to be placed into the
same library.  To handle the event that the library cannot hold over
500 files (250 for PMPI MPI 1 and 2, plus a version for the MPI
routines), the name of the library containing the profiling versions
should be separate.

Question: If we do have two libraries, then we must link with both,
even when the profiling routines are not otherwise used because any
internal functions may be defined in the PMPI versions.  We may need
to do this anyway, because any function that calls an MPI routine will
actually be calling the PMPI version.  Is this what we want to do?  Is
it the best thing to do?  Note that we do this now, but through the
confusing approach of using the MPI names, but redefining them as PMPI
with a file containing a redefinition of every single MPI routine.

Question: The pragma code is very ugly and hard to read.  It is also hard to
update.
It would be relatively
easy to create the correct form on the fly, using configure (or
another program, run by configure).  But that
would require at least an include file for each MPI routine.
An alternate approach is to make the inclusion entirely machine
generated.  In that case, updating it requires only re-running the
update editor.  In this case, the pragmas for implementing the
profiling interface should be placed between two clear markers, such
as 
\begin{verbatim}
/* -- begin profiling interface -- */
/* DO NOT EDIT.  Use the program xxx to update */
...
/* -- end profiling interface -- */
\end{verbatim}

\subsection{Deprecated Routines}
Question:  How should deprecated routines be handled?  At the very
least, their man pages should make clear that they are deprecated and
what functions should be used instead.

\subsection{Runtime Parameters}
MPICH-1 suffers from having many compile-time parameters that could just as
easily be either runtime or at least initialization-time.  These parameters
include search paths and buffer sizes.  These should have a compile-time
default (particularly the search paths) but have an easy way to override at
initialization and/or run time.  While environment variables are one way to do
this, we should not rely on them, since not all environments guarantee that
environment variables are propagated to all processes.

Question: Should we use the PETSc options database code or something similar
to provide uniform access to runtime parameters?  

\subsection{Coding Practices}
This section reviews some coding practices for the MPICH code.

\begin{description}
\item[Function prototypes.]
All routines should be prototyped.  The prototypes should be in the
smallest scope possible.  For example, if the routine is used only
within the files in a subdirectory, the prototype should be in an
include file within that directory.
Question: do we really want this?

\item[Static functions.]
Functions used entirely within a single file should be declared
\code{static}.  Functions that are not static must follow the naming
convention of starting with \code{MPI_} or \code{PMPI_} (for routines
implementing the MPI Standard) or \code{MPIR_} for internal routines.  
Global symbols, such as keyvals, should use \code{MPICH_} as the
prefix.  

\item[Parameter declarations.]
Parameters (with the exception of the MPI routines defined by the
standard) should follow the guidelines in \code{coding}.  In
particular, \code{const} and \code{restrict} should be used.
Parameters that are semantically arrays should be declared as arrays
(using \code{[]}).  

\item[Indentation style.]
Question: should we specify an indentation style?  If so, we need to
specify it at the top of each file (as an emacs command) so that emacs
will use the correct style for users with other styles or projects.

\item[File header.]
There should be a standard file header, containing the copyright and
standardized includes (e.g., something like "mpiimpl.h").  This header
template should be part of the development environment.  Question:
what should this header be?

\item[Function Name.]
There should be a macro, \code{__FUNCTION__}, containing the name of the
function.  This name is chosen because some versions of \code{gcc} will set it
for you.  The statement \code{SET_FUNCTION_NAME("name");} should be included
in the declarations part of the program \emph{or} we should include the
appropriate \code{\#define} line (as PETSc does) before the function
declaration. 
\end{description}

\subsection{Other Subsystems}
In MPICH, there is code that is not directly part of the MPI
implementation, such as the MPE and test suite code.  For MPICH2,
these should be cleanly separated.  To simplify the construction of a
full MPICH distribution, there should be a Makefile (and configure
options) that know how to build MPICH2 with MPE, perftest, the test
suite, and other options.  These should \emph{not} be part of the base
MPICH2 project (as far as CVS is concerned).  This will encourage
better separation of the projects.

Question: What about Fortran and C++?  These are required for full MPI support
(unlike MPE or the performance tests), but have many unique requirements
during configurations.  I believe that these should continue to have a
separate configure, though it is not necessary to make them standalone (e.g.,
work with other implementations of MPI).

\section{Include Files}
The include file \file{mpi.h} should not require any \code{-Dxxx}
definitions by the compiler.  This will require generating the
\file{mpi.h} from another file (and probably not using \code{configure} and an
\file{mpi.h.in}).  It should not include any other
files, with the (possible) exception of \file{mpi++.h} for C++ support.

The \file{mpif.h} (or \file{mpif.h.in}) should be created from
\file{mpi.h} so that the various integer values (e.g., error classes,
datatypes, etc.) are guaranteed consistent.  This can be done prior to
distribution, similar to the way \file{configure} is generated from
\file{configure.in}.  The point is to automate this and ensure that,
at least in the development Makefiles, the \file{mpif.h.in} file
should be created from the \file{mpi.h} automatically, at least with
respect to any values. 

\section{MPI Operations}
This section describes the implementation of the MPI operations.  The
descriptions may include discussion of some implementation issues.
These are split up according to function, and roughly (but not
exactly) match the MPI standard.  Each of these has a corresponding
directory in the MPI source tree.  Note that this means that the
directory structure does not exactly match the chapter structure of
the MPI Standard.

For each routine, the description may be text, usually indicating that no ADI
routines are involved.  For example, many of the process topology and other
informational routines fit into this category.
For routines that make use of the ADI, the description will often be presented
as follows:
\begin{adi3}The ADI3 routine(s) that are called
\begin{mmadi}One possible implementation of the above routine(s), using
  lower-level ADI3 routines.
\begin{core}One possible implementation of the MMADI routines, using just the
  \code{MPID_CORE}. 
\end{core}
\end{mmadi}
\end{adi3}
The purpose of this description is to evaluate the ADI-3 design (at all
levels, not just to core and the most general, top level) for the
implementation of MPI.  

\subsection{Attributes and Info}

Error values defined for keyvals and attributes:
\mpiconst{MPI_ERR_KEYVAL}

\paragraph{Info.}

Error values defined for info:
\mpiconst{MPI_ERR_INFO_KEY},
\mpiconst{MPI_ERR_INFO_VALUE},
\mpiconst{MPI_ERR_INFO_NOKEY}.

Constants defined for info: 
\mpiconst{MPI_MAX_INFO_KEY},
\mpiconst{MPI_MAX_INFO_VAL}.
Note that the MPI standard sets limits on the ranges that these can take.

Predefined info keys are:
\mpiconst{access_style}, 
\mpiconst{appnum}, 
\mpiconst{arch}, 
\mpiconst{cb_block_size}, 
\mpiconst{cb_buffer_size}, 
\mpiconst{cb_nodes}, 
\mpiconst{chunked}, 
\mpiconst{chunked_item}, 
\mpiconst{chunked_size}, 
\mpiconst{collective_buffering}, 
\mpiconst{external32}, 
\mpiconst{false}, 
\mpiconst{file}, 
\mpiconst{file_perm}, 
\mpiconst{filename}, 
\mpiconst{host}, 
\mpiconst{internal}, 
\mpiconst{io_node_list}, 
\mpiconst{ip_address}, 
\mpiconst{ip_port}, 
\mpiconst{native}, 
\mpiconst{nb_proc}, 
\mpiconst{no_locks}, 
\mpiconst{num_io_nodes}, 
\mpiconst{path}, 
\mpiconst{random}, 
\mpiconst{read_mostly}, 
\mpiconst{read_once}, 
\mpiconst{reverse_sequential}, 
\mpiconst{sequential}, 
\mpiconst{soft}, 
\mpiconst{striping_factor}, 
\mpiconst{striping_unit}, 
\mpiconst{true}, 
\mpiconst{wdir}, 
\mpiconst{write_mostly}, 
\mpiconst{write_once}.

\paragraph{Defining New Keys.}
The MPI Forum has not resolved an ambiguity in the definition of
info.  While it was clear during many of the discussions that the
expectation was that \code{MPI_Info}\index{MPI_Info} could be used by
layered implementations of parts of MPI, particularly the I/O part,
IBM did not interpret the standard this way and their interpretation
is both consistent with the standard and offers a feature not
otherwise available (specifically, the ability to determine what info
keys are recognized by the implementation).

Note also that the interface to access the info values is not
thread-safe, since it has the implicit assumption that the number of
keys in an info object does not change unless ...
For example, consider this sequence:
\begin{verbatim}
    MPI_Info_get_nkeys( info, &nkeys );
    MPI_Info_get_nthkey( info, nkeys-1, keystring );
\end{verbatim}
In a multi-threaded environment, another thread may have called
\begin{verbatim}
   MPI_Info_delete( info, "any-key-in-info");
\end{verbatim}
after \mpifunc{MPI_Info_get_nkeys} but before
\mpifunc{MPI_Info_get_nthkey}.  There's no way to really fix this, but
we can at least raise the issue in the manual pages and generate
helpful error messages in this case.  We may also want to add an
extension that raises a special error code if a different thread
modifies an \code{MPI_Info} while any of the routines with a notion of
the ``current'' state of info are operating on it.

\paragraph{MPI-1 Attribute Functions.}
The following five functions are deprecated.

\subsubsection{\mpifunc{MPI_ATTR_DELETE}}
Calls \mpifunc{MPI_COMM_DELETE_ATTR}.

\subsubsection{\mpifunc{MPI_ATTR_GET}}
Calls \mpifunc{MPI_COMM_GET_ATTR}.

\subsubsection{\mpifunc{MPI_ATTR_PUT}}
Calls \mpifunc{MPI_COMM_SET_ATTR}.

\subsubsection{\mpifunc{MPI_KEYVAL_CREATE}}
Calls \mpifunc{MPI_COMM_CREATE_KEYVAL}.

\subsubsection{\mpifunc{MPI_KEYVAL_FREE}}
Calls \mpifunc{MPI_COMM_FREE_KEYVAL}.

\subsubsection{\mpifunc{MPI_COMM_CREATE_KEYVAL}}
This should use a common MPID routine to create keyvals.  This routine, along
with others to allocate MPID objects, has not yet been defined in the ADI.

\subsubsection{\mpifunc{MPI_COMM_FREE_KEYVAL}}
Ditto to free keyvals.

%\subsubsection{\mpifunc{MPI_COMM_NULL_COPY_FN}}

%\subsubsection{\mpifunc{MPI_COMM_DUP_FN}}

\subsubsection{\mpifunc{MPI_COMM_GET_ATTR}}
\begin{adi3}\mpidfunc{MPID_Attr_find}, followed by access to the value.
Question: does this require a thread lock, or is that burden on the user?  Do
we want to have the ability to detect thread races (e.g., a flag to indicate
that another thread has touched the list)?
\end{adi3}

\subsubsection{\mpifunc{MPI_COMM_SET_ATTR}}
\begin{adi3}\mpidfunc{MPID_Attr_find}, followed by access to the value.
This has the same thread issues as \code{MPI_COMM_GET_ATTR}.
\end{adi3}

\subsubsection{\mpifunc{MPI_COMM_DELETE_ATTR}}
\begin{adi3}\mpidfunc{MPID_Attr_delete}.
This has the same thread issues as \code{MPI_COMM_GET_ATTR}.
\end{adi3}

\subsubsection{\mpifunc{MPI_TYPE_GET_ATTR}}
\begin{adi3}\mpidfunc{MPID_Attr_find}, followed by access to the value.
This has the same thread issues as \code{MPI_COMM_GET_ATTR}.
\end{adi3}

\subsubsection{\mpifunc{MPI_TYPE_SET_ATTR}}
\begin{adi3}\mpidfunc{MPID_Attr_find}, followed by access to the value.
This has the same thread issues as \code{MPI_COMM_GET_ATTR}.
\end{adi3}

\subsubsection{\mpifunc{MPI_TYPE_DELETE_ATTR}}
\begin{adi3}\mpidfunc{MPID_Attr_delete}.
This has the same thread issues as \code{MPI_COMM_GET_ATTR}.
\end{adi3}

\subsubsection{\mpifunc{MPI_TYPE_CREATE_KEYVAL}}
This should use a common MPID routine to create keyvals.  

\subsubsection{\mpifunc{MPI_TYPE_FREE_KEYVAL}}
Call a common MPID routine to free keyvals.
%\subsubsection{\mpifunc{MPI_TYPE_NULL_COPY_FN}}
%\subsubsection{\mpifunc{MPI_TYPE_DUP_FN}}

\subsubsection{\mpifunc{MPI_WIN_CREATE_KEYVAL}}
This should use a common MPID routine to create keyvals.  

\subsubsection{\mpifunc{MPI_WIN_FREE_KEYVAL}}

\subsubsection{\mpifunc{MPI_WIN_SET_ATTR}}
\begin{adi3}\mpidfunc{MPID_Attr_find}, followed by access to the value.
This has the same thread issues as \code{MPI_COMM_GET_ATTR}.
\end{adi3}

\subsubsection{\mpifunc{MPI_WIN_GET_ATTR}}
\begin{adi3}\mpidfunc{MPID_Attr_find}, followed by access to the value.
This has the same thread issues as \code{MPI_COMM_GET_ATTR}.
\end{adi3}

\subsubsection{\mpifunc{MPI_WIN_DELETE_ATTR}}
\begin{adi3}\mpidfunc{MPID_Attr_delete}.
This has the same thread issues as \code{MPI_COMM_GET_ATTR}.
\end{adi3}

%\subsubsection{\mpifunc{MPI_WIN_NULL_COPY_FN}}

\subsubsection{Info}
There are two ways to handle \code{MPI_Info}.  One is to implement a general
mechanism for handling key/value pairs, much like the code that is part of
\file{mpich/src/misc2}.  The other is to implement only the defined keys that
MPICH2 needs.  This is the approach used by IBM; in this model, the known keys
are not stored; instead, the keys are mapped to a predefined set of values
(e.g., to an enum).  This makes it easy to extract a value from a particular
info object (the values can be preconverted into booleans or integers and
stored in a small array); further, it provides a way (which is otherwise
lacking) to 
indicate which key values are known to the implementation.  
The problem with restricting keys to those known to the implementation is it
prevents using \code{MPI_Info} to pass information to another subsystem, such
as to the process manager and allocator (through \code{MPI_COMM_SPAWN}). 
Since \code{MPI_Info} is not used by any performance-critical functions
(\code{MPI_Info} is only used in \code{MPI_Alloc_mem}, \code{MPI_Comm_accept},
\code{MPI_Comm_connect}, 
\code{MPI_Comm_spawn}, \code{MPI_Comm_spawn_multiple}, \code{MPI_File_delete},
\code{MPI_File_open}, \code{MPI_File_set_view}, \code{MPI_Lookup_name},
\code{MPI_Open_port}, \code{MPI_Publish_name}, 
\code{MPI_Unpublish_name}, as well as the functions with \code{INFO} in their
name), speed is not critical for the Info functions.

Question: Since many of the predefined Info values encode either booleans or
integers, do we want an internal routine such as \mpidfunc{MPID_Info_get_int}
that returns an integer value if the key is found and has either an integer or
boolean value?  Similarly, should there be an \mpidfunc{MPID_Info_set_int}?
If we do this, do we want to cache the result in the Info item
structure?
There are a few info keys (e.g., \mpiconst{chuncked} or
\mpiconst{io_node_list}) that are lists of integers and at least one
(\mpiconst{access_style}) that is a list of strings.  \mpiconst{soft}
is a list of triplets; an info routine that returned triplets would be
needed for this.

Question: Do we want to make the key part of the structure, and set
\mpiconst{MPI_MAX_KEY_VALUE} to a small value such as 32 (the minimum
allowed)?  Doing so slightly simplifies the code to set and delete info
values. 

Question: Do we want the list to be sorted by key name?  The current
implementation uses a linear list, which is probably ok for most uses.

\subsubsection{\mpifunc{MPI_INFO_CREATE}}
Create an info object.  Does this use an MPID object creator?

\subsubsection{\mpifunc{MPI_INFO_DELETE}}
Remove a key from an info object:
\begin{verbatim}
lock info
find key and remove key and associated value
unlock info
\end{verbatim}

\subsubsection{\mpifunc{MPI_INFO_DUP}}
Note that this routine must be thread-safe.  This requires info routines that
modify the info structure or the list of key/value pairs to operate safely.
Because none of the info routines are performance critical, and because none
of these operations is very complex, using a single lock per \code{MPI_Info}
or even a single lock for \emph{all} info objects is probably adequate.


\subsubsection{\mpifunc{MPI_INFO_FREE}}
Delete an info object.  Uses an MPID object creator.

\subsubsection{\mpifunc{MPI_INFO_GET}}
\begin{verbatim}
lock info
find key and return associated value
unlock info
\end{verbatim}

\subsubsection{\mpifunc{MPI_INFO_GET_NKEYS}}
Question:  Should we keep a count of the number of keys, or just count them?
If we count them, then
\begin{verbatim}
lock info
run through list to count all keys
unlock info
\end{verbatim}

\subsubsection{\mpifunc{MPI_INFO_GET_NTHKEY}}
\begin{verbatim}
lock info
find indicated key and return name
unlock info
\end{verbatim}

Question:  Do we want to have this return an error if the info list is 
modified by another thread?  Is there any way to actually do this?  For
example, the self-id of a thread could record itself in the info object when
ever the object is modified.  This routine could (optionally) return an error
if the list is modified by another thread after \code{MPI_INFO_GET_NKEYS}.

\subsubsection{\mpifunc{MPI_INFO_GET_VALUELEN}}
\begin{verbatim}
lock info
find indicated key and return length of the associated value.
unlock info
\end{verbatim}
Note that the returned length does not include the end-of-string character.

\subsubsection{\mpifunc{MPI_INFO_SET}}
\begin{verbatim}
lock info
find indicated key.  
If found, set the value,
else add the key and value.
unlock info

\end{verbatim}


\subsection{Datatypes}
The ADI defines a datatype structure that is believed to be a good
choice for implementing operatoins that involve datatypes, such as
\mpifunc{MPI_Pack} and \mpifunc{MPI_Unpack}.

\subsubsection{\mpifunc{MPI_ADDRESS}}
Deprecated.  Use \mpifunc{MPI_GET_ADDRESS}.

\subsubsection{\mpifunc{MPI_TYPE_HINDEXED}}
Deprecated.  However, we can't easily use \code{MPI_TYPE_CREATE_HINDEXED}
because that could (if \code{MPI_Aint} is longer than \code{int}) require
making a copy of an array argument.

\subsubsection{\mpifunc{MPI_TYPE_HVECTOR}}
Deprecated.  Use \mpifunc{MPI_TYPE_CREATE_HVECTOR}.

\subsubsection{\mpifunc{MPI_TYPE_STRUCT}}
Deprecated.  For reasons similar to \code{MPI_TYPE_HINDEXED}, we do not want
to call the new function.

\subsubsection{\mpifunc{MPI_GET_ADDRESS}}
See the MPICH-1 \mpifunc{MPI_ADDRESS}.

\subsubsection{\mpifunc{MPI_TYPE_CONTIGUOUS}}
Create a new object (with \code{MPID_Object_create}?) and fill in the dataloop
with type \mpidconst{MPID_Contig}.  

\subsubsection{\mpifunc{MPI_TYPE_INDEXED}}
Create a new object (with \code{MPID_Object_create}?) and fill in the dataloop
with type \mpidconst{MPID_Indexed}.

\subsubsection{\mpifunc{MPI_TYPE_VECTOR}}
Create a new object (with \code{MPID_Object_create}?) and fill in the dataloop
with type \mpidconst{MPID_Vector}.

\subsubsection{\mpifunc{MPI_TYPE_CREATE_DARRAY}}
Create a new object (with \code{MPID_Object_create}?) and fill in the
dataloops (the number depends on the dimension of the darray) 
with type \mpidconst{MPID_Vector}.

\subsubsection{\mpifunc{MPI_TYPE_CREATE_HINDEXED}}
Create a new object (with \code{MPID_Object_create}?) and fill in the dataloop
with type \mpidconst{MPID_Indexed}.

\subsubsection{\mpifunc{MPI_TYPE_CREATE_HVECTOR}}
Create a new object (with \code{MPID_Object_create}?) and fill in the dataloop
with type \mpidconst{MPID_Vector}.

\subsubsection{\mpifunc{MPI_TYPE_CREATE_INDEXED_BLOCK}}
Create a new object (with \code{MPID_Object_create}?) and fill in the dataloop
with type \mpidconst{MPID_BlockIndexed}.

\subsubsection{\mpifunc{MPI_TYPE_CREATE_STRUCT}}
Create a new object (with \code{MPID_Object_create}?) and fill in the dataloop
with type \mpidconst{MPID_Struct}.

\subsubsection{\mpifunc{MPI_TYPE_CREATE_SUBARRAY}}
Create a new object (with \code{MPID_Object_create}?) and fill in the
dataloops (the number depends on the dimension of the subarray) 
with type \mpidconst{MPID_Vector}.

\subsubsection{\mpifunc{MPI_TYPE_CREATE_KEYVAL}}

\subsubsection{\mpifunc{MPI_TYPE_CREATE_RESIZED}}
Create a new object (with \code{MPID_Object_create}?) and copy in the
dataloop from the old type.  Then change the extent of the type as specified
by the \code{extent} argument and the lowerbound by the \code{lb} argument.

\subsubsection{\mpifunc{MPI_TYPE_COMMIT}}
Optimize the datatype for communication.  

Question:  How do we want to organize the optimization code for datatypes?  We
shouldn't embed it within the \code{MPI_TYPE_COMMIT} function.  Should each of
the dataloop types (e.g., \code{MPID_Vector}) have a corresponding routine
that is called with the entire datatype (not just the specific dataloop)?

\subsubsection{\mpifunc{MPI_TYPE_FREE}}
This first calls \mpidfunc{MPID_Datatype_incr} with an increment of $-1$.  If
the returned value is zero, it should free the contents of \code{dataloop} and
then call \mpidfunc{MPID_Datatype_free}.

\subsubsection{\mpifunc{MPI_TYPE_EXTENT}}
Simply uses the \code{extent} field in the structure.

\subsubsection{\mpifunc{MPI_TYPE_LB}}
Simply uses the \code{lb} field in the structure.

\subsubsection{\mpifunc{MPI_TYPE_SIZE}}
Simply uses the \code{size} field in the structure.

\subsubsection{\mpifunc{MPI_TYPE_UB}}
Simply uses the \code{ub} field in the structure.

\subsubsection{\mpifunc{MPI_TYPE_GET_TRUE_EXTENT}}
Simply uses the \code{true_extent} field in the structure.

\subsubsection{\mpifunc{MPI_TYPE_GET_CONTENTS}}
Uses the \code{dataloop} field to access the data uses to create the datatype.
Actually, must use a separate dataloop field so that the arguments that the
user provided are returned, rather than an optimized form).

\subsubsection{\mpifunc{MPI_TYPE_GET_ENVELOPE}}
Uses \code{dataloop} field to identify how the datatype was constructed.
The \code{combiner} type must be one of 

\noindent\mpiconst{MPI_COMBINER_NAMED}\\
\mpiconst{MPI_COMBINER_DUP}\\
\mpiconst{MPI_COMBINER_CONTIGUOUS}\\
\mpiconst{MPI_COMBINER_VECTOR}\\
\mpiconst{MPI_COMBINER_HVECTOR_INTEGER}\\
\mpiconst{MPI_COMBINER_HVECTOR}\\
\mpiconst{MPI_COMBINER_INDEXED}\\
\mpiconst{MPI_COMBINER_HINDEXED_INTEGER}\\
\mpiconst{MPI_COMBINER_HINDEXED}\\
\mpiconst{MPI_COMBINER_INDEXED_BLOCK}\\
\mpiconst{MPI_COMBINER_STRUCT_INTEGER}\\
\mpiconst{MPI_COMBINER_STRUCT}\\
\mpiconst{MPI_COMBINER_SUBARRAY}\\
\mpiconst{MPI_COMBINER_DARRAY}\\
\mpiconst{MPI_COMBINER_F90_REAL}\\
\mpiconst{MPI_COMBINER_F90_COMPLEX}\\
\mpiconst{MPI_COMBINER_F90_INTEGER}\\
\mpiconst{MPI_COMBINER_RESIZED}\\

\subsubsection{\mpifunc{MPI_TYPE_GET_EXTENT}}
Simply uses extent field.

\subsubsection{\mpifunc{MPI_TYPE_GET_NAME}}
Uses the \code{name} field.  Note that the Fortran versions must be careful to
blank-pad the value rather than null-terminating it.

Question: We should have a common routine to handle the Fortran version of all
of the get/set name routines.  What is it?

\subsubsection{\mpifunc{MPI_TYPE_SET_NAME}}
Sets the \code{name} field.  Returns error if supplied name is too long.

\subsubsection{\mpifunc{MPI_PACK}}
Call \mpidfunc{MPID_Pack} with a \code{rank} of \code{MPI_ANY_SOURCE}.

\subsubsection{\mpifunc{MPI_PACK_SIZE}}
Call \mpidfunc{MPID_Pack_size} with a \code{rank} of \code{MPI_ANY_SOURCE}.

Question: One issue is with IMPI \cite{impi}, which requires that there be no
header on any pack buffers.  Do we want to say something about a header on a
pack buffer?  Note that implementing the datatype signature
\cite{gro:mpi-datatypes:pvmmpi00} requires a header. 

\subsubsection{\mpifunc{MPI_UNPACK}}
Call \mpidfunc{MPID_Unpack} with a \code{rank} of \code{MPI_ANY_SOURCE}.

\subsubsection{Heterogeneity}
\label{sec:hetero}
Optimizing for the common case of machines or clusters with a common
data representation is important.  

In MPICH, macros were used to include code that handled heterogeneous
systems.  For MPICH2, I'd prefer to use clearer blocks of code rather
than special macros.  For example,
\begin{verbatim}
#define MPICH_IS_HETERO
...
#else
...
#endif
\end{verbatim}

Questions about the implementation of datatypes:

\begin{enumerate}
\item Should we require alignment of data when packing/unpacking?  The
   problem is in the heterogeneous case, where we'd need to communicate
   the alignment rules, along with byte ordering and data lengths.

\item For nested datatypes, should we allow loop interchange (as NEC did
   in their ``flattening on the fly'' paper)?  We can implement this
   within the current representation by creating new dataloop
   structures for the re-ordered loops.  

\item We need to provide for the important special cases of aligned moves
   of sizes 1, 2, 4, 8, and perhaps 16.

\item We could even compile code to pack and unpack the given datatype
   and dynamically load the code.  PETSc has code for this for some
   user-interface convenience functions.  In general, we could
   consider allowing the pack and unpack functions to be specified as
   part of the datatype, with defaults based on the dataloop
   structures.  A datatype attribute could be used to decide when to
   create a datatype-specific routine.

\item We need to include instrumentation on the pack/unpack functions
   themselves so that we can gather information about the performance
   of the pack/unpack.  Should this be stored by datatype instance?
   Datatype kind (e.g., vector, indexed, struct)?  pack/unpack?

\item Do we need separate pack and unpack descriptions (e.g., if we
   optimize for the transfers by reordering loops, will we want
   different versions for each direction)?

\item In dataloop, kind should include information on basic alignment
   and/or length (to allow fast loops using wide moves based on long
   or double instead of char).

\item For types that do not contain struct, we can preload the entire
   processing stack, since the elements never change (just the
   position on the stack).  This is close to creating a simple nested
   loop structure for an interpreter.  We may want the datatype to
   have a field indicating that it has this feature; alternately, we
   might encode this by specifying a different pack/unpack routine,
   one that preloads the stack and eliminates any code to fill the
   stack during processing.  Another approach that would apply to the
   more general case would be to cause datatypes that have simple
   nested structure to load the entire stack and switch the stack
   interpreter into a mode that knew that the stack had been loaded.

\item Struct alignment (pad) should have optional rules.  That is, we
   need to support all alignment options that a compiler might pick
   (we currently test for this in the MPICH configure).  For systems
   where different padding rules can be specified (e.g., xlc has 4
   different choices), we should allow an environment variable to
   select a different padding rule.  We might implement this by using
   a separate routine for each type of padding, and call a routine to
   compute the padding towards the end of creating a struct datatype.

\item For pack and unpack code, we need to handle the tests for sizes of
    the output buffers efficiently, hoisting the tests out of the
    loops where possible.  

\item For the homogeneous case, some struct types (those that contain
    only basic datatypes) can be changed into indexed types (as if
    they were all \code{MPI_BYTE}).

\item Structs with no gaps (except at the ends, possibly because of 
    structure padding, an \code{MPI_UB}, or an explicit resize,
    should be replaced with a strided type.  In the heterogeneous
    case, this can only be done when the struct contains a single
    basic type.

\item In the heterogeneous case, we may want two different
    representations: one for homogenous communication and one for
    heterogeneous communication.  Thus the datatype structure needs
    several dataloop entries, at least in the heterogeneous case.

\end{enumerate}

\subsection{Groups}

Groups are simple.  The key point here is to make sure that the use of
groups to map from ranks in a communicator to particular destination
process is fast.
To provide a fast implementation of \mpifunc{MPI_COMM_GROUP}, groups must
have reference counts.  

Question: Do we want a special case for the groups of self and comm
world?  This would eliminate a lookup in the group table for the a very
common case.  Should there be an \code{MPID_GROUP_WORLD}?  

Question: What data structure should be used to represent groups?
MPICH-1 used a simple array that mapped rank in a group to rank in the
group of \code{MPI_COMM_WORLD}.  In MPI-2, we can't use
\code{MPI_COMM_WORLD}.  ADI-3 defines ``local process ids'', which
refers simply to the processes known to the current process.

Question: How do we implement \mpifunc{MPI_Group_translate_ranks}?

Actually, groups are simple as long as we don't require a scalable
representation of group membership.  An interesting question is what
sort of representation should be used for truly massively parallel
systems such as Blue Gene.  

Plan: At least initially, the group implementation in ADI-3 will \emph{not} be
scalable.  Each group will have an array that maps ranks to local process ids.

\subsubsection{\mpifunc{MPI_GROUP_RANK}}
Simply return rank field.
\subsubsection{\mpifunc{MPI_GROUP_SIZE}}
Simply return size field.

\subsubsection{\mpifunc{MPI_GROUP_TRANSLATE_RANKS}}
We may want to detect the special case of a group that is a subset of
\code{MPI_COMM_WORLD} (does this imply a flag in the \mpidfunc{MPID_Group}
structure?).  
If either group is not a subset of \code{MPI_COMM_WORLD}, then 
\begin{enumerate}
\item For group2, if necessary, create a new array containing the pairs
  \code{local process id}, \code{local rank}, sorted by \code{local process
    id}. (Note that \mpifunc{MPI_GROUP_FREE} needs to free this array.)
\item For each rank in ranks1, find the corresponding \code{local process id}
  and then search for that in the sorted array in group2.  
\end{enumerate}
Question: should there be a \code{MINIMUM_MEMORY} option that controls when
memory is left allocated to speed subsequent operations?  In this case, we
leave the memory allocated so as to speed subsequent translations.  Normally,
this routine is only used in tracing libraries to convert relative ranks into
absolute ranks; in that case, it is likely to be called frequently.

\subsubsection{\mpifunc{MPI_GROUP_FREE}}
Free all internal fields and then call MPID routine to return objects.

\subsubsection{\mpifunc{MPI_GROUP_COMPARE}}
\begin{enumerate}
\item Check that sizes are the same.  If not, set \code{result} to
  \mpiconst{MPI_UNEQUAL} and return.
\item Check that the elements of \code{lrank_to_lpid} are the same.  If so,
set \code{result} to \mpiconst{MPI_IDENT} and return.
\item Check that the \code{lrank_to_lpid} arrays contain the same values, but
  in a different order.  We could use the same array needed by
  \code{MPI_GROUP_TRANSLATE_RANKS} here.  If those two arrays have the same
  local process ids, return \mpiconst{MPI_SIMILAR}, otherwise return
  \mpiconst{MPI_UNEQUAL}.  Question: should we free the new arrays if we
  allocate them?
\end{enumerate}

\subsubsection{\mpifunc{MPI_GROUP_EXCL}}
\begin{adi3}
  Construct new group from subset of \code{lrank_to_lpid} field of
  input group.

  To conform to Intel test suite error test for duplicate ranks, check for 
  duplicate ranks in the exclusion list.
\end{adi3}

\subsubsection{\mpifunc{MPI_GROUP_INCL}}
\begin{adi3}
  Construct new group from subset of \code{lrank_to_lpid} field of
  input group.

  Make sure to check for duplicate input ranges (invalid).
\end{adi3}

\subsubsection{\mpifunc{MPI_GROUP_RANGE_EXCL}}
\begin{adi3}
  Construct new group from subset of \code{lrank_to_lpid} field of
  input group.  This is a little tricky because multiple ranges can be
  specified and they can exclude overlapping ranges of ranks.  One easy way is
  to simply create a new array of markers set to zero, use the ranges to set
  the values to 1 for each element specified by the range, and then assemble
  a group from what's left (calling \mpifunc{MPI_GROUP_INCL}, of course).
\end{adi3}

\subsubsection{\mpifunc{MPI_GROUP_RANGE_INCL}}
\begin{adi3}
  Construct new group from subset of \code{lrank_to_lpid} field of
  input group.
\end{adi3}

\subsubsection{\mpifunc{MPI_GROUP_DIFFERENCE}}

This should use the local process id and rank array to identify the different
processes to include.  Note that this includes only the elements of the first
group that are not in the second group, ordered as in the first group.  
This can first save the indexes of entries that will be used; that array of
indices in then sorted by the rank-in-group field, and the new group can be
created by calling \mpifunc{MPI_GROUP_INCL} (or an internal version that works
directly with the non-opaque data structures and assumes valid inputs).

Note: the MPICH-1 implementation has complexity that is the product of the
sizes of the groups (!).

\subsubsection{\mpifunc{MPI_GROUP_INTERSECTION}}

This is implemented similarly to \mpifunc{MPI_GROUP_DIFFERENCE}.

Note: the MPICH-1 implementation has complexity that is the product of the
sizes of the groups (!).

\subsubsection{\mpifunc{MPI_GROUP_UNION}}

This is implemented similarly to \mpifunc{MPI_GROUP_DIFFERENCE}.

Note: the MPICH-1 implementation has complexity that is the product of the
sizes of the groups (!).

\subsection{Communicators}

Communicators have two main features: a context id and a group.  In
addition, communicators that are created with \mpifunc{MPI_Comm_dup} must
copy attributes (where requested) from the old communicator.

Question: do we want a ``hidden'' communicator for implementing the
collective routines?  Just a hidden context?  One concern when there
are two communicators: which do you lock (e.g., with
\mpidfunc{MPID_Comm_thread_lock})?  How do you avoid deadly embraces?  How do
you 
ensure that the expected communicator is acted on?

If a hidden communicator is used, it should have its error handler permanently
set to \mpifunc{MPI_ERRORS_RETURN} so that an error action that is
appropriate to 
the collective routine, not the routine called, may be used.

What utility routines do we wish to define?  There are a number of
routines that create communicators, including the topology routines.
Note that attributes are only copied by \mpifunc{MPI_Comm_dup}.

\subsubsection{\mpifunc{MPI_COMM_COMPARE}}
This compares first the \code{context_id} values (Question: this assumes that
we don't use the same \code{context_id} for communicators with disjoint
groups).  If the same, return \mpiconst{MPI_IDENT}.  Other wise, call
\mpifunc{MPI_GROUP_COMPARE} for the remote group (and if both are
intercommunicators, local group).  If the group comparision(s) return
\mpifunc{MPI_IDENT}, then return \mpifunc{MPI_CONGRUENT}.  Otherwise, return
the same value as given by \mpifunc{MPI_GROUP_COMPARE}.

\subsubsection{\mpifunc{MPI_COMM_CREATE}}
We need a basic communicator creation routine for this.  In
particular, many of the communicator construction routines can create
a group and then use that to specify the communicator.  

\subsubsection{\mpifunc{MPI_COMM_DUP}}
One approach is to extract the group from the incoming communicator,
invoke \mpifunc{MPI_Comm_create}, and then invoke the attribute
copying step.

Question: Who (if anyone) guarantees that two threads don't run the same
attribute copy functions at the same time?  The standard is silent here, but
some examples use code where the attribute is a pointer to storage that holds
an integer (e.g., a private tag) and the copy routine performs (without
locking) a fetch and increment.  Do we want to allow/force the attribute copy
functions to behave like Java synchronized methods?

This question was posed to the MPI Forum and the answer was that
because any operation (including communication) is permitted, it isn't
permissible to lock around the attribute copy routines.  Thus, we can
only warn the user on the man page.

\subsubsection{\mpifunc{MPI_COMM_FREE}}
This routine must invoke the attribute delete functions for each attribute.

\subsubsection{\mpifunc{MPI_COMM_GROUP}}
\begin{adi3}Return a duplicate (shallow copy) of the local group.  Call
  \mpidfunc{MPID_Group_incr}. 
\end{adi3}

\subsubsection{\mpifunc{MPI_COMM_RANK}}
Return the \code{rank} field.

\subsubsection{\mpifunc{MPI_COMM_REMOTE_GROUP}}
\begin{adi3}Return a duplicate (shallow copy) of the remote group.  Call
  \mpidfunc{MPID_Group_incr}. 
\end{adi3}

\subsubsection{\mpifunc{MPI_COMM_REMOTE_SIZE}}
See \code{MPI_COMM_SIZE}.

\subsubsection{\mpifunc{MPI_COMM_SIZE}}
Return the \code{size} field.  This is really the size of the local
group, which for an intercommunicator may be different from the remote
size.  Note that for point-to-point communication, error checking for
destination or source ranks must look at the remote size.  Should
there be a \code{local_size} field in the communicator?

\subsubsection{\mpifunc{MPI_COMM_SPLIT}}
Perform an Allgather on the color and key.  Create the subgroups,
ordered according to the color.  Use \mpifunc{MPI_Comm_create} to
build the new communicator.

\subsubsection{\mpifunc{MPI_COMM_TEST_INTER}}
Should this simply test to see if the remote group and the local groups are
the same, or should there be a separate communicator ``kind'' field?  Note
that the C++ binding defines 4 kinds of communicators: intercomm, intracomm,
graphcomm, and cartcomm. 

\subsubsection{\mpifunc{MPI_INTERCOMM_CREATE}}
The local leaders exchange messages with the remote leaders to gather the
information on the two groups and to agree on a context id.  (Question: should
a debugging version ensure 
that consistent local and remote leader ranks are specified?)  
Leaders then broadcast information to their respective groups (using
\mpiconst{PMPI_Bcast}). 

Question:  Should intercommunicators contain a private, internal
intracommunicator for use by intercommunicator collective operations including
\mpifunc{MPI_Comm_dup}?

\subsubsection{\mpifunc{MPI_INTERCOMM_MERGE}}
Create a new group from the union of the local and remote groups.  Rank 0 in
the group with \code{high == true} communicates with rank 0 in the group with
\code{high == false}.  Once this group is created, call
\mpifunc{MPI_COMM_CREATE} with this group.

\subsubsection{\mpifunc{MPI_COMM_CLONE}}
This is a special C++ function; it behaves similarly to
\mpifunc{MPI_COMM_DUP}, but returns a reference (pointer) to the created
communicator, rather than the communicator itself.  This is necessary because
the C++ binding makes a Comm an abstract base class, so you can't dup a
general communicator, just one of the four derived classes.
Question: should we design an internal dup function so that both
\mpifunc{MPI_COMM_DUP} and the C++ \code{MPI::Comm::Clone} function can use it?

\subsubsection{\mpifunc{MPI_COMM_GET_NAME}}
Return a copy of the \code{name} field.

\subsubsection{\mpifunc{MPI_COMM_SET_NAME}}
Set the \code{name} field.  Check for valid length.

\subsection{Point to Point Communication}

The ADI provides a relatively close match to the point-to-point
communication routines.  

Questions that remain:  Handling of persistent requests.  The ADI
contains memory registration.  Is anything else needed for persistent
requests? 

Since requests are allocated by the \mpidfunc{MPID_Request_xxx_FOA} routines,
should a persistent request simply have a pointer to the active request?  The
request pointer could be null to indicate an inactive persistent request.

\subsubsection{\mpifunc{MPI_PROBE}}
\begin{adi3}\mpidfunc{MPID_Request_probe}
\end{adi3}

\paragraph{Buffered send.}
The buffered send (both blocking and nonblocking) should first use
\mpidfunc{MPID_tBsend} to attempt 
and send the message before implementing the buffer-copying strategy.

Question: should the request needed to implement a buffered send be allocated
from the user-supplied buffer?  The standard suggests so, though it isn't
strictly required.  For example, only the information needed to create
the request could be saved; if no request is available, the bsend
handler can wait until later.

What utility routine should be defined to allocate buffer space from the
user-specified buffer?  How will it be made thread-safe?  What is the
interface to \mpifunc{MPI_REQUEST_FREE}?  How do we ensure that we wait on
pending bsend operations in a polling implementation?  See the file
mpich/src/util/bsendutil2.c in MPICH-1.

The bsend operations are roughly:
\begin{adi3}
Try \mpidfunc{MPID_tBsend}; if it succeeds, done.
Otherwise, find the first block in the buffer that is large enough for
the data, stored as packed with \mpifunc{MPI_Pack}.  
Save: the data, the type of the data (e.g., \code{MPI_PACKED} or some
contiguous type), the count, and the \code{MPI_Request} used to start
an \mpifunc{MPI_Isend} on the data.

As a slight refinement, we should be prepared to save the
communicator, tag, and rank, so that if no request is currently
available, the communication can be deferred until later.  SGI doesn't
do this currently, and as a result, their implementation fails on some
valid MPI programs.
\end{adi3}

\subsubsection{\mpifunc{MPI_IBSEND}}
See Buffered send.

\subsubsection{\mpifunc{MPI_BSEND}}
See Buffered send.

\subsubsection{\mpifunc{MPI_BSEND_INIT}}
Question:  Should this reserve space in the buffer that is used as
necessary by \code{MPI_Start}?  No, that doesn't conform to the
standard's model implementation.  This should setup a persistant
request in the same way as the other persistent routines.

\subsubsection{\mpifunc{MPI_BUFFER_ATTACH}}
\begin{adi3}
If a buffer is already attached, return error.
Otherwise, attach the designated buffer and initialize the buffer as empty.
The buffer is organized as a circular buffer as described in the model
implementation of buffered mode in the MPI standard.
\end{adi3}

\subsubsection{\mpifunc{MPI_BUFFER_DETACH}}
Buffer detach must first wait for all operations to complete before
returning.  
In order to catch race conditions in a multi-threaded environment, 
\code{MPI_Buffer_detach} should set a flag on the buffer on entrance; all
buffered send operations should check this value before proceeding, generating
a \mpiconst{MPI_ERR_OTHER} class of type \code{THREAD_RACE} if the flag is
set.

\subsubsection{\mpifunc{MPI_CANCEL}}
\begin{adi3}\mpidfunc{MPID_Request_cancel}
\begin{mmadi}??
Start by separating the cases:
\begin{itemize}
\item Inactive persistent.  Return error.
\item Active persistent.  Handle as a non-persistent of the same type.
\item Receive.  If already complete, cancel fails.  
\begin{core} Otherwise, remove request from receive queue.  In many
ADIs, this is a local operation.  However, some systems may require
logic similar to the send branch.
\end{core}
\item Send. If already complete or in progress, cancel fails.
\begin{core} Otherwise use \mpidfunc{MPID_Rhcv} to send a cancel
request.  The cancel operation must wait for a acknowledgement that
indicates whether the cancel succeeded or failed.  
\end{core}
\end{itemize}
\end{mmadi}
\end{adi3}

\subsubsection{\mpifunc{MPI_GET_COUNT}}
How does this get the value?
Question: Does this belong with datatypes? (Yes)

\subsubsection{\mpifunc{MPI_GET_ELEMENTS}}
How does this get the value?
Question: Does this belong with datatypes? (Yes)

\subsubsection{\mpifunc{MPI_IPROBE}}
\begin{adi3}\mpidfunc{MPID_Request_iprobe}
\end{adi3}

\subsubsection{\mpifunc{MPI_IRECV}}
\begin{adi3}\mpidfunc{MPID_Irecv}
\begin{mmadi}\mpidfunc{MPID_Request_recv_FOA}\\
\mpidfunc{MPID_Segment}\\
\mpidfunc{MPID_Stream_irecv}
\begin{core}
This should ensure that any polling call is made.
\end{core}
\end{mmadi}
\end{adi3}

\subsubsection{\mpifunc{MPI_IRSEND}}
Is this the same as \code{MPI_ISEND}, but with the ready mode set?  Or is
there a separate routine that exploits the ready nature of the operation?
Special note: we should be able to report an error when a ready send is not
matched by a posted receive.

\subsubsection{\mpifunc{MPI_ISEND}}
\begin{adi3}\mpidfunc{MPID_Isend}
\begin{mmadi}\mpidfunc{MPID_Request_send_FOA}\\
\mpidfunc{MPID_Segment}\\
\mpidfunc{MPID_Stream_isend}
\begin{core}
\mpidfunc{MPID_Rhcv}
\end{core}
\end{mmadi}
\end{adi3}

\subsubsection{\mpifunc{MPI_ISSEND}}
Same as \code{MPI_ISEND}, but with the synchronous mode set.

\subsubsection{\mpifunc{MPI_RECV}}
This can simply be \code{MPI_IRECV} followed by an \code{MPI_WAIT}.  However,
we might want to let the partner (matching sender) know that this is blocking
a thread or process.  

\subsubsection{\mpifunc{MPI_RECV_INIT}}
\begin{adi3}Create a persistent request and save the message
parameters (communicator, tag, rank, datatype, buffer, and count).
\code{MPID_Memory_register} for either the buffer or a created segment
that will be used for receiving the data.  

Note that a persistent request is not like an
\mpidconst{MPID_Request}; rather, it only contains enough information
to identify it as a persistent request and a pointer to a normal
\mpidconst{MPID_Request}.  In fact, the pointer to the request can be
used to indicate whether the persistent request is active, rather than
using a separate field.
\end{adi3}

\subsubsection{\mpifunc{MPI_REQUEST_GET_STATUS}}
Extract the status data from the request.
This requires either an \mpidfunc{MPID_Request_get_status} or clearly defined
status elements in the \mpidfunc{MPID_Request}.

\subsubsection{\mpifunc{MPI_REQUEST_FREE}}
\begin{adi3}\mpidfunc{MPID_Request_free}
\begin{mmadi}
\begin{core}
\end{core}
\end{mmadi}
\end{adi3}
Who handles persistent or user-defined requests?

Note that this is not the same as cancelling a request.  
A request that is freed must still complete.  Thus, the request needs
a reference count that must be checked when the related communication
completes.  If the count is zero, there will be no \code{MPI_Wait}
etc. call, and the request data strucutre must be recovered.  A
polling device that expects a wait or test call may need to maintain a
list of freed but not completed requests and effectively call
\mpidfunc{MPID_Testsome} on that list.

\subsubsection{\mpifunc{MPI_RSEND}}
Like \mpifunc{MPI_Send}, but with the ready mode.

\subsubsection{\mpifunc{MPI_RSEND_INIT}}
See \mpifunc{MPI_RECV_INIT}.

\subsubsection{\mpifunc{MPI_SEND}}
See \mpifunc{MPI_ISEND}.  This could simply be isend followed by wait.

Question: do we want to indicate to the receiver that this is a
blocking send?

\subsubsection{\mpifunc{MPI_SENDRECV}}
Question: If the source and destination are the same, is there anything
special that we want to do?  Note that this routine matches communication from
any point-to-point operation, not just other sendrecv calls.  
Simply use \mpifunc{MPI_Isend}, \mpifunc{MPI_Irecv}, and
\mpifunc{MPI_Waitall}. 

\subsubsection{\mpifunc{MPI_SENDRECV_REPLACE}}
Question: Should this use the segment code to bound the memory buffer that
must be allocated for the replacement?

\subsubsection{\mpifunc{MPI_SEND_INIT}}
See \mpifunc{MPI_RECV_INIT}.

\subsubsection{\mpifunc{MPI_SSEND}}
Like \mpifunc{MPI_Send}, but with the synchronous mode.

\subsubsection{\mpifunc{MPI_SSEND_INIT}}
See \mpifunc{MPI_RECV_INIT}.

\subsubsection{\mpifunc{MPI_START}}
This just calls \mpifunc{MPI_STARTALL} with a single request.  Note that
errors must indicate that the error occurred in \code{MPI_START}, not
\code{MPI_STARTALL}.  

\subsubsection{\mpifunc{MPI_STARTALL}}
Do we want startall to allow for some scheduling of the operations?  For
example, it could start the ``furthest away'' first.  It could also batch
operations (see the paper on improving TCP performance).

Should each of the individual persistent routines provide an internal
routine that is used to start the operation?

\subsubsection{\mpifunc{MPI_STATUS_SET_CANCELLED}}
Where are the values defined for indicating cancelled message?
A \mpidconst{MPID_TAG_MSG_CANCELLED} for the tag field in the status?

\subsubsection{\mpifunc{MPI_STATUS_SET_ELEMENTS}}
Where are the values defined for the count field(s) in the status?
Should this be with the datatype?

\subsubsection{\mpifunc{MPI_TEST}}
Call \mpifunc{MPI_Testall}.

\subsubsection{\mpifunc{MPI_TESTALL}}

\subsubsection{\mpifunc{MPI_TESTANY}}
Call \mpidfunc{MPID_Testsome} on each request, on at a time.

Question: do we want something better?  Can we subdivide
\mpidfunc{MPID_Testsome} into smaller building blocks from which we
can build the four test routines without dropping down to an
\mpidfunc{MPID_Test}?

\subsubsection{\mpifunc{MPI_TESTSOME}}
Call \mpidfunc{MPID_Testsome}.

\subsubsection{\mpifunc{MPI_TEST_CANCELLED}}
Does this use the status field in a request to test for a cancelled message?

\subsubsection{\mpifunc{MPI_WAIT}}
Call \mpifunc{MPI_Waitany}.

\subsubsection{\mpifunc{MPI_WAITALL}}
See \mpifunc{MPI_Testall}.

\subsubsection{\mpifunc{MPI_WAITANY}}
See \mpifunc{MPI_Testany}.

\subsubsection{\mpifunc{MPI_WAITSOME}}
Call \mpidfunc{MPID_Waitsome}.

\subsection{Collective Communication and Computation}
One of the major changes in MPICH2 is in the implementation of the
collective routines.  The MPICH2 implementation will exploit
pipelining and store and forward algorithms; these are supported by
the \code{MPID_Stream_xxx} routines.  

Since each system may have some feature that provides for even faster
implementation of the collective routines, it will be possible to
substitute a system-specific implementation for any of the collective
routines.  The purpose of the implementations provided with MPICH is
to provide a level of performance that will be adequate
for many users.  

The $\alpha$-tree approach described in
\cite{bern:mpi-collective:hpcn99} should be considered; this is a
simple variation on the binomial tree approach used in the MPICH
implementations of many of the collective routines.  We will consider
combining this with the pipelining and scatter/gather approaches
championed by van de Geijn (\cite{vandegeijn} isn't quite the right
reference but it will do for now).


\subsubsection{Reduction functions}
The reduction functions must use the \code{restrict} qualifier.

Question:
The MPICH code uses one gigantic file, \file{global_ops.c}, to implement
all of the reduction functions.  There are two problems with this.
First, some compilers become unhappy with it and do not optimize it
very well.  Second, all applications must load all of the routines
even though only a few (typically one) reduction function is used.  We
could break this into separate routines for each operation.  Those
could further be broken down by basic datatype, since the datatype is
known by the routine that calls the specific reduction function.  
For example, we could have \code{MPIR_SUM_Double},
\code{MPIR_SUM_Int}, etc.  This would also allow us to use Fortran
code for some or all of these routines, since Fortran compilers
typically produce better code for this kind of operation (though the
new definitions, using \code{restrict}, may be much better than the
current C code).

The down side of this is that, particularly in unstripped code, each
file (particularly if it includes any significant header files)
includes a significant amount of information.  A latency, if you will,
for each file.  That is, if putting all of the routines into a single
file takes $n$ bytes, putting them into $k$ files takes $n + (k-1)m$,
where $m$ is the size of the header.  In practice, the value of $m$ can be
relatively large (several kilobytes).

If we want to use the Fortran compiler for some or all of these, we'll
need a Fortran compiler and a backup when there is no Fortran compiler.

\subsubsection{Code Structure for the Implementation of the Collective
  functions} 

The MPICH code uses one gigantic file, \file{interops.c}, to provide a
generic implementation of each collective operation.  Each
communicator has a structure of pointers to functions.  Unless
otherwise set, each communicator points to the predefined structure
\code{MPIR_intra_collops}\index{MPIR_intra_collops} which is
initialized to point to \emph{all} of 
these functions.  

For MPICH2, I'd prefer that the default case have no structure, and
each of the MPI functions (in its own file) contains the generic
implementation of the collective operation, based (most likely) on the
stream operations.  This will simplify the process of tuning each
operation; it will also reduce the size of executables since few if
any programs use all of the collective operations.
See the discussion of the implementation of PMPI.

Question: Now that MPI-2 defines intercommunicator collective
routines, do we want these in the same file as the intracommunicator
routines, or in a alternate file.  E.g., should \file{bcast.c} contain
the intracommunicator implementation of \mpifunc{MPI_Bcast} and
\file{ibcast.c} contain the intercommunicator implementation.

We may want to have multiple ``generic'' implementations and an easy
way, say with environment variables, to select among them at runtime.

We may want to compute and save things like the neighbors for each
collective communication pattern; this can be done either when the
collective operation is first encountered or at communicator creation
time.  Question: how do we modularize this?  Is there a ``collective''
attribute?

\subsubsection{Collective Computation}
\mpiconst{MPI_Op} need to have reference counts.  How do we handle the
predefined types?  

\subsubsection{\mpifunc{MPI_OP_CREATE}}
Create the object and set the kind, language, and function.  Use an
internal routine that can be shared with the init routine to create
the predefined operations.

\subsubsection{\mpifunc{MPI_OP_FREE}}
If predefined and not in finalize, indicate error.
Otherwise, free.  

Question: do we need a reference count?  Not in a single threaded
case, an in a multithreaded case, you could argue that a user that
frees an \mpiconst{MPI_Op} while a collective routine is using it in
another thread.

\subsubsection{Intracommunicator Collective Operations}
The following section (will) briefly describe the algorithms used to implement
the intracommunicator collective operations.

Several functions support \mpiconst{MPI_IN_PLACE} as an argument.

One important check is to test for mismatched collective operations.
MPICH uses a different tag value for communication for each collective
operation, but has no way to test for a mismatch (because the
communication selects on tag and, without preceeding all communication
with an \mpifunc{MPI_Iprobe} call to check that the ``next'' message
has the right tag.  We might want a routine that returns an
``unexpected message'' when it finds a message with a different tag
from a particular source and communicator.
That is, if the communication is a virtual stream (virtual in the
sense of being separate for each communicator/rank pair), then it is
an error to see a message with an different tag value.  

Question: do we need an MPID routine to implement this? Is it a
(optional) feature of the stream routines?

We also want to provide the option to check other parameters in
collective calls, for example, that the message sizes conform or that
all processes agree on the root.

\subsubsection{\mpifunc{MPI_ALLGATHER}}
For short data, use recursive doubling algorithm.  For long data, consider the
bucket brigade algorithm.

\subsubsection{\mpifunc{MPI_ALLGATHERV}}
Same as \mpifunc{MPI_ALLGATHER}, since all processes can make the
short/long determination.

\subsubsection{\mpifunc{MPI_ALLREDUCE}}
Consider recursive doubling algorithm, with care taken to ensure that all
results are the same, particularly in the presence of extended registers
(e.g., 80 bit intermediate quantities on Intel).

\subsubsection{\mpifunc{MPI_ALLTOALL}}
For short data and for all data on completely connected networks, use a
hypercube algorithm: each process exchanges with its partner in that dimension
the data needed by the partner and that partner's subsequent partners (in the
remaining dimensions).

For long data on less capable networks, use a bucket brigade algorithm.

\subsubsection{\mpifunc{MPI_ALLTOALLV}}
This is similar to \code{MPI_ALLTOALL}, but the decision on size of data is
more complicated.  

\subsubsection{\mpifunc{MPI_ALLTOALLW}}
Like \code{MPI_ALLTOALLV}.

\subsubsection{\mpifunc{MPI_BARRIER}}
Barrier is a special case of allreduce with no operation.

\subsubsection{\mpifunc{MPI_BCAST}}
Broadcast will be implemented by a scatter followed by an allgather.  These
will use an ordering of nodes from \mpidfunc{MPID_Topology_xxx}, rather than
the rank ordering of the communicator.

\subsubsection{\mpifunc{MPI_EXSCAN}}
See \code{MPI_SCAN}.

\subsubsection{\mpifunc{MPI_GATHER}}
This will use a MST for short gathers to reduce the impact of latency.  

\subsubsection{\mpifunc{MPI_GATHERV}}
Like \mpifunc{MPI_Gather}, but the amount of data sent can be different in
each process.  The root process knows what is coming from each other process,
but the other processes can't tell how much data is being moved.  Thus, it
can't easily choose to use different algorithms for short and long messages.  

\subsubsection{\mpifunc{MPI_REDUCE}}
Use a spanning tree.  Pipeline for long vectors.

\subsubsection{\mpifunc{MPI_REDUCE_SCATTER}}
For short data, this can use \mpifunc{MPI_Reduce} followed by
\mpifunc{MPI_Scatterv}.  On 
complete networks, it is possible to implement this by using hypercube-like
exchange algorithms.

For long data, this should use the bucket brigade algorithm.

\subsubsection{\mpifunc{MPI_SCAN}}
Use a spanning tree.  Pipeline for long vectors.

\subsubsection{\mpifunc{MPI_SCATTER}}
For short data, use an MST. On store and forward networks, MST should be used
for long data as well.  To avoid excessive memory consumption, the
\mpidfunc{MPID_Stream_iforward} routines should be used.

\subsubsection{\mpifunc{MPI_SCATTERV}}
For short data, use an MST. 

\subsubsection{Intercommunicator Collective Operations}
\subsubsection{\mpifunc{MPI_ALLGATHER}}
\subsubsection{\mpifunc{MPI_ALLGATHERV}}
\subsubsection{\mpifunc{MPI_ALLTOALL}}
\subsubsection{\mpifunc{MPI_ALLTOALLV}}
\subsubsection{\mpifunc{MPI_ALLTOALLW}}
\subsubsection{\mpifunc{MPI_BARRIER}}
\subsubsection{\mpifunc{MPI_BCAST}}
\subsubsection{\mpifunc{MPI_ALLREDUCE}}
\subsubsection{\mpifunc{MPI_REDUCE}}
\subsubsection{\mpifunc{MPI_EXSCAN}}
\subsubsection{\mpifunc{MPI_GATHER}}
\subsubsection{\mpifunc{MPI_GATHERV}}
\subsubsection{\mpifunc{MPI_SCAN}}
\subsubsection{\mpifunc{MPI_SCATTER}}
\subsubsection{\mpifunc{MPI_SCATTERV}}

\subsection{Topology}

How do we implement \mpifunc{MPI_Cart_create} and \mpifunc{MPI_Dims_create}
with the MPID routines?   
Do we need an \mpidfunc{MPID_Topology_cart} and
\mpidfunc{MPID_Topology_cart_dims}?  Constructing a mesh from the 
hierarchical description that we've included can only be done
approximately.

The MPICH implementation uses private attributes to hold this information 
within a communicator.  Do we still want to do that?  Do we want to make the
topology routines a separate module so that, at the MPI level, it is easy to
substitute for these routines?  How do we define the routine called by
\mpifunc{MPI_Init} to initialize these routines (e.g., acquire attribute keys
for the topology information)?

One advantage to using attributes (or equivalently a pointer to a
structure) is it allows any information to be saved in with the
communicator, not just some predefined fields.  Note that
\mpifunc{MPI_Comm_dup} is required to copy both attributes and
topologies, so it makes sense to implement topologies as an attribute.

If attributes are used for holding topology information, we need to
\emph{add} a routine to initialize the corresponding keyval; this will
allow the topology routines to be replaced as a module.

Question: how do we define routines needed to support the MPI (not
MPID) calls?  Should we have \code{MPIi} routines?  For topology, we
could have \code{MPIi_Topo_init}.

\subsubsection{\mpifunc{MPI_CARTDIM_GET}}
Access the topology description and return the number of dimensions of
Cartesian topology (if defined).

\subsubsection{\mpifunc{MPI_CART_CREATE}}
This routine argues for a corresponding MPID routine, along with one for dims
create. Alternately, as suggested by the MPI standard, this could call
\mpifunc{MPI_CART_MAP} followed by \mpifunc{MPI_COMM_SPLIT}.

\subsubsection{\mpifunc{MPI_CART_GET}}
Access the topology description and return the associated fields.

\subsubsection{\mpifunc{MPI_CART_MAP}}
This routine should call \mpidfunc{MPID_Cart_map}, which will have a default
implementation.  

\subsubsection{\mpifunc{MPI_CART_RANK}}
Access the topology description and convert the specified Cartesian
coordinates into a rank.

\subsubsection{\mpifunc{MPI_CART_SHIFT}}
This routine accesses the topology description and computes the requested
shifted rank.

\subsubsection{\mpifunc{MPI_CART_SUB}}
This routine can be implemented with \mpifunc{MPI_COMM_SPLIT} (see the MPI-1
standard, section 6.5.7 ``Low-level topology functions'').  It may also
want to call \mpidfunc{MPID_Cart_map} to allow subdimensions to be reordered.

\subsubsection{\mpifunc{MPI_DIMS_CREATE}}
This routine calls \mpidfunc{MPID_Dims_create}.

\subsubsection{\mpifunc{MPI_GRAPHDIMS_GET}}
Access the topology description and return the number of dimensions of
the nodes and edges of a graph topology (if defined).

\subsubsection{\mpifunc{MPI_GRAPH_CREATE}}
This is implemented using \mpifunc{MPI_GRAPH_MAP} and
\mpifunc{MPI_COMM_SPLIT}. 

\subsubsection{\mpifunc{MPI_GRAPH_GET}}
Access the topology description and return the associated fields.

\subsubsection{\mpifunc{MPI_GRAPH_MAP}}
This should eventually have an MPID routine, but not in ADI-3.

\subsubsection{\mpifunc{MPI_GRAPH_NEIGHBORS}}
Access the topology description and return the associated fields.

\subsubsection{\mpifunc{MPI_GRAPH_NEIGHBORS_COUNT}}
Access the topology description and return the associated fields.

\subsubsection{\mpifunc{MPI_TOPO_TEST}}
Return \mpiconst{MPI_GRAPH} for graph topology, \mpiconst{MPI_CART} for
Cartesian topology, and \mpiconst{MPI_UNDEFINED} otherwise.

\subsection{RMA}

My original plan was to implement this using the \code{Segment},
\code{Rhcv}, \code{Put_contig} and \code{Get_contig} routines.  We
will need code to support datatype caching at the destination process.
We may want to provide a way to define datatypes in globally shared
memory for systems like large SMPs that provide global access to at
least some memory.  Currently, there is no ADI interface for that.
I have since added additional put/get for the case where the origin
and target datatypes are the same.  

Question:  Should there be a model of remotely-defined datatypes that
would allow processes to avoid caching the description?  How would
this work in the multi-method case where some processes might have
shared memory and others might not?

For systems with ordered delivery, we may want a simpler completion
model, one that has completion per destination process (or per process
per window) rather than per RMA operation.  This is a further reason
to require that completion flags be created, and that this creation
contain both destination process and window.  Where operations are
ordered, this flag can simply count the number of started but not
completed operations, or it could contain a sequence number of some
sort for the most recent operation.  

Question:  For this to work with the waitflags and testflags, we
really need a flag set for the RMA window, which each RMA operation
takes (instead of a separate flag address).  How should the API for
both the flag set creation, reference, and completion work?  

The current ADI-3 interface defines put and get operations for both
contiguous data (at both origin and target) and for the case where the
same datatype is used at both origin and target.  Who is responisble
for the other cases?  The MPICH code or the ADI code?

The completion flags for the \code{MPID_Put_contig} etc. operations
have not be throughly though out.  For example, there is no explicit
support for the group-based window completion (\code{MPI_Win_post}
etc.), nor is there simple support for systems like the Cray T3E that
have (roughly) hardware support for \mpifunc{MPI_Win_fence}.

Question: The MPI RMA design is actually pretty lean and general, and
without further constraints or properties, it is hard to create a
simpler interface.  However, we might be able to simplify by
considering three important cases:
\begin{description}
\item[Shared Memory]This is not fully shared, but shared memory
segments or shared \code{mmap} regions. There may need to be special
calls to enforce memory ordering and coherency.
\item[Distributed Memory with DMA]This is for systems that support
some one-sided data delivery, such as VIA or LAPI.
\item[Distributed Memory with no DMA]This is for simple
network-connected processes, such as Unix processes connected by TCP.
\end{description}
Question: are these sufficient?  Should we put these classes into the
method-based interface instead?

For example, where shared memory is available, the synchronization and
lock operations can act directly on the shared memory area that is
allocated as part of the window object.  For example, the
start/post/complete/wait can use counters and flags in shared memory.
Locks can be acquired directly and quickly in shared memory, and (for
the passive target operations), the RMA operations can then be done
directly in shared memory.

In contrast, in the distributed memory case, particular with high
latency interconnects, deferred synchronization can be used.  For
example, a \code{MPI_Win_lock} in that case could return immediately.
At the first RMA operation, particularly if the amount of data is
small, the request for a lock can be piggy-backed on the RMA request.
In fact, following the BSP style, all of the RMA operations could be
held until the \code{MPI_Win_unlock}.

Clearly, the choice of immediate or deferred locks depends on the kind
of communication between processes.

Question: are there any special values for window objects similar to the ones
considered for datatypes and communicators?  For example, one bit could
indicate whether all windows of the window object are in shared memory.

To remove the complexity of datatypes, we might want a
\mpidfunc{MPID_Stream_put} that acts on a segment, rather than using several
special-case versions of put.  It would still need to work on \emph{two}
segments; that is, both the origin and targets.

\subsubsection{\mpifunc{MPI_ACCUMULATE}}
\begin{tcp}
The assumption here is that the target process must perform the operation.

Determine if destination datatype is known at target.  If not, send it using
\mpidfunc{MPID_Rhcv} with a type of \mpidconst{MPID_Hid_datatype_desc}.  Note
that if this is a complex datatype, this operation may require a rendezvous.

In that case, do we want a \mpidconst{MPID_Hid_datatype_desc_rts} (rts for
request to send)?  
\mpidconst{MPID_Hid_datatype_desc} needs either to specify an id for this
datatype (for future use) or an acknowledgment needs to indicate what id to
use.  Note that the ids are not global; beyond the predefined types, the ids
are valid only between the particular pair of processes that established them.

This operation should have an MPID routine since Put and Get also require it. 

Once the datatype is known at the target, the data can be sent with
\mpidfunc{MPID_Stream_isend}.  Using a stream allows the receiver to receive
part of the data and combine that with the target buffer without either
allocating a temporary buffer the size of the full message or waiting for the
all of data to be delivered.  In fact, a double-buffer arrangement can be used
(the Stream operations should support this).  

Question: how is this stream matched between the sender and receiver?  Should
this use a ``Stream_put'' instead, where the target sends the buffer address
to use, or do we just use a context from a communicator within in the Window
object, combined with a unique tag value?

Note that high-latency systems may want to defer any communication until the
access epoch completes (e.g., the closing \code{MPI_Win_fence} or
\code{MPI_Win_unlock}.  Exploiting that requires merging messages into a
single message (as seen by the OS).  The above description doesn't handle this
case. 
\end{tcp}

\begin{shmem}
\begin{enumerate}
\item If the target window is in shared memory, 
If both origin and target datatypes are simple, then the origin process simply
applies the 
operation (e.g., both contiguous or both vectors).  Otherwise, move through an
intermediate form (e.g., contiguous).  

An alternate implementation would have the target process rather than the
origin process perform the operation.  The difficulty with this is that the
origin buffer need not be in shared memory, so it is less likely that this
single-move form can be carried out.

\item Otherwise (If the target window is not in shared memory),
use the \tcpname\ code (send datatype description, use stream to deposit
data).

\end{enumerate}
\end{shmem}
\begin{via}
Use the \tcpname\ code.  
\end{via}

One question is whether active and passive target operations should be handled
separately.  For example, a TCP device could establish two sockets for each
communication path; one to be used for active target operations and one for
passive.  The passive socket could be handled by a separate thread while the
active socket could be handled by routines invoked by the main thread, thus
eliminating a context switch on active-target operations (active target could
include MPI-1 communication, particular blocking calls).

\subsubsection{\mpifunc{MPI_PUT}}
\begin{adi3}\mpidfunc{MPID_Put}
\begin{mmadi}If target and origin datatype are contiguous, use
  \mpidfunc{MPID_Put_contig}.  Otherwise, if they are the same (and system is
  homogeneous?), use \mpidfunc{MPID_Put_sametype}.  
  Otherwise, what?

\begin{tcp}
Like \code{MPI_ACCUMULATE}, with \mpiconst{MPI_REPLACE} as the operation.  
\end{tcp}

\begin{shmem}
Like \code{MPI_ACCUMULATE}, with \mpiconst{MPI_REPLACE} as the operation.  
\end{shmem}

\begin{via}
If the target datatype is supported (e.g., contiguous) and the target window
is registered, then use \code{MPID_Put_contig}.  Question: do we want a
special version that works only on registered memory?  If the origin datatype
is \emph{not} simple, this will require copying the data to cannonical form.
Question: do we want to define an \code{MPID_Put_contig_stream} that would
allow an overlap of packing and sending?

Otherwise, like \code{MPI_ACCUMULATE}, with \mpiconst{MPI_REPLACE} as the
operation.  
\end{via}

\end{mmadi}
\end{adi3}

\subsubsection{\mpifunc{MPI_GET}}
\begin{tcp}
This is roughly like \code{MPI_PUT}, except the target is requested to send
the data.  
\end{tcp}
\begin{shmem}
\begin{enumerate}
\item If the target window is in shared memory, 
If both origin and target datatypes are simple, then the origin process simply
reads the data from the target window and stores it in the origin buffer.
Otherwise, move through an intermediate form (e.g., contiguous).  

\item Otherwise (the target window is not in shared memory), 
use \tcpname\ approach.
\end{enumerate}
\end{shmem}
\begin{via}
If the target datatype is supported (e.g., contiguous) and the target window
is registered, then use \code{MPID_Get_contig}.  The destination on the origin
process is either the origin buffer (if registered) or a temporary registered
buffer.  

Otherwise, use \tcpname\ approach.
\end{via}

\subsubsection{\mpifunc{MPI_WIN_FENCE}}
\begin{tcp}
This can be viewed as a special case of the post/start/complete/wait
synchronization, with a carefully chosen set of neighbors (e.g., the usual
barrier tree).
\end{tcp}

\begin{shmem}
As for \tcpname, this can be viewed as a special case of the
post/start/complete/wait synchronization. 
\end{shmem}

\begin{via}
As for \tcpname, this can be viewed as a special case of the
post/start/complete/wait synchronization. 
\end{via}

\subsubsection{\mpifunc{MPI_ALLOC_MEM}}
Call \mpidfunc{MPID_Mem_alloc}.  We also need a routine that
\mpifunc{MPI_WIN_CREATE} can all to determine if memory was allocated with
this (or a similar) routine.

\subsubsection{\mpifunc{MPI_FREE_MEM}}
Call \mpidfunc{MPID_Mem_free}.

For error reporting, we may want to keep a reference count so that a
\mpifunc{MPI_Free_mem} applied to a window that is currently part of a window
object generates an error message.

\subsubsection{\mpifunc{MPI_WIN_CREATE}}

\subsubsection{\mpifunc{MPI_WIN_FREE}}

\subsubsection{\mpifunc{MPI_WIN_GET_GROUP}}
Access the group of the related communicator (Question: does this increment the
reference count for the group?)

\subsubsection{\mpifunc{MPI_WIN_GET_NAME}}
Uses the \code{name} field.  Note that the Fortran versions must be careful to
blank-pad the value rather than null-terminating it.

\subsubsection{\mpifunc{MPI_WIN_SET_NAME}}
Sets the \code{name} field.  Returns error if supplied name is too long.

\subsubsection{\mpifunc{MPI_WIN_LOCK} and \mpifunc{MPI_WIN_UNLOCK}}
There are two types of lock and unlock implementations.  In the most
obvious, based on the name, \code{MPI_WIN_LOCK} waits until the
request process acknowledges the lock. This is appropriate when the
window is in memory that is shared among the processes in the window
object, such as a fully shared-memory implementation or a distributed
shared memory implementation.

For systems without direct access to the memory, an alternate but
equally valid approach is to make the lock a local operation, and wait
to issue it until the first RMA operation.  This is particularly
appropriate when the RMA operation (e.g., the put or accumulate)
involves a small amount of data and the interprocess communications
have high latency.  In fact, in the high-latency case, we may prefer
to hold all operations until the \code{MPI_WIN_UNLOCK} and then issue
them in a single communication.  I believe this is similar to what BSP
does, but for fence operations (I need the same discussion under fence).

Question.  For the nonblocking lock case, should we have an info key
for \mpifunc{MPI_WIN_CREATE} that asks for the blocking lock?  

\begin{tcp}
\end{tcp}
\begin{shmem}
\end{shmem}
\begin{via}
\end{via}

\subsubsection{\mpifunc{MPI_WIN_POST}}
Begin an exposure epoch for the local window.
\begin{tcp}
\end{tcp}
\begin{shmem}
\end{shmem}
\begin{via}
\end{via}

\subsubsection{\mpifunc{MPI_WIN_START}}
Start creates an access epoch for the processes in the specified group.  The
implementations here block until the matching \mpifunc{MPI_WIN_POST} calls are
made (implementations that defer communicating can proceed through
\mpifunc{MPI_WIN_START} as long as the matching post occurs before and RMA
actions are taken).

\begin{tcp}
\end{tcp}
\begin{shmem}

\end{shmem}
\begin{via}
\end{via}

\subsubsection{\mpifunc{MPI_WIN_COMPLETE}}
Complete ends an access epoch for the processes in the group specified with
\mpifunc{MPI_Win_start}.

\begin{tcp}
\end{tcp}
\begin{shmem}
\end{shmem}
\begin{via}
\end{via}

\subsubsection{\mpifunc{MPI_WIN_WAIT}}
End an exposure epoch for the local window.
\begin{tcp}
\end{tcp}
\begin{shmem}
\end{shmem}
\begin{via}
\end{via}

\subsection{Starting and Ending MPI}

\subsubsection{\mpifunc{MPI_ABORT}}
\begin{adi3}\mpidfunc{MPID_Abort}
\end{adi3}

\subsubsection{\mpifunc{MPI_INIT_THREAD}}
\begin{adi3}
\mpidfunc{MPID_Init}
\end{adi3}
This should also set the value \mpidfunc{MPID_THREAD_PROVIDED}.

\subsubsection{\mpifunc{MPI_QUERY_THREAD}}
This returns the level of thread support provided.  Should there be an
\mpidfunc{MPID_THREAD_PROVIDED} variable?

\subsubsection{\mpifunc{MPI_IS_THREAD_MAIN}}
This needs a thread-private variable that is set only by the thread that calls
\mpifunc{MPI_INIT_THREAD}.  Alternately, we can use a public variable that
holds the thread id of the thread that called \mpifunc{MPI_INIT_THREAD}.  For
example, 
\begin{verbatim}
static pthread_t main_thread_id;
...
is_main_thread = pthread_equal( main_thread_id, pthread_self() );
\end{verbatim}
This does require that the thread library used by the user is the same as the
one that the MPICH library is built for.  We may want to put this routine in a
separate library, allowing several different thread libraries to be used with
MPICH. 

\subsubsection{\mpifunc{MPI_FINALIZED}}
See \code{MPI_INITIALIZED}

\subsubsection{\mpifunc{MPI_INIT}}
Call \mpifunc{MPI_INIT_THREAD} with \code{MPI_THREAD_MULTIPLE} as the
requested level of thread support.

\subsubsection{\mpifunc{MPI_INITIALIZED}}
As part of the error checking code, each routine should check the
state of the \code{is_initialized} flag.  Should there be an 
\begin{verbatim}
    enum { MPICH_PRE_INIT=0, MPICH_IS_INITIALIZED=1,
           MPICH_IS_FINALIZED=2 } MPIR_Initialized;
\end{verbatim}
variable?  This can be used by the \mpifunc{MPI_INITIALIZE} and
\mpifunc{MPI_FINALIZED} calls.

\subsubsection{\mpifunc{MPI_FINALIZE}}
The MPI-2 standard requires that \code{MPI_Finalize} first delete the
attributes associated with \mpiconst{MPI_COMM_SELF}, even before
\mpifunc{MPI_FINALIZED} would return true.  This allows any number of
modules to attach ``end-of-job'' actions to \code{MPI_Finalize}.


\subsection{Dynamic Processes}

This section has not been written.  A big question is how or if the BNR
interface to a parallel process manager is used in the implementation
of the MPICH routines.  

\subsubsection{\mpifunc{MPI_COMM_CONNECT}}

\subsubsection{\mpifunc{MPI_COMM_DISCONNECT}}

\subsubsection{\mpifunc{MPI_COMM_GET_PARENT}}

\subsubsection{\mpifunc{MPI_COMM_JOIN}}

\subsubsection{\mpifunc{MPI_COMM_SPAWN}}
Question:  Should this be a special case of spawn multiple?

\subsubsection{\mpifunc{MPI_COMM_SPAWN_MULTIPLE}}

\subsubsection{\mpifunc{MPI_LOOKUP_NAME}}
Question:  Do we want to define an API or a wire protocol for the name
service?

\subsubsection{\mpifunc{MPI_PUBLISH_NAME}}

\subsubsection{\mpifunc{MPI_UNPUBLISH_NAME}}

\subsubsection{\mpifunc{MPI_OPEN_PORT}}

\subsubsection{\mpifunc{MPI_CLOSE_PORT}}

\subsection{User-Defined Requests}

Question:  What ADI support is required for these?  Note that the
request is under the control of the device, so many of the fields
aren't defined yet.

Note that if \mpidfunc{MPID_Waitsome} is implemented in the ADI, then the
ADI must understand these requests (or at least be able to ignore
them).
Note that a user-defined request is started with callbacks (functions
to call for query, cancel, and free); these need to be associated with
the request.  

\subsubsection{\mpifunc{MPI_GREQUEST_START}}
Where are the typedefs for the query, free, and cancel function defined? 
Are this in mpi.h?  Where are they stored for the generalized request?

\subsubsection{\mpifunc{MPI_GREQUEST_COMPLETE}}

(Need to describe the user-request handling)

\subsection{Error Handlers}


\subsubsection{\mpifunc{MPI_ERRHANDLER_FREE}}
\mpidfunc{MPID_Errhandler_free}?

\subsubsection{\mpifunc{MPI_ERRHANDLER_CREATE}}
Deprecated.  Call \mpifunc{MPI_COMM_CREATE_ERRHANDLER}.

\subsubsection{\mpifunc{MPI_ERRHANDLER_GET}}
Deprecated.  Call \mpifunc{MPI_COMM_GET_ERRHANDLER}.

\subsubsection{\mpifunc{MPI_ERRHANDLER_SET}}
Deprecated.  Call \mpifunc{MPI_COMM_SET_ERRHANDLER}.

\subsubsection{\mpifunc{MPI_ERROR_CLASS}}
Return the error class of an error code.

\subsubsection{\mpifunc{MPI_ERROR_STRING}}
Return the error string associated with an error code.

\subsubsection{\mpifunc{MPI_ADD_ERROR_CLASS}}

\subsubsection{\mpifunc{MPI_ADD_ERROR_CODE}}

\subsubsection{\mpifunc{MPI_ADD_ERROR_STRING}}

\subsubsection{\mpifunc{MPI_COMM_CALL_ERRHANDLER}}
All error handler calls use the common error handler structure
\mpidfunc{MPID_Errhandler} structure.  

\subsubsection{\mpifunc{MPI_COMM_CREATE_ERRHANDLER}}
Create an \mpidconst{MPID_Errhandler}, set the kind to
\mpidconst{MPID_COMM_OBJ}, set the language to \mpidconst{MPID_LANG_C}, and
save the function.  

Question: do we want to define a generic error handler
creation function that could be used from C, Fortran, and C++, as well as from
communicators, windows, and files?  Or is it simple enough to inline?

\subsubsection{\mpifunc{MPI_COMM_GET_ERRHANDLER}}
Return the errhandler from the \code{err_handler} field.  Increment the 
reference count for the error handler.

\subsubsection{\mpifunc{MPI_COMM_SET_ERRHANDLER}}
Error checking: ensure that the error handler is of the correct type.  
Question: do we need a special case for \mpifunc{MPI_ERRORS_RETURN} etc?

Free (reduce the \code{ref_count} and free if zero) the current error handler
and set the error handler field to the specified error handler.

\subsubsection{\mpifunc{MPI_WIN_CREATE_ERRHANDLER}}
Similar to the communicator versions.

\subsubsection{\mpifunc{MPI_WIN_CALL_ERRHANDLER}}
Similar to the communicator versions.

\subsubsection{\mpifunc{MPI_WIN_GET_ERRHANDLER}}
Similar to the communicator versions.

\subsubsection{\mpifunc{MPI_WIN_SET_ERRHANDLER}}
Similar to the communicator versions.

\paragraph{MPI I/O Error Handlers.}
We need to ensure that ROMIO's error handlers are the same as the MPICH-2
handlers.

\subsection{Handle Transfers}
These provide for the conversion of handles to and from the C and
Fortran representations.  C++ is handled as a descendant of C (that
is, there is no Fortran representation of a C++ handle directly, but
C++ can use C handles.  
\begin{figure}
\begin{verbatim}
     +-----+         +---------+
     |  C  | < --- > | Fortran |
     +-----+         +---------+
        ^
        |
        v
     +-----+ 
     | C++ | 
     +-----+ 
\end{verbatim}
\caption{Relationship of handle conversion functions.  The C to/from
C++ are part of the C++ binding of MPI.}\label{fig:handle-transfers}
\end{figure}
These should normally (i.e., unless
\code{--disable-mpi-macros}\cfgindex{--disable-mpi-macros} is
specified to configure) be implemented as macros.  

Unresolved question (raised by Barry Smith):  What should happen if
the handle is invalid?  Should there even be a check?  Raise the error
on \mpiconst{MPI_COMM_SELF} or \mpiconst{MPI_COMM_WORLD}?

The current plan is that all handle transfers will be handled by casting;
the handle transfer routines should all be available as macros, as allowed by
the MPI standard.  The handle transfer routines are:
\mpifunc{MPI_COMM_C2F},
\mpifunc{MPI_COMM_F2C},
\mpifunc{MPI_ERRHANDLER_F2C},
\mpifunc{MPI_ERRHANDLER_C2F},
\mpifunc{MPI_FILE_C2F},
\mpifunc{MPI_FILE_F2C},
\mpifunc{MPI_GROUP_F2C},
\mpifunc{MPI_GROUP_C2F},
\mpifunc{MPI_INFO_F2C},
\mpifunc{MPI_INFO_C2F},
\mpifunc{MPI_OP_F2C},
\mpifunc{MPI_OP_C2F},
\mpifunc{MPI_REQUEST_F2C},
\mpifunc{MPI_REQUEST_C2F},
\mpifunc{MPI_TYPE_C2F},
\mpifunc{MPI_TYPE_F2C},
\mpifunc{MPI_WIN_C2F}, and
\mpifunc{MPI_WIN_F2C}.

% \subsubsection{\mpifunc{MPI_COMM_C2F}}
% \subsubsection{\mpifunc{MPI_COMM_F2C}}
% \subsubsection{\mpifunc{MPI_ERRHANDLER_F2C}}
% \subsubsection{\mpifunc{MPI_ERRHANDLER_C2F}}
% \subsubsection{\mpifunc{MPI_FILE_C2F}}
% \subsubsection{\mpifunc{MPI_FILE_F2C}}
% \subsubsection{\mpifunc{MPI_GROUP_F2C}}
% \subsubsection{\mpifunc{MPI_GROUP_C2F}}
% \subsubsection{\mpifunc{MPI_INFO_F2C}}
% \subsubsection{\mpifunc{MPI_INFO_C2F}}
% \subsubsection{\mpifunc{MPI_OP_F2C}}
% \subsubsection{\mpifunc{MPI_OP_C2F}}
% \subsubsection{\mpifunc{MPI_REQUEST_F2C}}
% \subsubsection{\mpifunc{MPI_REQUEST_C2F}}
% \subsubsection{\mpifunc{MPI_TYPE_C2F}}
% \subsubsection{\mpifunc{MPI_TYPE_F2C}}
% \subsubsection{\mpifunc{MPI_WIN_C2F}}
% \subsubsection{\mpifunc{MPI_WIN_F2C}}

\subsubsection{\mpifunc{MPI_STATUS_F2C}}
This needs to recognize the constants \mpiconst{MPI_F_STATUS_IGNORE} and
\mpiconst{MPI_F_STATUSES_IGNORE} (which must be declared in \file{mpi.h}; see
Section 4.12.5 ``Status'' in MPI-2).  

\subsubsection{\mpifunc{MPI_STATUS_C2F}}
Like \mpifunc{MPI_STATUS_F2C}, but must handle the C constants
\mpiconst{MPI_STATUS_IGNORE} and \mpiconst{MPI_STATUSES_IGNORE}.

\subsection{Timers}
The MPI standard allows \code{MPI_Wtime} and \code{MPI_Wtick} to be
implemented as macros; we should also allow that, at least as an
option.  Question:  should there be a
\code{--enable-mpi-macros}\cfgindex{--enable-mpi-macros} feature in
configure?

Eventually, we should allow for a synchronized timer.  That is, even
if the underlying hardware does not provide a global timer, we should
provide one as an option.

Question: Since we have \mpidfunc{MPID_Gwtime}, should we make that available?

\subsubsection{\mpifunc{MPI_WTICK}}
Call \mpidfunc{MPID_Wtick}.

\subsubsection{\mpifunc{MPI_WTIME}}
Call \mpidfunc{MPID_Wtime}.  Use \mpidfunc{MPID_Wtime_diff} to convert this to
a double and return that value.
This requires that \mpifunc{MPI_Init} cause \mpidfunc{MPID_Wtime_init} to be
called, and that an initial time stamp is stored.

\subsection{I/O}
MPI I/O will be provided by ROMIO.  We expect to update ROMIO to exploit both
MPI-2 functions and to make use of MPID functions (such as the Stream and
Segment modules) when ROMIO is part of MPICH.

There are a few places where we need to improve the current code to provide
better integration:
\begin{itemize}
\item I/O requests must be integrated with all other MPI requests.  This
  eliminates the need for ROMIO's \code{MPIO_Wait} routine.
\item Error handler should consistent with the rest of MPI-2.  Error reporting
  should follow the rest of MPICH-2.
\end{itemize}

% \code{MPI_FILE_CALL_ERRHANDLER}\\
% \code{MPI_FILE_CLOSE}\\
% \code{MPI_FILE_CREATE_ERRHANDLER}\\
% \code{MPI_FILE_DELETE}\\
% \code{MPI_FILE_GET_AMODE}\\
% \code{MPI_FILE_GET_ATOMICITY}\\
% \code{MPI_FILE_GET_BYTE_OFFSET}\\
% \code{MPI_FILE_GET_ERRHANDLER}\\
% \code{MPI_FILE_GET_GROUP}\\
% \code{MPI_FILE_GET_INFO}\\
% \code{MPI_FILE_GET_POSITION}\\
% \code{MPI_FILE_GET_POSITION_SHARED}\\
% \code{MPI_FILE_GET_SIZE}\\
% \code{MPI_FILE_GET_TYPE_EXTENT}\\
% \code{MPI_FILE_GET_VIEW}\\
% \code{MPI_FILE_IREAD}\\
% \code{MPI_FILE_IREAD_AT}\\
% \code{MPI_FILE_IREAD_SHARED}\\
% \code{MPI_FILE_IWRITE}\\
% \code{MPI_FILE_IWRITE_AT}\\
% \code{MPI_FILE_IWRITE_SHARED}\\
% \code{MPI_FILE_OPEN}\\
% \code{MPI_FILE_PREALLOCATE}\\
% \code{MPI_FILE_READ}\\
% \code{MPI_FILE_READ_ALL_BEGIN}\\
% \code{MPI_FILE_READ_AT}\\
% \code{MPI_FILE_READ_AT_ALL_BEGIN}\\
% \code{MPI_FILE_READ_ORDERED}\\
% \code{MPI_FILE_READ_ORDERED_BEGIN}\\
% \code{MPI_FILE_READ_SHARED}\\
% \code{MPI_FILE_SEEK}\\
% \code{MPI_FILE_SEEK_SHARED}\\
% \code{MPI_FILE_SET_ATOMICITY}\\
% \code{MPI_FILE_SET_ERRHANDLER}\\
% \code{MPI_FILE_SET_INFO}\\
% \code{MPI_FILE_SET_SIZE}\\
% \code{MPI_FILE_SET_VIEW}\\
% \code{MPI_FILE_SYNC}\\
% \code{MPI_FILE_WRITE}\\
% \code{MPI_FILE_WRITE_ALL_BEGIN}\\
% \code{MPI_FILE_WRITE_AT}\\
% \code{MPI_FILE_WRITE_AT_ALL_BEGIN}\\
% \code{MPI_FILE_WRITE_ORDERED}\\
% \code{MPI_FILE_WRITE_ORDERED_BEGIN}\\
% \code{MPI_FILE_WRITE_SHARED}\\

% Related constants
% \code{MPI_MODE_APPEND}\\
% \code{MPI_MODE_CREATE}\\
% \code{MPI_MODE_DELETE_ON_CLOSE}\\
% \code{MPI_MODE_NOCHECK}\\
% \code{MPI_MODE_RDONLY}\\
% \code{MPI_MODE_SEQUENTIAL}\\
% \code{MPI_MODE_UNIQUE_OPEN}\\
% \code{MPI_SEEK_CUR}\\
% \code{MPI_SEEK_END}\\
% \code{MPI_SEEK_SET}\\
% \code{MPI_DISPLACEMENT_CURRENT}\\

\section{Portability}

This section discusses how the MPICH implementation is written to
provide portability to a wide variety of systems.  MPICH will continue
to rely on \code{configure}.  However, the use of \code{configure}
will rely on more carefully defined macros, along with more
information stored in external files, allowing for simpler adaptation
to site-specific environments, such as special compilers and runtime
environments. 

Question: What about shared libraries?  Using libtool is awkward for
development, but not using it is awkward for portability (libtool
knows a \emph{lot} about making shared libraries).  However, we
\emph{must} have support for shared libraries.  The plan is to develop
a simple perl program to extract the information stored in the
\code{libtool} source to take advantage of that source of information
without requiring the \code{libtool} development environment.

\subsection{Configure}
We will use autoconf version 2.13 or later.  The top-level configure will only
test for items used in the implementation of the MPI routines.  For
other parts of the package, such as the ADI implementation, the
top-level configure will invoke a configure or other setup script for
each package.

Question: configure understands how to invoke configure for other
packages.  If we use a level of indirection between the ADI configure
(e.g., a setup script), the top-level configure won't know about
this.  Do we care?  If we don't care, how do we ensure that
re-executing \file{config.status} executes the correct routines?

\subsection{Supporting Cross-compilation}
In some cases (e.g., IBM SP or ASCI Red), the compiler that must be used to
compile parallel programs produces executables that must be run under
the parallel environment, which may be difficult and time consuming.
This is a type of cross-compilation.  To support this, configure must
be carefully written both to support cross-compilation and to provide
for a way to specify the results that would have been determined by
running a program.

For each test that requires running a program, a variable of the form
\code{CROSS_xxx} must be defined and documented.  For example, for
variable sizes, \code{CROSS_SIZEOF_INT} will give the size of an
integer in bytes.

We need to document all \code{CROSS_xxx} variables.  Here is a start at that
list:
\begin{description}
\item[\texttt{CROSS_BYTE_ORDERING}]
\item[\texttt{CROSS_STRUCT_ALIGNMENT}]Structure alignment.  One of 
    \begin{description}
    \item[\texttt{packed}]No padding
    \item[\texttt{largest}]Aligned on the largest item
    \item[\texttt{two}]Aligned to two bytes
    \item[\texttt{four}]Aligned to four bytes
    \item[\texttt{eight}]Aligned to eight bytes
    \item[\texttt{other}]Unable to determine
    \end{description}
    A program to determine these already exists and is part of the
    current MPICH.
\item[\texttt{CROSS_SIZEOF_INT}]\code{sizeof(int)}
\item[\texttt{CROSS_SIZEOF_VOID_P}]\code{sizeof(void*)}
\item[\texttt{CROSS_SIZEOF_CHAR}]\code{sizeof(char)}
\item[\texttt{CROSS_SIZEOF_LONG}]\code{sizeof(long)}
\item[\texttt{CROSS_SIZEOF_FLOAT}]\code{sizeof(float)}
\item[\texttt{CROSS_SIZEOF_DOUBLE}]\code{sizeof(double)}
\item[\texttt{CROSS_SIZEOF_LONG_DOUBLE}]\code{sizeof(long double)}
\item[\texttt{CROSS_F77_SIZEOF_INTEGER}]The size of an \code{INTEGER} in
  Fortran (as if there was a \code{sizeof} operator in Fortran)
\item[\texttt{CROSS_F77_SIZEOF_REAL}]Ditto for \code{REAL}
\item[\texttt{CROSS_F77_SIZEOF_DOUBLE_PRECISION}]Ditto for
\code{DOUBLE PRECISION}.
\item[\texttt{CROSS_F90_INTEGER_KIND}]The Fortran 90 kind for an integer that
  corresponds to a C \code{int}.
\item[\texttt{CROSS_OFFSET_KIND}]The Fortran 90 kind for an integer that
  corresponds to a \code{MPI_Offset}.
\item[\texttt{CROSS_ADDRESS_KIND}]The Fortran 90 kind for an integer that
  corresponds to a \code{MPI_Aint}.
\end{description}

Other items, such as the allowed types for the f90 modules, also needs
to be specifyable from the environment.

See also mpi-maint request 5556 for the values needed by the Intel
Tflops system.  It should be possible to specify these with a special
site file.  Also note the need to export variables; we may want to
create a step that does something like \code{set | grep 'CROSS_' | 
sed ...}.  

\subsubsection{Complex Configuration Data}
Much of the complexity in the current configure system comes from
handling special cases, particularly for compiler and linker options.
I propose replacing this code with a separate (yet simple) data list
that can be edited separately from the configure script, and which
contains information on various compilers and linkers.
This file can be considered a very simple database, where each record
contains the following information (some of the information is used
only by compile steps; other by link or shared library steps.  There
is enough overlap that the combined list is given).
\begin{description}
\item[kind]C, C++, Fortran 77, or Fortran 90.  Perhaps Java as well.
\item[action]compile, link, create static library, or create shared library
\item[name]E.g., cc, xlc, pgcc, gcc
\item[ostype]OS that has this compiler.  For some compilers, this is *
(e.g., gcc, pgcc, cc)
\item[optimize]Options for optimizing.
\item[debug]Options for debugging.
\item[ansi]Options to force ANSI (or a superset)
\item[posix]Options to force Posix
\item[threads]Options to allow threads
\item[sharedobject]Options to create a shared object
\item[version]Options to generate a version and name string.  See signature
\item[signature]A regular expression that should match the value
generated by version.  This may be slightly extended to allow a
particular line of the output to be matched.
\item[size32]Options for 32bit pointers
\item[size64]Options for 64bit pointers
\item[size=xx]Options for other sizes (128 bit pointers, anyone?) or
for special versions (e.g., IRIX n32)
\item[searchdir]Options to specify search directories for shared
libraries
\item[sharedlib]Options to create a shared library
\item[cross]Specify values for \code{CROSS_xxx} for cross-compiler case
\item[verbose]Options to generate verbose output, particularly showing
the command-line options passed to other tools such as ld when a
compiler command is used to link a program.  This is useful in
determining the libraries needed for multilanguage programs.
\item[verboseoptionsep]Option separator for the output from verbose.
Often either space or comma.
\item[strict]Options for strict (lint-like) compilation
\end{description}
Question: are these enough?  We should check what libtool uses.
We may also want a option that says ``check for a clean compile before
accepting these values''.  Compiler version numbers may also be
important; for that, there needs to be a command specified to extract and
compare the version number.

This file should have both specific rules and generic rules.  For
example, a generic description of \code{cc} would specify \code{-g}
for debugging, \code{-O} for optimization, and little else.  

In some cases, we may want to try several options.  For example, for
optimization, we may want to try \code{-Ofast} or \code{-O2} before \code{-O}.
Question:
what should the syntax for this be?

I recommend that the syntax for this file be key=value pairs, using a
trailing backslash to continue lines:
\begin{verbatim}
# comment describing compiler
kind=c action=compile name=xlc system=AIX \
  optimize="-O3 -qarch=native" \
  etc.
kind=c action=compile name=cc system=* \
  optimize=-O \
  debug=-g \
  etc.
\end{verbatim}
This is most easily processed using perl or (possibly) python, though
another alternative is to bootstrap by using the usual configure
macros to find a C compiler and then compile a simple program to read
this file.  In the cross compilation case, either a different compiler
may be used, the user can prebuild the program, or all of the
necessary data can be supplied through environment variables.

\subsection{Coding Rules}
We will attempt to enforce the coding rules by building tools that
check for conformance to these rules.

Tools already available:
\begin{description}
\item[checkforglobs]Check for global symbol names
\item[tool-to-be-created]Check for use of banned routines (e.g., \code{printf},
\code{alloca})
\item[gcc -Wall etc.]Missing prototypes, return values,
etc. (Question: add an \code{--enable-strict} to the standard
configure macros?)
\item[tool-to-be-created]Check for OS name or system type in a preprocessor
  statement 
\end{description}

We also need tools for the following:
\begin{itemize}
\item Look for functions and header files that are not universal and
ensure that they are properly guarded.
\item Look for functions that are not universal (e.g., \code{rindex}).
\end{itemize}

\subsubsection{Printing and Other Messages}
Rule: do not use \code{printf} or \code{fprintf} except (possibly) for
messages intended only the for the developers of MPICH2.

Even for developer messages, using \code{printf} is not a good idea.
It is better to call a routine that can handle recording the results.

% See PETSc approach for printing.  

Question: Should we define \code{PRINTF} etc. as we have in MPICH, or
ban those values entirely?  Should we define a general output routine
that can be implemented to output to a file, stdout, or a graphical display?

\subsection{NT Friendly}

Declare all user-visible routines \code{EXPORT_MPI_API}\index{EXPORT_MPI_API}.
This is a 
macro that is defined as empty for UNIX and as the appropriate
Microsoft-specific extension for Windows.

Avoiding \code{printf} is important for Windows applications.

\subsection{Fortran Support}
Fortran support has two parts: Fortran 77 and Fortran 90/95.  

There are a few functions unique to Fortran.  They include
\mpifunc{MPI_SIZEOF} and \mpifunc{MPI_TYPE_CREATE_F90_COMPLEX}, 
\mpifunc{MPI_TYPE_CREATE_F90_REAL}, and 
\mpifunc{MPI_TYPE_CREATE_F90_INTEGER}.


\subsubsection{\mpifunc{MPI_SIZEOF}}
This must be implemented in an MPI module.

\subsubsection{Fortran Wrappers}
One added complexity of the Fortran wrappers is handling the possibility that
the types \code{MPI_Fint} and \code{int} have different sizes.  When the
Fortran codes simply call the C codes, this results in copying array arguments
to a temporary array, calling the C code, and freeing the array.  Allocation
and deallocation of small arrays can be avoided by using local arrays (to be
thread-safe).  However, I'd prefer to avoid this whenever possible.  Thus, we
need a CPP value that indicates whether \code{MPI_Fint} and \code{int} are the
same size, such as \mpifunc{MPICH_FINT_EQ_INT}.

In addition, the MPICH Fortran wrapper code is intended for use with MPE and
other MPI implementations, and makes no assumptions about the structure of
MPI opaque objects, necessitating a transfer of values for each array-value
argument.  I'd like to avoid this as well.  Can we ignore the other MPI's, or
use special routines as part of the MPE support?  Alternately, should we
define \code{MPICH_REQUEST_C_IS_F77} etc.?

\subsection{C++ Support}

I'd like to consider adding more native C++ support.  While the Notre
Dame C++ code is valuable and helpful, there are some difficulties:
\begin{enumerate}
\item Supports only MPI-1.
\item Their configure is based too much on particular systems rather than on
capabilities. 
\item Because it is a separate package, the build process is awkward.
\item Testing is separate from the MPICH testing, causing inadequate
testing of the MPICH/C++ combination.
\item Layering makes some things difficult. 
\item Does not pass our (sometimes stricter) coding standards.
\item Layering can be awkward because some data structures must be
duplicated since MPI itself leaves them opaque. 
\end{enumerate}

\section{Standard Features}
(this section contains the standard features, such as command line
handling, environment variables, configure options for error checking,
etc.)

The implementation must provide the \emph{service} of providing
command-line arguments to each process.  If the startup environment
does not do so, the implementation must (see the discussion of
\mpidfunc{MPID_Init}).  

\section{Testing}

The tests for MPICH2 need to be more organized that for MPICH. They
should follow these principles:
\begin{enumerate}
\item Require no user intervention (e.g., to inspect the results)
\item Should be implementation independent (i.e., useful for
\emph{any} MPI implementation) unless they are testing \emph{only}
features specific to MPICH2.  An example is a test of error messages
and error class/code values; any test that expects a particular
message must not be combined with a general test of standard
conformance.
\item Require no extra files (the \file{.std} files in the MPICH
version).
\item Testing should be applied to a range of communicators and
datatypes, not just \code{MPI_COMM_WORLD}.  
\item Testing should be controlled by a file listing the tests rather
than a script.  That is, the script \code{runtests} should be generic,
working with a file in each testing directory that lists the test
programs and any special options (e.g., number of processes, command
line arguments, environment variables).
\item Testing that enables memory tracing should be simple and
organized so that it can be run regularly.
\item Should have short duration, so that the testing program can
signal failure because a program has not completed within the given
time.
\end{enumerate}

Results should be maintained in a database which should include
\begin{enumerate}
\item Configuration options
\item System description (including compiler version and machines file)
\item CVS tag (or nearest value, e.g., weekly tag + date that source
cut was made).
\item Results summary (success or failure; if failure, reason).
\item We should consider an XML format for the output.
\end{enumerate}

Testing should also contain performances tests.
\begin{enumerate}
\item Configuration options
\item System description (including compiler version and machines file)
\item CVS tag (or nearest value, e.g., weekly tag + date that source
cut was made).
\item Results for the following tests:
    \begin{enumerate}
    \item \code{mpptest -logscale} To get the general trend in performance
    \item \code{mpptest -auto} To get details for the short message
performance. 
    \item bisection bandwidth test for a large number of processes.
    We may want to use beff instead of mpptest.
    \item \code{mpptest -logscale -halo -npartner 8} To get more
    realistic communication performance
    \item \code{mpptest -logscale -gop -dsum} 
    \item Similar test for alltoallv. 
    \item Tests for I/O (Rajeev and Rob to identify)
    \item Tests for put/get/accumulate as those become available.
    Start with \code{mpptest -logscale -put} and \code{mpptest
-logscale -halo -npartner 8}
    \end{enumerate}
\item This should also be in XML format.
    Question: Should we add an XML output format to \code{mpptest}?
Is there any clear XML definition yet for 2-d data?

\end{enumerate}
We should maintain the same data for vendor and other MPI
implementations, and we should ask for a standard set of tests.  

\subsection{Debugger Interface}
We need a test that the DLL that provides access to the internal
symbols is correct, as well as a test that the MPICH library correctly
identifies the location of the DLL.

To test the DLL, we need a program \file{mpichdlltest.c} that can
load the dll and call the routines, ensuring that it runs correctly.
This program should use the ADI's include files to provide the data
structures that the DLL accesses.

Note that the debugger interface must be compiled in 32-bit mode on
platforms with mixed 32 and 64-bit modes.  We don't currently do this
(which is a bug) but we need to.  To do this, we need a configure step
that may need to know that a system has both options (in the case that
\code{sizeof(void *)} is 64.  See ``complex configuration
options'' above.

Still needed: Information needed for debugger startup and message
queues.

\subsection{Performance and Tracing Data}

One critical piece of information that is hard currently to acquire is
the amount of idle time spent in completing a communication operation.
This is information that a tracing library would like to have.  Should
there be an \code{MPID_xxx} call that could be made available to a tracing
package?  For example, it could have semantics roughly like
\code{MPI_Wtime}, except that it would contain cumulative idle time.  In a
multithreaded environment, it could give a per-thread cumulative idle
time (or could it)?  Should we define \code{MPID_Idle_time}, and modify MPE
to look for that name in the MPI library?

To tune the layered routines, including the collective routines, we
should include from the beginning standardized tracing for:
\begin{enumerate}
\item All (major?) MPID calls
\item Idle time
\item Context switches (if using threads)
\item Resource usage
\item Flow control
\end{enumerate}

%\let\SaveBibliography=\thebibliography
%\def\thebibliography#1{\SaveBibliography{#1}\addcontentsline{toc}{section}{References}}

\section{Appendix}
This section contain \emph{all} of the MPI functions and terms.  These
will be moved into the body of this document; an index will then
replace this appendix.  Until then, this section serves as a place to
cache these items.

\subsection{Constants}
(I need to get the union of MPI-1 and MPI-2 constants)

\subsection{MPI-1 Functions}
\code{MPI_GET_PROCESSOR_NAME}\\
\code{MPI_PCONTROL}\\

\subsection{MPI-2 Functions}
This also includes MPI-1 functions with significant new functionality,
such as the collective communication routines that are extended, in
MPI-2, from intracommunicators to intercommunicators.

\code{MPI_GET_VERSION}\\
\\
\code{MPI_PACK_EXTERNAL}\\
\code{MPI_PACK_EXTERNAL_SIZE}\\
\code{MPI_UNPACK_EXTERNAL}\\
\\
\code{MPI_REGISTER_DATAREP}\\
\\
\code{MPI_TYPE_DUP}\\
\code{MPI_TYPE_MATCH_SIZE}\\
\\


\subsection{Typedefs}
\code{MPI_Aint}\\
\code{MPI_Comm_copy_attr_function}\\
\code{MPI_Comm_errhandler_fn}\\
\code{MPI_Datarep_conversion_function}\\
\code{MPI_Datarep_extent_function}\\
\code{MPI_File_errhandler_fn}\\
\code{MPI_Grequest_cancel_function}\\
\code{MPI_Type_copy_attr_function}\\
\code{MPI_Win_copy_attr_function}\\
\code{MPI_Win_errhandler_fn}\\

\section{Development Plan}
\label{sec:development}
This section proposes a development and implementation plan.  The
emphasis here is on independent subprojects, allowing development to
proceed without waiting for all of the pieces to be in place.

\begin{enumerate}
\item ADI Implementation
    \begin{enumerate}
    \item Design Development
        \begin{enumerate}
        \item Design and implement a subset of general datatype pack/unpack
        \item Design and implement a subset (e.g., Allreduce, Bcast, and
          Alltoall) of collective operations using the Stream and Segment MPID
          functions.
        \item Test improved collectives for performance and functionality
        \item Similar design/implement/test for RMA and Dynamic; details yet
          to be determined.
        \end{enumerate}
    \item Multi-method ADI for shared memory, VIA, and TCP/UDP
        \begin{enumerate}
        \item Design and implement general datatype point-to-point
        \item Design and implement general datatype collective
        (layered on ADI segment and stream interfaces)
        \item Design and implement RMA
        \item Design and implement Dynamic and connect with BNR
        \end{enumerate}
    \item ADI on \code{MPID_CORE}
        \begin{enumerate}
        \item Implement all ADI-3 modules on top of the core.  Refine
        the design of the core during this process.   
        \end{enumerate}
    \item Implement common ADI services.  These are modules that are
    needed by other ADI modules that do not (directly) involve
    interprocess communication but will be common for most ADI
    implementations, including both the core and the multi-method
    implementation. 
        \begin{enumerate}
        \item Error handling
        \item Attributes
        \item Topology
        \item Datatype
        \item Group
        \item Timer
        \item Utility
        \end{enumerate}
    \end{enumerate}
\item MPICH Implementation on ADI
    \begin{enumerate}
    \item For each MPI routine, write the ``top'': structured comment,
    routine header and error checking.
    \item Implement the action of each MPI routine in terms of the
    full ADI-3 design.
    \end{enumerate}
\item Test Suite
    \begin{enumerate}
    \item Design standard test harness: test communicators, datatypes,
    operation mixtures, run script, and result checking.
    \end{enumerate}
\item Deployment
    \begin{enumerate}
    \item Implement new configure.  Design database for both configure
    macros and for data (such as compiler options and names) that
    configure cannot determine.
    \item Shared library support.
    \end{enumerate}
\end{enumerate}

%
% Eventually include table or a project as Postscript from project.
%

\let\SaveBibliography=\thebibliography
\def\thebibliography#1{\SaveBibliography{#1}\addcontentsline{toc}{section}{References}}
\bibliography{/home/MPI/allbib,/home/gropp/Update/new/gropp,mpich2}
\bibliographystyle{plain}

% Index
%\openin\testfile{adi3man.ind}
%\ifeof\testfile\else
\let\SaveIndex=\theindex
\def\theindex{\SaveIndex\addcontentsline{toc}{section}{Index}}
\input mpich2.ind
%\fi
%\closein\testfile

\end{document}
