-------------------------------------------------------------------------------
		       Major Changes Impacting Usability
-------------------------------------------------------------------------------

- A new and improved process manager that spawns processes using fork() in now
  available.  This process manager, "gforker", should be used in place of the
  previous "forker" process manager.

- (see the CHANGES file for a more extensive list of changes and new features)

-------------------------------------------------------------------------------
                              Known Deficiencies
-------------------------------------------------------------------------------

- Several of the tests in the MPICH2 test suite may fail.  On the other hand, a
  few hundred pass :-)

  The scancel test is expected to fail for all configurations.

  The sock channel is the only channel that implements dynamic process support
  (i.e., MPI_COMM_SPAWN, MPI_COMM_CONNECT, MPI_COMM_ACCEPT, etc.).  All other
  channels will experience failures for tests exercising dynamic process
  functionality.

  The implementation of MPI-IO has a few outstanding issues with 64-bit
  pointers, so test failures in these areas are expected on 64-bit platforms.

- Support for the "external32" data representation is incomplete.  This affects
  the MPI_PACK_EXTERNAL and MPI_UNPACK_EXTERNAL routines, as well the external
  data representation capabilities of ROMIO.

- MPI_THREAD_MULTIPLE and MPI_THREAD_SERIALIZE are not supported at present;
  however, we expect MPICH2 to be thread-safe in time for the next release.

- Previous releases of MPICH2 supported the "forker" process manager.  This
  process manager is still available but is unsupported and untested.  It
  does not support the dynamic-process operations in MPI-2 and thus will
  fail many of the tests included with MPICH2.

- The CH3 device does not presently support heterogeneous communication.  That
  is to say that the processes involved in a job must use the same basic type
  sizes and format.  The sizes and format are typically determined by the
  processor architecture, although it may also be influenced by compiler
  options.

- MPI_IRECV operations that are not explicitly completed before MPI_FINALIZE is
  called may fail to complete before MPI_FINALIZE returns, and thus never
  complete.  Furthermore, any matching send operations may erroneously fail.
  By explicitly completed, we mean that the request associated with the
  operation is completed by one of the MPI_TEST or MPI_WAIT routines.

- MPI_INTERCOMM_CREATE does not work correctly when the union of the local and
  remote communicators are not already part of an existing (intra or inter)
  communicator.  For example, if communicator A spawns a set of processes whose
  MPI_COMM_WORLD we will refer to as communicator B.  Likewise, communicator B
  spawns a set of processes whose MPI_COMM_WORLD we will refer to as
  communicator C.  Despite being a legal request, MPI_INTERCOMM_CREATE will
  fail to create a new intercommunicator between A and C using a commom process
  in B.

- C++ Binding:
  The following functions do not yet have C++ bindings
  Grequest::Create
  Comm::Join  
  
  There is an unidentified problem with the intercommunicator collective
  routines, except for Intercomm::Barrier.

  The MPI datatypes corresponding to Fortran datatypes are not available
  (e.g., no MPI::DOUBLE_PRECISION).

  The Comm class is not an Abstract Base Class (ABC).  This does not 
  affect correct MPI programs, but will allow some incorrect programs to 
  compile.  In addition, no functions are virtual; the standard requires
  most (but not all) to be declared virtual. Again, most correct programs
  will not notice this limitation.

  The C++ binding does not implement a separate profiling interface, 
  as allowed by the MPI-2 Standard (Section 10.1.10 Profiling).  

  With the exception of the profiling interface, future releases of MPICH2 
  will address these limitations of the C++ binding.

- For passive target RMA, there is no asynchronous agent at the target
  that will cause progress to occur. Progress occurs only when the user
  calls an MPI function at the target (which could well be MPI_WIN_FREE).


-------------------------------------------------------------------------------
			Issues for Developers
-------------------------------------------------------------------------------

- MPICH2 requires autoconf version 2.53 or lower; however, MPE2, which is
  distributed as part of MPICH2, requires autoconf 2.52 or higher.  By default,
  MPICH2 and MPE use the autoconf programs in one's path.  The environment
  variables AUTOCONF and AUTOHEADER can be set to specify alternative commands
  for MPICH2.  Likewise, MPE_AUTOCONF and MPE_AUTOHEADER can be set to specify
  alternative commands for MPE2.  Obviously, these environment variables must
  be set before running "maint/updatefiles" or "make dist".  At present, we use
  a patched version of 2.13 for MPICH2 and 2.59 for MPE2 for releases, although
  2.53 appears to work fine for both packages.  The patch for 2.13 is located
  in maint/ac2.13.diff.
